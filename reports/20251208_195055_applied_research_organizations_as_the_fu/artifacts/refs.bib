@article{Attention_Is_Al_Vaswan_2017,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  eprint = {1706.03762v7},
  eprintclass = {cs.CL},
  eprinttype = {arxiv},
  month = {6},
  title = {Attention Is All You Need},
  url = {http://arxiv.org/abs/1706.03762v7},
  year = {2017},
}

@article{s1_Simple_test_Muenni_2025,
  abstract = {Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI\textquotesingle s o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model\textquotesingle s thinking process or lengthening it by appending "Wait" multiple times to the model\textquotesingle s generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27\% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50\% to 57\% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1},
  author = {Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\`e}s, Emmanuel and Hashimoto, Tatsunori},
  eprint = {2501.19393v3},
  eprintclass = {cs.CL},
  eprinttype = {arxiv},
  month = {1},
  title = {s1: Simple test-time scaling},
  url = {http://arxiv.org/abs/2501.19393v3},
  year = {2025},
}

@article{Bias_in_meta_an_Egger_1997,
  author = {Egger, M. and Smith, G. D. and Schneider, M. and Minder, C.},
  doi = {10.1136/bmj.315.7109.629},
  issue = {7109},
  journal = {BMJ},
  language = {en},
  month = {9},
  pages = {629--634},
  publisher = {BMJ},
  title = {Bias in meta-analysis detected by a simple, graphical test},
  url = {https://doi.org/10.1136/bmj.315.7109.629},
  volume = {315},
  year = {1997},
}

@article{Government_poli_Tsai_2004,
  author = {Tsai, W.T. and Chou, Y.H.},
  doi = {10.1016/s0959-6526(03)00053-2},
  issue = {7},
  journal = {Journal of Cleaner Production},
  language = {en},
  month = {9},
  pages = {725--736},
  publisher = {Elsevier BV},
  title = {Government policies for encouraging industrial waste reuse and pollution prevention in Taiwan},
  url = {https://doi.org/10.1016/s0959-6526(03)00053-2},
  volume = {12},
  year = {2004},
}

@inproceedings{THE_MAIN_FACTOR_Sidoro_2019,
  author = {Sidorova, Elena},
  booktitle = {19th SGEM International Multidisciplinary Scientific GeoConference EXPO Proceedings},
  doi = {10.5593/sgem2019/5.3/s21.106},
  journal = {SGEM International Multidisciplinary Scientific GeoConference EXPO Proceedings},
  month = {6},
  publisher = {STEF92 Technology},
  title = {THE MAIN FACTORS AND CONDITIONS DETERMINING THE FEASIBILITY OF PRODUCTION OF HIGH-TECH PRODUCTS BASED ON THE POTENTIAL OF APPLIED RESEARCH ORGANIZATIONS},
  url = {https://doi.org/10.5593/sgem2019/5.3/s21.106},
  year = {2019},
}

@article{Attention_is_Al_Subaka_2020,
  abstract = {Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning. RNNs, however, are inherently sequential models that do not allow parallelization of their computations. Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations with a multi-head attention mechanism. In this paper, we propose the SepFormer, a novel RNN-free Transformer-based neural network for speech separation. The SepFormer learns short and long-term dependencies with a multi-scale approach that employs transformers. The proposed model achieves state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the parallelization advantages of Transformers and achieves a competitive performance even when downsampling the encoded representation by a factor of 8. It is thus significantly faster and it is less memory-demanding than the latest speech separation systems with comparable performance.},
  author = {Subakan, Cem and Ravanelli, Mirco and Cornell, Samuele and Bronzi, Mirko and Zhong, Jianyuan},
  eprint = {2010.13154v2},
  eprintclass = {eess.AS},
  eprinttype = {arxiv},
  month = {10},
  title = {Attention is All You Need in Speech Separation},
  url = {http://arxiv.org/abs/2010.13154v2},
  year = {2020},
}

@article{Why_dont_trans_Khomya_2025,
  abstract = {<jats:p>Large language models in the form of chatbots very realistically imitate a dialogue as an omniscient interlocutor and therefore have become widespread. But even Google in its Gemini chatbot does not recommend trusting what the chatbot will write and asks to check its answers. In this review, various types of LLM errors such as the curse of inversion, number processing, etc. will be analyzed to identify their causes. Such an analysis led to the conclusion about the common causes of all errors, which is that transformers do not have deep analogy, hierarchy of schemes and selectivity of content taken into account in the inference. But the most important conclusion is that transformers, like other neural networks, are built on the concept of processing the input signal, which creates a strong dependence on superficial noise and irrelevant information that the transformer’s attention layer cannot compensate for. The concept of neural networks was laid down in the 1950s by the idea of F. Rosenblatt’s perceptron and did not take into account the achievements of cognitive psychology that appeared later. According to the constructivist paradigm, the input word (or perception) is only a way to check the correctness of the constructed predictive model for possible situations. This is the cause of the biggest problem of transformers, called hallucinations. And its elimination is possible only by changing the architecture of the neural network, but not by increasing the amount of data in training.</jats:p>},
  author = {Khomyakov, A. B.},
  doi = {10.17726/philit.2024.2.6},
  issue = {2},
  journal = {Philosophical Problems of IT \&amp; Cyberspace (PhilIT\&amp;C)},
  month = {1},
  pages = {87--98},
  publisher = {Pyatigorsk State University},
  title = {Why don’t transformers think like humans?},
  url = {https://doi.org/10.17726/philit.2024.2.6},
  year = {2025},
}

@article{Neural_Machine_Bahdan_2014,
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  eprint = {1409.0473v7},
  eprintclass = {cs.CL},
  eprinttype = {arxiv},
  month = {9},
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  url = {http://arxiv.org/abs/1409.0473v7},
  year = {2014},
}

@article{Effective_Appro_Luong_2015,
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT\textquotesingle 15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  eprint = {1508.04025v5},
  eprintclass = {cs.CL},
  eprinttype = {arxiv},
  month = {8},
  title = {Effective Approaches to Attention-based Neural Machine Translation},
  url = {http://arxiv.org/abs/1508.04025v5},
  year = {2015},
}

@article{An_Image_is_Wor_Dosovi_2020,
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  eprint = {2010.11929v2},
  eprintclass = {cs.CV},
  eprinttype = {arxiv},
  month = {10},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  url = {http://arxiv.org/abs/2010.11929v2},
  year = {2020},
}

@article{Linear_attentio_Ahn_K_2023,
  abstract = {Transformer training is notoriously difficult, requiring a careful design of optimizers and use of various heuristics. We make progress towards understanding the subtleties of training Transformers by carefully studying a simple yet canonical linearized shallow Transformer model. Specifically, we train linear Transformers to solve regression tasks, inspired by J.\textasciitilde von Oswald et al.\textasciitilde (ICML 2023), and K.\textasciitilde Ahn et al.\textasciitilde (NeurIPS 2023). Most importantly, we observe that our proposed linearized models can reproduce several prominent aspects of Transformer training dynamics. Consequently, the results obtained in this paper suggest that a simple linearized Transformer model could actually be a valuable, realistic abstraction for understanding Transformer optimization.},
  author = {Ahn, Kwangjun and Cheng, Xiang and Song, Minhak and Yun, Chulhee and Jadbabaie, Ali and Sra, Suvrit},
  eprint = {2310.01082v2},
  eprintclass = {cs.LG},
  eprinttype = {arxiv},
  month = {10},
  title = {Linear attention is (maybe) all you need (to understand transformer optimization)},
  url = {http://arxiv.org/abs/2310.01082v2},
  year = {2023},
}

@article{Attention_Is_No_Gerber_2025,
  abstract = {Decoder-only transformer networks have become incredibly popular for language modeling tasks. State-of-the-art models can have over a hundred transformer blocks, containing billions of trainable parameters, and are trained on trillions of tokens of text. Each transformer block typically consists of a multi-head attention (MHA) mechanism and a two-layer fully connected feedforward network (FFN). In this paper, we examine the importance of the FFN during the model pre-training process through a series of experiments, confirming that the FFN is important to model performance. Furthermore, we show that models using a transformer block configuration with three-layer FFNs with fewer such blocks outperform the standard two-layer configuration delivering lower training loss with fewer total parameters in less time.},
  author = {Gerber, Isaac},
  eprint = {2505.06633v1},
  eprintclass = {cs.CL},
  eprinttype = {arxiv},
  month = {5},
  title = {Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models},
  url = {http://arxiv.org/abs/2505.06633v1},
  year = {2025},
}

@article{Is_Space_Time_A_Bertas_2021,
  abstract = {We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named "TimeSformer," adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that "divided attention," where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: https://github.com/facebookresearch/TimeSformer.},
  author = {Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  eprint = {2102.05095v4},
  eprintclass = {cs.CV},
  eprinttype = {arxiv},
  month = {2},
  title = {Is Space-Time Attention All You Need for Video Understanding?},
  url = {http://arxiv.org/abs/2102.05095v4},
  year = {2021},
}

@article{Attention_is_no_Jain_2019,
  abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful \textasciigrave explanations\textquotesingle  for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/AttentionExplanation.},
  author = {Jain, Sarthak and Wallace, Byron C.},
  eprint = {1902.10186v3},
  eprintclass = {cs.CL},
  eprinttype = {arxiv},
  month = {2},
  title = {Attention is not Explanation},
  url = {http://arxiv.org/abs/1902.10186v3},
  year = {2019},
}

@article{Efficient_Trans_Tay_Y_2020,
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  eprint = {2009.06732v3},
  eprintclass = {cs.LG},
  eprinttype = {arxiv},
  month = {9},
  title = {Efficient Transformers: A Survey},
  url = {http://arxiv.org/abs/2009.06732v3},
  year = {2020},
}

@article{Attention_is_no_Wiegre_2019,
  abstract = {Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model\textquotesingle s prediction, and consequently reach insights regarding the model\textquotesingle s decision-making process. A recent paper claims that \textasciigrave Attention is not Explanation\textquotesingle  (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one\textquotesingle s definition of explanation, and that testing it needs to take into account all elements of the model, using a rigorous experimental design. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don\textquotesingle t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
  author = {Wiegreffe, Sarah and Pinter, Yuval},
  eprint = {1908.04626v2},
  eprintclass = {cs.CL},
  eprinttype = {arxiv},
  month = {8},
  title = {Attention is not not Explanation},
  url = {http://arxiv.org/abs/1908.04626v2},
  year = {2019},
}

@article{BERT_Pre_train_Devlin_2018,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.   BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  eprint = {1810.04805v2},
  eprintclass = {cs.CL},
  eprinttype = {arxiv},
  month = {10},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url = {http://arxiv.org/abs/1810.04805v2},
  year = {2018},
}

@article{Language_Models_Brown_2020,
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\textquotesingle s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  eprint = {2005.14165v4},
  eprintclass = {cs.CL},
  eprinttype = {arxiv},
  month = {5},
  title = {Language Models are Few-Shot Learners},
  url = {http://arxiv.org/abs/2005.14165v4},
  year = {2020},
}

@article{FlashAttention_Dao_T_2022,
  abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\textdollar \textbackslash times\textdollar  speedup on GPT-2 (seq. length 1K), and 2.4\textdollar \textbackslash times\textdollar  speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  eprint = {2205.14135v2},
  eprintclass = {cs.LG},
  eprinttype = {arxiv},
  month = {5},
  title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  url = {http://arxiv.org/abs/2205.14135v2},
  year = {2022},
}

@article{Switch_Transfor_Fedus_2021,
  abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  eprint = {2101.03961v3},
  eprintclass = {cs.LG},
  eprinttype = {arxiv},
  month = {1},
  title = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  url = {http://arxiv.org/abs/2101.03961v3},
  year = {2021},
}

@article{Public_Support_Chatzi_2024,
  abstract = {<jats:title>Abstract</jats:title>
          <jats:p>This study explores the challenges of integrating macro, meso, and micro in the articulation of advanced innovation policy and examines, respectively, dimensions of public business support, intermediary organizations, and knowledge transfer. It conducts an integrative review of the pertinent literature and a bibliometric analysis of 440 articles. It reveals three major obstacles that seemingly impede the effective integration of macro, meso, and micro in contemporary policymaking and socioeconomic analyses: entrenched boundaries between different thematic areas, methodological discrepancies, and the relative lack of integrated theoretical models. These factors contribute to the absence of unified functional hubs focused on microlevel interventions. The proposed Institutes of Local Development and Innovation (ILDIs) could mitigate these challenges as they are presented as multilevel policy instruments intended to provide support to businesses\textemdash particularly to those facing chronic and structural problems.</jats:p>},
  author = {Chatzinikolaou, Dimos and Vlados, Charis},
  doi = {10.1007/s13132-024-02161-y},
  issue = {1},
  journal = {Journal of the Knowledge Economy},
  language = {en},
  month = {6},
  pages = {4605--4625},
  publisher = {Springer Science and Business Media LLC},
  title = {Public Support for Business, Intermediary Organizations, and Knowledge Transfer: Critical Development and Innovation Policy Bottlenecks},
  url = {https://doi.org/10.1007/s13132-024-02161-y},
  volume = {16},
  year = {2024},
}

@article{Bridging_the_V_Hansen_2022,
  abstract = {<jats:title>Abstract</jats:title><jats:p>Government funding supports industry-academia research and innovation projects in Norway, sharing the risk of the research component in innovation. However, funding alone may not be sufficient to overcome potential differences in collaborative agendas and ways of working. As a result, positive research outcomes often get stuck in the <jats:italic>valley of death</jats:italic>, instead of ending up as successful innovations that create value. To contribute to bridging the <jats:italic>valley of death</jats:italic>, we investigated the importance of six agile principles for collaborative industry-academia research and innovation projects, abbreviated IPN in Norway. The study was limited to the manufacturing sector. We surveyed 124 IPN project leaders (70 from industry; 54 from academia) to evaluate the importance of the knowledge management practices associated with the six agile principles across the three project stages. The statistical analyses indicate the consistency of the agile principles throughout the project stages. This means that agile principles are relevant for IPN projects and can be used as guidelines for improvement of the knowledge management practices. Moreover, the study identifies the agile principles that are perceived as most important to use in different stages of a project. It also identifies the different perceptions of the importance of agile principles of the project leaders from industry and academia. These findings can support project leaders who are implementing agile principles to industry-academia research and innovation projects. The results from the study can also support national and federal research/innovation councils in decision-making when assessing industrial research applications.</jats:p>},
  author = {Hansen, Irina-Emily and Mork, Ola Jon and Welo, Torgeir and Ringen, Geir},
  doi = {10.1007/s13132-021-00846-2},
  issue = {4},
  journal = {Journal of the Knowledge Economy},
  language = {en},
  month = {12},
  pages = {3172--3194},
  publisher = {Springer Science and Business Media LLC},
  title = {Bridging the ‘Valley of Death’: Can Agile Principles Be Applied in Industry-Academia Research and Innovation Projects?},
  url = {https://doi.org/10.1007/s13132-021-00846-2},
  volume = {13},
  year = {2022},
}

@article{Systems_Enginee_DiMari_2023,
  abstract = {<jats:title>ABSTRACT</jats:title><jats:p>A failure of a great many early research and development programs is the result of encountering the traditional valley of death that shadows early research and technology development. The elements that create the valley of death leads to research and technology development high risk and poor return on investment for a great many research and development organizations. This leads eventually to avoiding research and technology development all together because the organizations cannot viably manage the outcome of their early‐stage research and development (ESR\&amp;D) efforts. Unfortunately, there are few established frameworks and processes for enabling smooth transitions to avoid failure and manage risk across fundamental research, applied research, development, and productization. Many leaders, program managers, and scientists are unwilling to involve systems engineering because of the perception that systems engineering is heavily process oriented, adds unnecessary costs, and should be applied only to mature technologies. The value of systems engineering as applied to ESR\&amp;D is unclear to these key individuals. The unfortunate result is that systems engineering is not applied to ESR\&amp;D. This article discusses the potential of application of systems engineering to ESR\&amp;D to improve return on investment and decrease risk.</jats:p>},
  author = {DiMario, Michael and Hodges, Ann},
  doi = {10.1002/inst.12451},
  issue = {3},
  journal = {INSIGHT},
  language = {en},
  month = {9},
  pages = {8--14},
  publisher = {Wiley},
  title = {Systems Engineering Management in Research and Development Valley of Death},
  url = {https://doi.org/10.1002/inst.12451},
  volume = {26},
  year = {2023},
}

@article{Research_and_te_Khelfa_2023,
  abstract = {<jats:title>Abstract</jats:title><jats:p>Research and technology organizations (RTOs) are studied in the innovation policy literature mainly as providers of R\&amp;D services and as intermediaries between universities and the private sector. Through the case of the Institut National d’Optique (INO), Canada’s leading RTO in the optics and photonics industry, we argue that RTOs can also act as entrepreneurs by generating technologies and commercializing them through licensing, technology transfers and spin-offs. By analyzing the broad range of activities undertaken by INO, we also discuss what characteristics make some RTOs more likely to embrace entrepreneurship than others. Those characteristics include the following: renewed access to government funding to build a strong in-house research infrastructure and scientific workforce; strategic R\&amp;D planning that incorporates commercial objectives and an environment that encourages a culture of entrepreneurship among employees; the ability to act as the driving force of a network of academic, government and private sector organizations. From a policy perspective, the INO case indicates that the main value of using RTOs as entrepreneurship instruments does not lie in profitability but rather in developing dynamic regional systems of innovation.</jats:p>},
  author = {Khelfaoui, Mahdi and Bernier, Luc},
  doi = {10.1186/s13731-023-00321-z},
  issue = {1},
  journal = {Journal of Innovation and Entrepreneurship},
  language = {en},
  month = {8},
  publisher = {Springer Science and Business Media LLC},
  title = {Research and technology organizations as entrepreneurship instruments: the case of the Institut National d’Optique in the Canadian optics and photonics industry},
  url = {https://doi.org/10.1186/s13731-023-00321-z},
  volume = {12},
  year = {2023},
}

@article{Research_and_Te_Sheikh_2021,
  abstract = {<jats:p>Research and Technology Organizations (RTOs) have key roles in stories of national industrial development in many countries, and in various contexts they have transformed according to changes in their surrounding economic and policy environments. This paper proposes a conceptual framework of ‘RTOs as super intermediaries’ as they play multiple intermediary roles in the triple helix (government, research and industry), the overlap of industrial policy and research policy, and research-industry frontiers. The framework helps in understanding and advancing the role of RTOs in industrial development, particularly in developing countries. For a case study, the paper showcases research in Tanzania that explored possibilities of revamping RTOs and whether investing in them would help in spurring Tanzania\textquotesingle s industrial development. Through key informant interviews and systemic literature review, a case study on the challenges and opportunities of RTOs was designed to examine their role and potential in industrial development and technology innovation processes. The study findings were overall in-line with two main lenses of inquiry: 1) that for RTOs to play their key roles in Tanzania, industrial policies shaped by the command economy era before the 1990s need to be reviewed and modified; and 2) that more investment in revamping RTOs will take place if policymaking processes acknowledge RTOs as super intermediaries. To organize policy lessons drawn, a multi-level policy map\textemdash micro, meso and macro\textemdash was utilized as an analytical tool.</jats:p>},
  author = {Sheikheldin, Gussai H.},
  doi = {10.3389/frma.2021.691247},
  journal = {Frontiers in Research Metrics and Analytics},
  month = {6},
  publisher = {Frontiers Media SA},
  title = {Research and Technology Organizations as Super Intermediaries: A Conceptual Framework for Policy and a Case Study From Tanzania},
  url = {https://doi.org/10.3389/frma.2021.691247},
  volume = {6},
  year = {2021},
}

@article{Economic_Footpr_Taverd_2022,
  author = {Taverdet-Popiolek, Nathalie},
  doi = {10.1007/s13132-020-00709-2},
  issue = {1},
  journal = {Journal of the Knowledge Economy},
  language = {en},
  month = {3},
  pages = {44--69},
  publisher = {Springer Science and Business Media LLC},
  title = {Economic Footprint of a Large French Research and Technology Organisation in Europe: Deciphering a Simplified Model and Appraising the Results},
  url = {https://doi.org/10.1007/s13132-020-00709-2},
  volume = {13},
  year = {2022},
}

@article{New_R_D_man_Albors_2010,
  author = {Albors-Garrigos, Jose and Zabaleta, Noemi and Ganzarain, Jaione},
  doi = {10.1111/j.1467-9310.2010.00611.x},
  issue = {5},
  journal = {R\&amp;D Management},
  language = {en},
  month = {11},
  pages = {435--454},
  publisher = {Wiley},
  title = {New R\&amp;D management paradigms: rethinking research and technology organizations strategies in regions},
  url = {https://doi.org/10.1111/j.1467-9310.2010.00611.x},
  volume = {40},
  year = {2010},
}