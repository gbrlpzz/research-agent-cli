2025-12-07 21:31:57,484 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:31:57,485 - INFO - Script started.
2025-12-07 21:31:57,485 - INFO - Starting search for: attention is all you need
2025-12-07 21:31:57,485 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:31:57,485 - DEBUG - Headers: {}
2025-12-07 21:31:57,485 - DEBUG - Payload: None
2025-12-07 21:31:57,485 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:31:57,513 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:31:57,721 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1019e6af0>
2025-12-07 21:31:57,722 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1019c4510> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:31:57,814 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1019d1cd0>
2025-12-07 21:31:57,815 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:31:57,816 - DEBUG - send_request_headers.complete
2025-12-07 21:31:57,816 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:31:57,816 - DEBUG - send_request_body.complete
2025-12-07 21:31:57,816 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:31:58,369 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:31:58 GMT'), (b'x-amz-apigw-id', b'VPCZzGALvHcEUVQ='), (b'x-amzn-RequestId', b'9d13b932-45b4-4dd0-be86-8218f470468a'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 f92e634cae29c615cbe8f686d8333cf6.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'48jFsOcPIgfJyL1kI25mqIhMbCdUums0XtG9G_FNu5p9dB93XBcKTA==')])
2025-12-07 21:31:58,371 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 429 "
2025-12-07 21:31:58,371 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:31:58,372 - DEBUG - receive_response_body.complete
2025-12-07 21:31:58,372 - DEBUG - response_closed.started
2025-12-07 21:31:58,372 - DEBUG - response_closed.complete
2025-12-07 21:31:58,373 - DEBUG - close.started
2025-12-07 21:31:58,373 - DEBUG - close.complete
2025-12-07 21:32:28,375 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:32:28,375 - DEBUG - Headers: {}
2025-12-07 21:32:28,375 - DEBUG - Payload: None
2025-12-07 21:32:28,376 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:32:28,385 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:32:28,514 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101d076d0>
2025-12-07 21:32:28,514 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10174b890> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:32:28,612 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101782220>
2025-12-07 21:32:28,614 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:32:28,614 - DEBUG - send_request_headers.complete
2025-12-07 21:32:28,614 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:32:28,615 - DEBUG - send_request_body.complete
2025-12-07 21:32:28,615 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:32:29,165 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:32:29 GMT'), (b'x-amz-apigw-id', b'VPCenFczPHcEpqA='), (b'x-amzn-RequestId', b'5edfd0a0-2e20-4dd6-b1cf-e779149e139d'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 5d27236a5f6fb9836424f23bb92b0cd0.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'REIHVgwLD7OTRP_HsaWITfli8Gt2CzBKuyYM-YGiKTj3XbtR_5xLVQ==')])
2025-12-07 21:32:29,166 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 429 "
2025-12-07 21:32:29,166 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:32:29,166 - DEBUG - receive_response_body.complete
2025-12-07 21:32:29,167 - DEBUG - response_closed.started
2025-12-07 21:32:29,167 - DEBUG - response_closed.complete
2025-12-07 21:32:29,167 - DEBUG - close.started
2025-12-07 21:32:29,168 - DEBUG - close.complete
2025-12-07 21:32:59,170 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:32:59,170 - DEBUG - Headers: {}
2025-12-07 21:32:59,170 - DEBUG - Payload: None
2025-12-07 21:32:59,170 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:32:59,178 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:32:59,272 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101d18c40>
2025-12-07 21:32:59,272 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10174b900> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:32:59,407 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1019f4880>
2025-12-07 21:32:59,408 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:32:59,409 - DEBUG - send_request_headers.complete
2025-12-07 21:32:59,409 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:32:59,409 - DEBUG - send_request_body.complete
2025-12-07 21:32:59,409 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:33:00,950 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'62409'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:33:00 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:33:00 GMT'), (b'x-amzn-RequestId', b'8f90b91a-514b-4be5-9857-63361beaaf38'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'62409'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPCjbEoLvHcEHLA='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 495353e55c32c4243e459034dba6126a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'lM3cf2RUeX1yJxGtrrFhfecUIVF9Dv_739BgYvSX_Vi_f_ZGWMeFHg==')])
2025-12-07 21:33:00,951 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:33:00,951 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:33:01,116 - DEBUG - receive_response_body.complete
2025-12-07 21:33:01,117 - DEBUG - response_closed.started
2025-12-07 21:33:01,117 - DEBUG - response_closed.complete
2025-12-07 21:33:01,118 - DEBUG - close.started
2025-12-07 21:33:01,118 - DEBUG - close.complete
2025-12-07 21:33:01,125 - INFO - Found 20 results. Preparing FZF.
2025-12-07 21:33:01,126 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=20&limit=20
2025-12-07 21:33:01,126 - DEBUG - Headers: {}
2025-12-07 21:33:01,126 - DEBUG - Payload: None
2025-12-07 21:33:01,126 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=20&limit=20
2025-12-07 21:33:01,132 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:33:01,188 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101d187f0>
2025-12-07 21:33:01,188 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1019f10b0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:33:01,299 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101d1a340>
2025-12-07 21:33:01,300 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:33:01,301 - DEBUG - send_request_headers.complete
2025-12-07 21:33:01,301 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:33:01,301 - DEBUG - send_request_body.complete
2025-12-07 21:33:01,301 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:33:03,390 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'58744'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:33:03 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:33:03 GMT'), (b'x-amzn-RequestId', b'c3ee9742-d772-4b77-853f-ca721fa2928e'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'58744'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPCjuEBwvHcEjZg='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 c0d148503ffeda0d8d54a05acac45e20.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'uCEyzZ3vM1knZZVZD__4WkWI3XmmZOMXg3hwPGEjdCX5-2rp6wYiYQ==')])
2025-12-07 21:33:03,391 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=20&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:33:03,391 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:33:03,495 - DEBUG - receive_response_body.complete
2025-12-07 21:33:03,496 - DEBUG - response_closed.started
2025-12-07 21:33:03,496 - DEBUG - response_closed.complete
2025-12-07 21:33:03,497 - DEBUG - close.started
2025-12-07 21:33:03,497 - DEBUG - close.complete
2025-12-07 21:33:03,500 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=40&limit=20
2025-12-07 21:33:03,500 - DEBUG - Headers: {}
2025-12-07 21:33:03,500 - DEBUG - Payload: None
2025-12-07 21:33:03,501 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=40&limit=20
2025-12-07 21:33:03,507 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:33:03,567 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10176c8b0>
2025-12-07 21:33:03,568 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x101a66c10> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:33:03,690 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10175afd0>
2025-12-07 21:33:03,691 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:33:03,692 - DEBUG - send_request_headers.complete
2025-12-07 21:33:03,692 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:33:03,692 - DEBUG - send_request_body.complete
2025-12-07 21:33:03,692 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:33:03,917 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:33:03 GMT'), (b'x-amz-apigw-id', b'VPCkCFNTvHcEpqA='), (b'x-amzn-RequestId', b'd4d37379-4b6a-4b06-95a2-678b345f6bd3'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 c0d148503ffeda0d8d54a05acac45e20.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'53qDibHQASLLo_7CD3KnJ8nl6OdG7QpoIpNqHW5I4e6ES6i6iCpyMA==')])
2025-12-07 21:33:03,918 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=40&limit=20 "HTTP/1.1 429 "
2025-12-07 21:33:03,918 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:33:03,918 - DEBUG - receive_response_body.complete
2025-12-07 21:33:03,919 - DEBUG - response_closed.started
2025-12-07 21:33:03,919 - DEBUG - response_closed.complete
2025-12-07 21:33:03,919 - DEBUG - close.started
2025-12-07 21:33:03,920 - DEBUG - close.complete
2025-12-07 21:33:33,922 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=40&limit=20
2025-12-07 21:33:33,922 - DEBUG - Headers: {}
2025-12-07 21:33:33,922 - DEBUG - Payload: None
2025-12-07 21:33:33,922 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=40&limit=20
2025-12-07 21:33:33,936 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:33:38,888 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101d7a6d0>
2025-12-07 21:33:38,888 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10174b820> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:33:39,735 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101d18a60>
2025-12-07 21:33:39,737 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:33:39,737 - DEBUG - send_request_headers.complete
2025-12-07 21:33:39,737 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:33:39,738 - DEBUG - send_request_body.complete
2025-12-07 21:33:39,738 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:33:39,981 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:33:39 GMT'), (b'x-amz-apigw-id', b'VPCprGEKPHcEshA='), (b'x-amzn-RequestId', b'16af1dcd-9ac1-4a65-9088-3dc3297343bb'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 15e6afc335dbbf726da2dd95038f862a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'sN6nxQ-CpdQpFnL17ahiZbXAU_PDxpu8F_MFkzXhkVXvUNKOKeKDzA==')])
2025-12-07 21:33:39,982 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=40&limit=20 "HTTP/1.1 429 "
2025-12-07 21:33:39,982 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:33:39,982 - DEBUG - receive_response_body.complete
2025-12-07 21:33:39,983 - DEBUG - response_closed.started
2025-12-07 21:33:39,983 - DEBUG - response_closed.complete
2025-12-07 21:33:39,984 - DEBUG - close.started
2025-12-07 21:33:39,985 - DEBUG - close.complete
2025-12-07 21:34:09,987 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=40&limit=20
2025-12-07 21:34:09,987 - DEBUG - Headers: {}
2025-12-07 21:34:09,987 - DEBUG - Payload: None
2025-12-07 21:34:09,987 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=40&limit=20
2025-12-07 21:34:09,998 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:34:10,101 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101d80d60>
2025-12-07 21:34:10,101 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x101d624a0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:34:21,773 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101d769a0>
2025-12-07 21:34:21,774 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:34:21,775 - DEBUG - send_request_headers.complete
2025-12-07 21:34:21,775 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:34:21,775 - DEBUG - send_request_body.complete
2025-12-07 21:34:21,775 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:34:28,863 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'55354'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:34:28 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:34:28 GMT'), (b'x-amzn-RequestId', b'43627457-d672-4a29-9595-67124e844d28'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'55354'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPCxKGr4PHcEDDA='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 782307cc86daaa076cbdb91c6d06353e.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'l7Ao0Nk_TEYjM7FfbVF5qCNg2UepBcBodnReqwj9BAnF6a4Rt35bGA==')])
2025-12-07 21:34:28,863 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=40&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:34:28,864 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:34:29,032 - DEBUG - receive_response_body.complete
2025-12-07 21:34:29,032 - DEBUG - response_closed.started
2025-12-07 21:34:29,033 - DEBUG - response_closed.complete
2025-12-07 21:34:29,033 - DEBUG - close.started
2025-12-07 21:34:29,034 - DEBUG - close.complete
2025-12-07 21:34:29,036 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=60&limit=20
2025-12-07 21:34:29,037 - DEBUG - Headers: {}
2025-12-07 21:34:29,037 - DEBUG - Payload: None
2025-12-07 21:34:29,037 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=60&limit=20
2025-12-07 21:34:29,043 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:34:29,179 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101d18d30>
2025-12-07 21:34:29,179 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x101d62d60> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:34:34,821 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101d0a910>
2025-12-07 21:34:34,822 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:34:34,823 - DEBUG - send_request_headers.complete
2025-12-07 21:34:34,823 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:34:34,823 - DEBUG - send_request_body.complete
2025-12-07 21:34:34,823 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:34:35,921 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:34:35 GMT'), (b'x-amz-apigw-id', b'VPCyaFO0PHcEAQw='), (b'x-amzn-RequestId', b'b67b5b17-e230-48e7-b7c8-ecef6a3da1ff'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 66a3254753daef98131e391b49752390.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'caXhXa7xivduDMhGSTHw92RBLe0CuLwspfPNCcPYdoj6aWD2Y0YEIQ==')])
2025-12-07 21:34:35,921 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=60&limit=20 "HTTP/1.1 429 "
2025-12-07 21:34:35,921 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:34:35,922 - DEBUG - receive_response_body.complete
2025-12-07 21:34:35,922 - DEBUG - response_closed.started
2025-12-07 21:34:35,922 - DEBUG - response_closed.complete
2025-12-07 21:34:35,923 - DEBUG - close.started
2025-12-07 21:34:35,923 - DEBUG - close.complete
2025-12-07 21:35:05,925 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=60&limit=20
2025-12-07 21:35:05,925 - DEBUG - Headers: {}
2025-12-07 21:35:05,925 - DEBUG - Payload: None
2025-12-07 21:35:05,925 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=60&limit=20
2025-12-07 21:35:05,934 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:35:06,089 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101da8820>
2025-12-07 21:35:06,089 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x101d62dd0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:35:06,202 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101d1a0a0>
2025-12-07 21:35:06,203 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:35:06,203 - DEBUG - send_request_headers.complete
2025-12-07 21:35:06,204 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:35:06,204 - DEBUG - send_request_body.complete
2025-12-07 21:35:06,204 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:35:07,813 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'60385'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:35:07 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:35:07 GMT'), (b'x-amzn-RequestId', b'f60e9811-2a3b-490d-ba1a-28f491dd9b1b'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'60385'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPC3PH5oPHcEX7g='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 fbbaf051f1b6e237fbee09c998e075cc.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'4fAvJ6-rWHEha8Cnm-US95VBEIlJcYtQNseCMlJKlShPP2p1VCrKMg==')])
2025-12-07 21:35:07,814 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=60&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:35:07,814 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:35:07,933 - DEBUG - receive_response_body.complete
2025-12-07 21:35:07,934 - DEBUG - response_closed.started
2025-12-07 21:35:07,934 - DEBUG - response_closed.complete
2025-12-07 21:35:07,935 - DEBUG - close.started
2025-12-07 21:35:07,935 - DEBUG - close.complete
2025-12-07 21:35:07,937 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=80&limit=20
2025-12-07 21:35:07,937 - DEBUG - Headers: {}
2025-12-07 21:35:07,937 - DEBUG - Payload: None
2025-12-07 21:35:07,937 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=80&limit=20
2025-12-07 21:35:07,945 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:35:08,019 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101dd4610>
2025-12-07 21:35:08,019 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x101da9890> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:35:08,133 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101daf8e0>
2025-12-07 21:35:08,135 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:35:08,135 - DEBUG - send_request_headers.complete
2025-12-07 21:35:08,135 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:35:08,135 - DEBUG - send_request_body.complete
2025-12-07 21:35:08,135 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:35:08,383 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:35:08 GMT'), (b'x-amz-apigw-id', b'VPC3fG7BvHcESTg='), (b'x-amzn-RequestId', b'c69c5d0d-7fa4-49a4-807e-6b549099bc44'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 92d05088b4c5dd8daf39fac07c1c6b00.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'ARg5lHDF-P_a3c3ychBdtby9jZFHQ0HRbcHYEJoNXwMUXToRlHwtcw==')])
2025-12-07 21:35:08,384 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=80&limit=20 "HTTP/1.1 429 "
2025-12-07 21:35:08,384 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:35:08,384 - DEBUG - receive_response_body.complete
2025-12-07 21:35:08,384 - DEBUG - response_closed.started
2025-12-07 21:35:08,385 - DEBUG - response_closed.complete
2025-12-07 21:35:08,385 - DEBUG - close.started
2025-12-07 21:35:08,385 - DEBUG - close.complete
2025-12-07 21:35:38,387 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=80&limit=20
2025-12-07 21:35:38,388 - DEBUG - Headers: {}
2025-12-07 21:35:38,388 - DEBUG - Payload: None
2025-12-07 21:35:38,388 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=80&limit=20
2025-12-07 21:35:38,399 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:35:38,483 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101dd8c10>
2025-12-07 21:35:38,483 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x101da9900> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:35:38,642 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101dd4f10>
2025-12-07 21:35:38,643 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:35:38,643 - DEBUG - send_request_headers.complete
2025-12-07 21:35:38,644 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:35:38,644 - DEBUG - send_request_body.complete
2025-12-07 21:35:38,644 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:35:41,604 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'65450'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:35:41 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:35:41 GMT'), (b'x-amzn-RequestId', b'b012a124-910b-4ba9-b480-018a464bf543'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'65450'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPC8RFS2vHcEh5w='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2f9d40e1286737e2a1a91819dee481a8.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'T8lQOp-w53CZY-AD2An4IGYEsUKgSmijHlHyX-YXh2ICWB5l27Tf6g==')])
2025-12-07 21:35:41,605 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=80&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:35:41,605 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:35:41,767 - DEBUG - receive_response_body.complete
2025-12-07 21:35:41,768 - DEBUG - response_closed.started
2025-12-07 21:35:41,768 - DEBUG - response_closed.complete
2025-12-07 21:35:41,769 - DEBUG - close.started
2025-12-07 21:35:41,769 - DEBUG - close.complete
2025-12-07 21:35:41,772 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=100&limit=20
2025-12-07 21:35:41,772 - DEBUG - Headers: {}
2025-12-07 21:35:41,772 - DEBUG - Payload: None
2025-12-07 21:35:41,772 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=100&limit=20
2025-12-07 21:35:41,779 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:35:41,818 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101dd8940>
2025-12-07 21:35:41,818 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x101da9ba0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:35:41,943 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101ddea30>
2025-12-07 21:35:41,944 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:35:41,944 - DEBUG - send_request_headers.complete
2025-12-07 21:35:41,945 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:35:41,945 - DEBUG - send_request_body.complete
2025-12-07 21:35:41,945 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:35:45,597 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'66587'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:35:45 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:35:45 GMT'), (b'x-amzn-RequestId', b'e3b9de92-400c-45d6-b1c6-842ab91b43fe'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'66587'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPC80EiRvHcECwQ='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 946a47118939fc2908c7ce8e46e8d16c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'r4DMZbQ7ApcGWSd_0l4q44V9XdZOSHNciP0kDaUkwHBdWLoaNP6ufw==')])
2025-12-07 21:35:45,598 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=100&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:35:45,598 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:35:45,767 - DEBUG - receive_response_body.complete
2025-12-07 21:35:45,768 - DEBUG - response_closed.started
2025-12-07 21:35:45,768 - DEBUG - response_closed.complete
2025-12-07 21:35:45,768 - DEBUG - close.started
2025-12-07 21:35:45,769 - DEBUG - close.complete
2025-12-07 21:35:45,772 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=120&limit=20
2025-12-07 21:35:45,772 - DEBUG - Headers: {}
2025-12-07 21:35:45,772 - DEBUG - Payload: None
2025-12-07 21:35:45,772 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=120&limit=20
2025-12-07 21:35:45,779 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:35:45,821 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104905d60>
2025-12-07 21:35:45,822 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x101da9dd0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:35:45,965 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101dd81f0>
2025-12-07 21:35:45,966 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:35:45,967 - DEBUG - send_request_headers.complete
2025-12-07 21:35:45,967 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:35:45,967 - DEBUG - send_request_body.complete
2025-12-07 21:35:45,967 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:35:47,653 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'61753'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:35:47 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:35:47 GMT'), (b'x-amzn-RequestId', b'7d253d0d-26c9-43a0-a868-7f4b14b557b8'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'61753'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPC9dFp1PHcEkCg='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 495353e55c32c4243e459034dba6126a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'OlKRCHrjC3vMpCCu1qDKdRLY3MkoL9C72O307xJUeNy3R55vANYNkw==')])
2025-12-07 21:35:47,654 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=120&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:35:47,655 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:35:47,825 - DEBUG - receive_response_body.complete
2025-12-07 21:35:47,826 - DEBUG - response_closed.started
2025-12-07 21:35:47,826 - DEBUG - response_closed.complete
2025-12-07 21:35:47,827 - DEBUG - close.started
2025-12-07 21:35:47,827 - DEBUG - close.complete
2025-12-07 21:35:47,830 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=140&limit=20
2025-12-07 21:35:47,830 - DEBUG - Headers: {}
2025-12-07 21:35:47,830 - DEBUG - Payload: None
2025-12-07 21:35:47,830 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=140&limit=20
2025-12-07 21:35:47,837 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:35:47,890 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104956e80>
2025-12-07 21:35:47,891 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x101da9dd0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:35:48,022 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101dd8d00>
2025-12-07 21:35:48,023 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:35:48,023 - DEBUG - send_request_headers.complete
2025-12-07 21:35:48,023 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:35:48,023 - DEBUG - send_request_body.complete
2025-12-07 21:35:48,023 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:35:49,688 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'62189'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:35:49 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:35:49 GMT'), (b'x-amzn-RequestId', b'52c4273e-c79c-4602-ad3d-4cf369027b16'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'62189'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPC9xEzZPHcEUMQ='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 fd1922963c91836bd3bc6df1671c0612.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'X7sBZiG4mPYmMIQDxeOQ2RWDd76sDCK35s_Fhip-HT5PQMKYXiCdMw==')])
2025-12-07 21:35:49,689 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=140&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:35:49,689 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:35:49,857 - DEBUG - receive_response_body.complete
2025-12-07 21:35:49,858 - DEBUG - response_closed.started
2025-12-07 21:35:49,859 - DEBUG - response_closed.complete
2025-12-07 21:35:49,859 - DEBUG - close.started
2025-12-07 21:35:49,860 - DEBUG - close.complete
2025-12-07 21:35:49,862 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=160&limit=20
2025-12-07 21:35:49,862 - DEBUG - Headers: {}
2025-12-07 21:35:49,862 - DEBUG - Payload: None
2025-12-07 21:35:49,862 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=160&limit=20
2025-12-07 21:35:49,869 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:35:49,936 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10498e610>
2025-12-07 21:35:49,936 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1049479e0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:35:50,086 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104987880>
2025-12-07 21:35:50,087 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:35:50,087 - DEBUG - send_request_headers.complete
2025-12-07 21:35:50,087 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:35:50,087 - DEBUG - send_request_body.complete
2025-12-07 21:35:50,087 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:35:50,372 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:35:50 GMT'), (b'x-amz-apigw-id', b'VPC-DE4IvHcEpZA='), (b'x-amzn-RequestId', b'ec9770f2-696b-4deb-8200-116b89ef07e8'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 b61152e740e230075fb9605e656b9520.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'6_sVx-5MAS9Q_VTvH0AKOM2vA2Y5O9Bf1jWYbIyuWsf5bLsSb5GpIQ==')])
2025-12-07 21:35:50,373 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=160&limit=20 "HTTP/1.1 429 "
2025-12-07 21:35:50,373 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:35:50,373 - DEBUG - receive_response_body.complete
2025-12-07 21:35:50,374 - DEBUG - response_closed.started
2025-12-07 21:35:50,374 - DEBUG - response_closed.complete
2025-12-07 21:35:50,375 - DEBUG - close.started
2025-12-07 21:35:50,375 - DEBUG - close.complete
2025-12-07 21:36:20,377 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=160&limit=20
2025-12-07 21:36:20,377 - DEBUG - Headers: {}
2025-12-07 21:36:20,377 - DEBUG - Payload: None
2025-12-07 21:36:20,377 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=160&limit=20
2025-12-07 21:36:20,390 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:36:20,499 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104994c10>
2025-12-07 21:36:20,499 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x104947a50> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:36:20,690 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10498ef10>
2025-12-07 21:36:20,691 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:36:20,691 - DEBUG - send_request_headers.complete
2025-12-07 21:36:20,691 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:36:20,691 - DEBUG - send_request_body.complete
2025-12-07 21:36:20,691 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:36:21,332 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:36:21 GMT'), (b'x-amz-apigw-id', b'VPDC4HEwvHcEkNA='), (b'x-amzn-RequestId', b'd88d7f14-2038-4628-8e72-89d9315fcf75'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 61e929f79f2623281d7e3a91da354352.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'Q143WLSzCZOs-KS8k0hauCHHmqXN7_heX09R1xj3395vdkJ_RF6yxQ==')])
2025-12-07 21:36:21,333 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=160&limit=20 "HTTP/1.1 429 "
2025-12-07 21:36:21,333 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:36:21,334 - DEBUG - receive_response_body.complete
2025-12-07 21:36:21,334 - DEBUG - response_closed.started
2025-12-07 21:36:21,334 - DEBUG - response_closed.complete
2025-12-07 21:36:21,335 - DEBUG - close.started
2025-12-07 21:36:21,336 - DEBUG - close.complete
2025-12-07 21:36:51,338 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=160&limit=20
2025-12-07 21:36:51,338 - DEBUG - Headers: {}
2025-12-07 21:36:51,338 - DEBUG - Payload: None
2025-12-07 21:36:51,338 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=160&limit=20
2025-12-07 21:36:51,346 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:36:51,508 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104994610>
2025-12-07 21:36:51,508 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x104947ac0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:36:56,809 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104987a00>
2025-12-07 21:36:56,810 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:36:56,811 - DEBUG - send_request_headers.complete
2025-12-07 21:36:56,811 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:36:56,811 - DEBUG - send_request_body.complete
2025-12-07 21:36:56,811 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:36:58,418 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'44085'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:36:57 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:36:57 GMT'), (b'x-amzn-RequestId', b'eadd7c22-12ee-45b9-a5bd-58c1d7f26cfd'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'44085'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPDIeFUdvHcEnSA='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 15e6afc335dbbf726da2dd95038f862a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'JdNB3oRMLBjHpbMlHJJ-nn7zQuzMah8RtXB5MmZnkXLDpZoaJzAwFQ==')])
2025-12-07 21:36:58,420 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=160&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:36:58,420 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:36:58,607 - DEBUG - receive_response_body.complete
2025-12-07 21:36:58,608 - DEBUG - response_closed.started
2025-12-07 21:36:58,608 - DEBUG - response_closed.complete
2025-12-07 21:36:58,609 - DEBUG - close.started
2025-12-07 21:36:58,609 - DEBUG - close.complete
2025-12-07 21:36:58,611 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=180&limit=20
2025-12-07 21:36:58,611 - DEBUG - Headers: {}
2025-12-07 21:36:58,611 - DEBUG - Payload: None
2025-12-07 21:36:58,611 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=180&limit=20
2025-12-07 21:36:58,620 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:36:58,949 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104996a30>
2025-12-07 21:36:58,950 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1049b20b0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:36:59,587 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1049969d0>
2025-12-07 21:36:59,587 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:36:59,588 - DEBUG - send_request_headers.complete
2025-12-07 21:36:59,588 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:36:59,588 - DEBUG - send_request_body.complete
2025-12-07 21:36:59,588 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:37:02,061 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'66890'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:37:01 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:37:01 GMT'), (b'x-amzn-RequestId', b'4b7cb682-d933-4a6c-a07c-e29e6650a797'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'66890'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPDI_GoVvHcEYtw='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 b238d3f6f579ec0d467edb5df6f43bbe.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'12Ae88O3FVmuE5GQZI8EtD5U6ljrKX_95643uiX6cKVGwe5GkB5jQg==')])
2025-12-07 21:37:02,062 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=180&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:37:02,062 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:37:02,185 - DEBUG - receive_response_body.complete
2025-12-07 21:37:02,186 - DEBUG - response_closed.started
2025-12-07 21:37:02,186 - DEBUG - response_closed.complete
2025-12-07 21:37:02,187 - DEBUG - close.started
2025-12-07 21:37:02,188 - DEBUG - close.complete
2025-12-07 21:37:02,189 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=200&limit=20
2025-12-07 21:37:02,190 - DEBUG - Headers: {}
2025-12-07 21:37:02,190 - DEBUG - Payload: None
2025-12-07 21:37:02,190 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=200&limit=20
2025-12-07 21:37:02,197 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:37:02,426 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1049bc460>
2025-12-07 21:37:02,427 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1049b24a0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:37:04,225 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1049bc3d0>
2025-12-07 21:37:04,226 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:37:04,226 - DEBUG - send_request_headers.complete
2025-12-07 21:37:04,227 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:37:04,227 - DEBUG - send_request_body.complete
2025-12-07 21:37:04,227 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:37:07,019 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'55286'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:37:06 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:37:06 GMT'), (b'x-amzn-RequestId', b'd43f84c7-68d2-4680-a9bd-36772dd3a0e9'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'55286'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPDJzG9aPHcEiZg='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2cb4f544b3021873f01656b0bfa9897a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'xKFS73Cmf9psU9EmCSsTBYnhv0bScpSDITS6Uz7ElcrOJzS9HjtC4w==')])
2025-12-07 21:37:07,020 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=200&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:37:07,020 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:37:07,374 - DEBUG - receive_response_body.complete
2025-12-07 21:37:07,375 - DEBUG - response_closed.started
2025-12-07 21:37:07,375 - DEBUG - response_closed.complete
2025-12-07 21:37:07,376 - DEBUG - close.started
2025-12-07 21:37:07,376 - DEBUG - close.complete
2025-12-07 21:37:07,378 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=220&limit=20
2025-12-07 21:37:07,378 - DEBUG - Headers: {}
2025-12-07 21:37:07,378 - DEBUG - Payload: None
2025-12-07 21:37:07,378 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=220&limit=20
2025-12-07 21:37:07,384 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:37:07,651 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104a01ca0>
2025-12-07 21:37:07,651 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1049b29e0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:37:11,768 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104a01ee0>
2025-12-07 21:37:11,769 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:37:11,769 - DEBUG - send_request_headers.complete
2025-12-07 21:37:11,769 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:37:11,770 - DEBUG - send_request_body.complete
2025-12-07 21:37:11,770 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:37:12,937 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:37:12 GMT'), (b'x-amz-apigw-id', b'VPDK9He5PHcEOpA='), (b'x-amzn-RequestId', b'004ccaec-f0b8-481a-89e8-800c484046c7'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 495353e55c32c4243e459034dba6126a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'Q1SRuoz9LFqujyoUYl9qXYO7bFQOEyZ1ceFT1LTfmkZvi5xYfAGzTw==')])
2025-12-07 21:37:12,938 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=220&limit=20 "HTTP/1.1 429 "
2025-12-07 21:37:12,938 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:37:12,938 - DEBUG - receive_response_body.complete
2025-12-07 21:37:12,938 - DEBUG - response_closed.started
2025-12-07 21:37:12,939 - DEBUG - response_closed.complete
2025-12-07 21:37:12,939 - DEBUG - close.started
2025-12-07 21:37:12,939 - DEBUG - close.complete
2025-12-07 21:37:42,941 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=220&limit=20
2025-12-07 21:37:42,942 - DEBUG - Headers: {}
2025-12-07 21:37:42,942 - DEBUG - Payload: None
2025-12-07 21:37:42,942 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=220&limit=20
2025-12-07 21:37:42,952 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:37:43,100 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1049948b0>
2025-12-07 21:37:43,100 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1049b2a50> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:37:43,217 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104a01100>
2025-12-07 21:37:43,219 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:37:43,220 - DEBUG - send_request_headers.complete
2025-12-07 21:37:43,220 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:37:43,220 - DEBUG - send_request_body.complete
2025-12-07 21:37:43,220 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:37:44,690 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'46384'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:37:44 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:37:44 GMT'), (b'x-amzn-RequestId', b'efd3d537-c562-4dcb-8e3a-d42caec078ec'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'46384'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPDPuHghvHcElqA='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 495353e55c32c4243e459034dba6126a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'Oe9UPiw9K395d2exHiiqvo8ZYZnz2nvJ9a6e0T09VZh22hnF4KWAJw==')])
2025-12-07 21:37:44,691 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=220&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:37:44,691 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:37:44,823 - DEBUG - receive_response_body.complete
2025-12-07 21:37:44,824 - DEBUG - response_closed.started
2025-12-07 21:37:44,825 - DEBUG - response_closed.complete
2025-12-07 21:37:44,826 - DEBUG - close.started
2025-12-07 21:37:44,826 - DEBUG - close.complete
2025-12-07 21:37:44,828 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=240&limit=20
2025-12-07 21:37:44,828 - DEBUG - Headers: {}
2025-12-07 21:37:44,828 - DEBUG - Payload: None
2025-12-07 21:37:44,828 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=240&limit=20
2025-12-07 21:37:44,834 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:37:44,883 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10495ec70>
2025-12-07 21:37:44,884 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x101d62dd0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:37:45,004 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101daf1f0>
2025-12-07 21:37:45,005 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:37:45,006 - DEBUG - send_request_headers.complete
2025-12-07 21:37:45,006 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:37:45,006 - DEBUG - send_request_body.complete
2025-12-07 21:37:45,006 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:37:45,262 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:37:45 GMT'), (b'x-amz-apigw-id', b'VPDQAEsdPHcENsQ='), (b'x-amzn-RequestId', b'd1453f6d-7348-4cf8-b87c-bbc22e5f4eb7'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 4d4b96028acf63781fb210bb3b43b4f4.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'luQQrJU2U_LT1YjJOdSlyeQ9z6bYOw2zkUGT575nQA1gvFRO9_jVBw==')])
2025-12-07 21:37:45,263 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=240&limit=20 "HTTP/1.1 429 "
2025-12-07 21:37:45,264 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:37:45,264 - DEBUG - receive_response_body.complete
2025-12-07 21:37:45,265 - DEBUG - response_closed.started
2025-12-07 21:37:45,265 - DEBUG - response_closed.complete
2025-12-07 21:37:45,266 - DEBUG - close.started
2025-12-07 21:37:45,266 - DEBUG - close.complete
2025-12-07 21:38:15,268 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=240&limit=20
2025-12-07 21:38:15,268 - DEBUG - Headers: {}
2025-12-07 21:38:15,268 - DEBUG - Payload: None
2025-12-07 21:38:15,268 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=240&limit=20
2025-12-07 21:38:15,275 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:38:15,364 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1049ddeb0>
2025-12-07 21:38:15,365 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x101d62f90> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:38:17,563 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1049bc700>
2025-12-07 21:38:17,564 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:38:17,564 - DEBUG - send_request_headers.complete
2025-12-07 21:38:17,565 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:38:17,565 - DEBUG - send_request_body.complete
2025-12-07 21:38:17,565 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:38:19,812 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'44188'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:38:19 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:38:19 GMT'), (b'x-amzn-RequestId', b'1db433ef-a88e-4950-8d6a-6df937016d1d'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'44188'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPDVOHq9vHcEtCA='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 861c9a33ccdd7a6a61b188ea139dd8fa.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'mJNYWq4vAajK96l-hbXjoXzSJFJoVuELQJW0g-PimXLevqWPvprEJA==')])
2025-12-07 21:38:19,812 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=240&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:38:19,813 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:38:19,942 - DEBUG - receive_response_body.complete
2025-12-07 21:38:19,942 - DEBUG - response_closed.started
2025-12-07 21:38:19,943 - DEBUG - response_closed.complete
2025-12-07 21:38:19,943 - DEBUG - close.started
2025-12-07 21:38:19,944 - DEBUG - close.complete
2025-12-07 21:38:19,945 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=260&limit=20
2025-12-07 21:38:19,945 - DEBUG - Headers: {}
2025-12-07 21:38:19,945 - DEBUG - Payload: None
2025-12-07 21:38:19,945 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=260&limit=20
2025-12-07 21:38:19,952 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:38:20,012 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104a01c10>
2025-12-07 21:38:20,012 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x104a1f3c0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:38:20,133 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x101d87d00>
2025-12-07 21:38:20,134 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:38:20,134 - DEBUG - send_request_headers.complete
2025-12-07 21:38:20,134 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:38:20,134 - DEBUG - send_request_body.complete
2025-12-07 21:38:20,134 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:38:20,684 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:38:20 GMT'), (b'x-amz-apigw-id', b'VPDViFfNPHcEPiA='), (b'x-amzn-RequestId', b'2aa6ca6e-6844-46a3-82e0-46707aecf269'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 66a3254753daef98131e391b49752390.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'lDr_x9MBrKBdj0Cqz0R0k91D-3H4QTDlOJc9K_ZuZt_ABUwbmainEQ==')])
2025-12-07 21:38:20,685 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=260&limit=20 "HTTP/1.1 429 "
2025-12-07 21:38:20,685 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:38:20,685 - DEBUG - receive_response_body.complete
2025-12-07 21:38:20,685 - DEBUG - response_closed.started
2025-12-07 21:38:20,685 - DEBUG - response_closed.complete
2025-12-07 21:38:20,686 - DEBUG - close.started
2025-12-07 21:38:20,686 - DEBUG - close.complete
2025-12-07 21:38:44,506 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:38:44,506 - INFO - Script started.
2025-12-07 21:38:44,506 - INFO - Starting search for: attention is all you need
2025-12-07 21:38:44,507 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:38:44,507 - DEBUG - Headers: {}
2025-12-07 21:38:44,507 - DEBUG - Payload: None
2025-12-07 21:38:44,507 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:38:44,534 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:38:44,608 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10633cd30>
2025-12-07 21:38:44,608 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10631c580> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:38:44,737 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1060d9ca0>
2025-12-07 21:38:44,738 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:38:44,738 - DEBUG - send_request_headers.complete
2025-12-07 21:38:44,738 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:38:44,738 - DEBUG - send_request_body.complete
2025-12-07 21:38:44,738 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:38:46,436 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'61299'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:38:46 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:38:46 GMT'), (b'x-amzn-RequestId', b'f885d266-910b-4361-af59-608f7bbe753c'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'61299'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPDZYEJqvHcETNg='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 e502901885892afe656ee1a0e9f33d48.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'FeQYqhgHOl1XIIQbYixGSkLTzt3DQruowaFYcyy3pdnX1wM9H_XiFA==')])
2025-12-07 21:38:46,438 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:38:46,438 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:38:46,510 - DEBUG - receive_response_body.complete
2025-12-07 21:38:46,511 - DEBUG - response_closed.started
2025-12-07 21:38:46,511 - DEBUG - response_closed.complete
2025-12-07 21:38:46,512 - DEBUG - close.started
2025-12-07 21:38:46,512 - DEBUG - close.complete
2025-12-07 21:38:46,520 - INFO - Search returned (generator). Processing first 20 results.
2025-12-07 21:38:46,520 - INFO - Invoking FZF subprocess.
2025-12-07 21:39:00,300 - INFO - FZF finished. Parsing selections.
2025-12-07 21:39:00,306 - INFO - Selected 1 papers: ['https://arxiv.org/abs/1706.03762']
2025-12-07 21:39:00,308 - INFO - Entering add_to_library loop.
2025-12-07 21:39:00,310 - INFO - Adding URL: https://arxiv.org/abs/1706.03762
2025-12-07 21:39:00,310 - DEBUG - Executing: /Users/gabriele/Documents/GitHub/research/.venv/bin/papis add --from-url https://arxiv.org/abs/1706.03762
2025-12-07 21:39:00,479 - ERROR - CalledProcessError for https://arxiv.org/abs/1706.03762: [ERROR] commands.default: Library 'papers' does not have any folders attached to it. Please create and add the required paths to the configuration file.
Usage: papis add [OPTIONS] [FILES]...

Error: No such option: --from-url Did you mean --from?

2025-12-07 21:39:00,480 - INFO - Starting search for: attention is all you need
2025-12-07 21:39:00,480 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:39:00,480 - DEBUG - Headers: {}
2025-12-07 21:39:00,480 - DEBUG - Payload: None
2025-12-07 21:39:00,480 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:39:00,483 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:39:00,658 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1063df6a0>
2025-12-07 21:39:00,658 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10631c4a0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:39:00,803 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10633c040>
2025-12-07 21:39:00,804 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:39:00,805 - DEBUG - send_request_headers.complete
2025-12-07 21:39:00,805 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:39:00,805 - DEBUG - send_request_body.complete
2025-12-07 21:39:00,805 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:39:01,389 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:39:01 GMT'), (b'x-amz-apigw-id', b'VPDb5Hp4PHcEQqw='), (b'x-amzn-RequestId', b'fe7af12c-089e-463a-9b64-042f4c7e3296'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 861c9a33ccdd7a6a61b188ea139dd8fa.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'woQD0dPrs6YvU4_KVQlvXnoZdlGwBIOc3iwmUooD3cud8x-D3zQG1Q==')])
2025-12-07 21:39:01,390 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 429 "
2025-12-07 21:39:01,391 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:39:01,391 - DEBUG - receive_response_body.complete
2025-12-07 21:39:01,392 - DEBUG - response_closed.started
2025-12-07 21:39:01,392 - DEBUG - response_closed.complete
2025-12-07 21:39:01,393 - DEBUG - close.started
2025-12-07 21:39:01,393 - DEBUG - close.complete
2025-12-07 21:39:31,395 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:39:31,395 - DEBUG - Headers: {}
2025-12-07 21:39:31,396 - DEBUG - Payload: None
2025-12-07 21:39:31,396 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:39:31,403 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:39:31,540 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10661d6a0>
2025-12-07 21:39:31,540 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1060b67b0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:39:31,687 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1063df970>
2025-12-07 21:39:31,688 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:39:31,689 - DEBUG - send_request_headers.complete
2025-12-07 21:39:31,689 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:39:31,689 - DEBUG - send_request_body.complete
2025-12-07 21:39:31,689 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:39:33,292 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'62409'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:39:33 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:39:33 GMT'), (b'x-amzn-RequestId', b'3b0cf439-371f-42f2-a0b9-a5ba915ca2f7'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'62409'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPDgrGILPHcEU3Q='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 946a47118939fc2908c7ce8e46e8d16c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'tbZWe6Z9NC3eGC9SNNEkUSQ4IeDDK24n3nyP95tg2kG1QqZzRHS-7A==')])
2025-12-07 21:39:33,292 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:39:33,293 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:39:33,815 - DEBUG - receive_response_body.complete
2025-12-07 21:39:33,815 - DEBUG - response_closed.started
2025-12-07 21:39:33,815 - DEBUG - response_closed.complete
2025-12-07 21:39:33,816 - DEBUG - close.started
2025-12-07 21:39:33,816 - DEBUG - close.complete
2025-12-07 21:39:33,818 - INFO - Search returned (generator). Processing first 20 results.
2025-12-07 21:39:33,818 - INFO - Invoking FZF subprocess.
2025-12-07 21:39:36,846 - INFO - FZF finished. Parsing selections.
2025-12-07 21:39:36,847 - INFO - Selected 1 papers: ['https://arxiv.org/abs/1706.03762']
2025-12-07 21:39:36,848 - INFO - Entering add_to_library loop.
2025-12-07 21:39:36,850 - INFO - Adding URL: https://arxiv.org/abs/1706.03762
2025-12-07 21:39:36,850 - DEBUG - Executing: /Users/gabriele/Documents/GitHub/research/.venv/bin/papis add --from-url https://arxiv.org/abs/1706.03762
2025-12-07 21:39:36,983 - ERROR - CalledProcessError for https://arxiv.org/abs/1706.03762: [ERROR] commands.default: Library 'papers' does not have any folders attached to it. Please create and add the required paths to the configuration file.
Usage: papis add [OPTIONS] [FILES]...

Error: No such option: --from-url Did you mean --from?

2025-12-07 21:44:42,997 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:42,997 - INFO - Script started.
2025-12-07 21:44:42,997 - INFO - Starting search for: attention is all you need
2025-12-07 21:44:42,998 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:44:42,998 - DEBUG - Headers: {}
2025-12-07 21:44:42,998 - DEBUG - Payload: None
2025-12-07 21:44:42,998 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:44:43,027 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:44:43,179 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x105f0eb80>
2025-12-07 21:44:43,179 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x105cdd7b0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:44:43,291 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x105ce7d60>
2025-12-07 21:44:43,291 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:44:43,291 - DEBUG - send_request_headers.complete
2025-12-07 21:44:43,291 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:44:43,291 - DEBUG - send_request_body.complete
2025-12-07 21:44:43,291 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:44:45,042 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'62409'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:44:44 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:44:44 GMT'), (b'x-amzn-RequestId', b'574da4fe-22ae-4838-9c0b-4b8293df5c1e'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'62409'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPERaHyEvHcEYXg='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 b238d3f6f579ec0d467edb5df6f43bbe.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'03BGyji2ZLLBnvjOQsw60dbM5jECNDlbKRSyBpmKRxdVSz5_yiLmKQ==')])
2025-12-07 21:44:45,044 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:44:45,044 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:44:45,135 - DEBUG - receive_response_body.complete
2025-12-07 21:44:45,136 - DEBUG - response_closed.started
2025-12-07 21:44:45,136 - DEBUG - response_closed.complete
2025-12-07 21:44:45,137 - DEBUG - close.started
2025-12-07 21:44:45,137 - DEBUG - close.complete
2025-12-07 21:44:45,144 - INFO - Search returned (generator). Processing first 20 results.
2025-12-07 21:44:45,145 - INFO - Invoking FZF subprocess with preview.
2025-12-07 21:44:45,273 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:45,273 - INFO - Script started.
2025-12-07 21:44:47,487 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:47,487 - INFO - Script started.
2025-12-07 21:44:48,005 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:48,005 - INFO - Script started.
2025-12-07 21:44:48,213 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:48,213 - INFO - Script started.
2025-12-07 21:44:48,361 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:48,361 - INFO - Script started.
2025-12-07 21:44:48,543 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:48,543 - INFO - Script started.
2025-12-07 21:44:48,707 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:48,707 - INFO - Script started.
2025-12-07 21:44:48,873 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:48,873 - INFO - Script started.
2025-12-07 21:44:49,069 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:49,069 - INFO - Script started.
2025-12-07 21:44:49,217 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:49,217 - INFO - Script started.
2025-12-07 21:44:49,381 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:49,381 - INFO - Script started.
2025-12-07 21:44:49,548 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:49,548 - INFO - Script started.
2025-12-07 21:44:49,726 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:49,726 - INFO - Script started.
2025-12-07 21:44:52,462 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:52,462 - INFO - Script started.
2025-12-07 21:44:52,967 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:52,967 - INFO - Script started.
2025-12-07 21:44:53,049 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:53,049 - INFO - Script started.
2025-12-07 21:44:53,130 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:53,130 - INFO - Script started.
2025-12-07 21:44:53,211 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:53,211 - INFO - Script started.
2025-12-07 21:44:53,292 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:53,292 - INFO - Script started.
2025-12-07 21:44:53,373 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:53,373 - INFO - Script started.
2025-12-07 21:44:53,454 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:53,454 - INFO - Script started.
2025-12-07 21:44:53,535 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:53,535 - INFO - Script started.
2025-12-07 21:44:53,616 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:53,616 - INFO - Script started.
2025-12-07 21:44:53,697 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:53,697 - INFO - Script started.
2025-12-07 21:44:55,936 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:55,937 - INFO - Script started.
2025-12-07 21:44:57,456 - INFO - FZF finished. Parsing selections.
2025-12-07 21:44:57,461 - INFO - Selected 1 papers: ['https://arxiv.org/abs/1706.03762']
2025-12-07 21:44:57,463 - INFO - Entering add_to_library loop.
2025-12-07 21:44:57,465 - INFO - Adding URL: https://arxiv.org/abs/1706.03762
2025-12-07 21:44:57,465 - DEBUG - Executing: /Users/gabriele/Documents/GitHub/research/.venv/bin/papis add --from https://arxiv.org/abs/1706.03762 --lib main
2025-12-07 21:44:57,614 - ERROR - CalledProcessError for https://arxiv.org/abs/1706.03762: [ERROR] commands.default: Library 'papers' does not have any folders attached to it. Please create and add the required paths to the configuration file.
Usage: papis add [OPTIONS] [FILES]...

Error: Invalid value for '--from': 'https://arxiv.org/abs/1706.03762' is not one of 'arxiv', 'bibtex', 'crossref', 'dblp', 'doi', 'folder', 'isbn', 'lib', 'pdf2arxivid', 'pdf2doi', 'pmid', 'yaml', 'zenodo'.

2025-12-07 21:44:57,616 - INFO - Starting search for: attention is all you need
2025-12-07 21:44:57,616 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:44:57,616 - DEBUG - Headers: {}
2025-12-07 21:44:57,616 - DEBUG - Payload: None
2025-12-07 21:44:57,616 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:44:57,619 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:44:57,700 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x105a88460>
2025-12-07 21:44:57,700 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x105a5aac0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:44:57,817 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x105f4d8b0>
2025-12-07 21:44:57,817 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:44:57,818 - DEBUG - send_request_headers.complete
2025-12-07 21:44:57,818 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:44:57,818 - DEBUG - send_request_body.complete
2025-12-07 21:44:57,818 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:44:59,378 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'61299'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:44:59 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:44:59 GMT'), (b'x-amzn-RequestId', b'5ff9c59c-a589-4b27-9b52-3f5ba0b8507e'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'61299'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPETrHPbvHcEJ2g='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 92d05088b4c5dd8daf39fac07c1c6b00.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'EaKbuSON-MUA-zasOgjdES8Zhu5bTcx8vfApP2ENV_ow8JRBfkLn7w==')])
2025-12-07 21:44:59,379 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:44:59,379 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:44:59,531 - DEBUG - receive_response_body.complete
2025-12-07 21:44:59,532 - DEBUG - response_closed.started
2025-12-07 21:44:59,532 - DEBUG - response_closed.complete
2025-12-07 21:44:59,533 - DEBUG - close.started
2025-12-07 21:44:59,533 - DEBUG - close.complete
2025-12-07 21:44:59,536 - INFO - Search returned (generator). Processing first 20 results.
2025-12-07 21:44:59,537 - INFO - Invoking FZF subprocess with preview.
2025-12-07 21:44:59,659 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:44:59,659 - INFO - Script started.
2025-12-07 21:45:03,995 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:45:03,995 - INFO - Script started.
2025-12-07 21:45:04,681 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:45:04,682 - INFO - Script started.
2025-12-07 21:45:05,194 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:45:05,194 - INFO - Script started.
2025-12-07 21:45:06,037 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:45:06,037 - INFO - Script started.
2025-12-07 21:45:06,542 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:45:06,542 - INFO - Script started.
2025-12-07 21:45:06,750 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:45:06,750 - INFO - Script started.
2025-12-07 21:45:09,150 - INFO - FZF finished. Parsing selections.
2025-12-07 21:45:09,152 - INFO - Selected 3 papers: ['https://arxiv.org/abs/1706.03762', 'https://arxiv.org/abs/2010.13154', 'https://doi.org/10.1609/AAAI.V34I07.6693']
2025-12-07 21:45:09,153 - INFO - Entering add_to_library loop.
2025-12-07 21:45:09,156 - INFO - Adding URL: https://arxiv.org/abs/1706.03762
2025-12-07 21:45:09,156 - DEBUG - Executing: /Users/gabriele/Documents/GitHub/research/.venv/bin/papis add --from https://arxiv.org/abs/1706.03762 --lib main
2025-12-07 21:45:09,283 - ERROR - CalledProcessError for https://arxiv.org/abs/1706.03762: [ERROR] commands.default: Library 'papers' does not have any folders attached to it. Please create and add the required paths to the configuration file.
Usage: papis add [OPTIONS] [FILES]...

Error: Invalid value for '--from': 'https://arxiv.org/abs/1706.03762' is not one of 'arxiv', 'bibtex', 'crossref', 'dblp', 'doi', 'folder', 'isbn', 'lib', 'pdf2arxivid', 'pdf2doi', 'pmid', 'yaml', 'zenodo'.

2025-12-07 21:45:09,284 - INFO - Adding URL: https://arxiv.org/abs/2010.13154
2025-12-07 21:45:09,284 - DEBUG - Executing: /Users/gabriele/Documents/GitHub/research/.venv/bin/papis add --from https://arxiv.org/abs/2010.13154 --lib main
2025-12-07 21:45:09,382 - ERROR - CalledProcessError for https://arxiv.org/abs/2010.13154: [ERROR] commands.default: Library 'papers' does not have any folders attached to it. Please create and add the required paths to the configuration file.
Usage: papis add [OPTIONS] [FILES]...

Error: Invalid value for '--from': 'https://arxiv.org/abs/2010.13154' is not one of 'arxiv', 'bibtex', 'crossref', 'dblp', 'doi', 'folder', 'isbn', 'lib', 'pdf2arxivid', 'pdf2doi', 'pmid', 'yaml', 'zenodo'.

2025-12-07 21:45:09,383 - INFO - Adding URL: https://doi.org/10.1609/AAAI.V34I07.6693
2025-12-07 21:45:09,383 - DEBUG - Executing: /Users/gabriele/Documents/GitHub/research/.venv/bin/papis add --from https://doi.org/10.1609/AAAI.V34I07.6693 --lib main
2025-12-07 21:45:09,481 - ERROR - CalledProcessError for https://doi.org/10.1609/AAAI.V34I07.6693: [ERROR] commands.default: Library 'papers' does not have any folders attached to it. Please create and add the required paths to the configuration file.
Usage: papis add [OPTIONS] [FILES]...

Error: Invalid value for '--from': 'https://doi.org/10.1609/AAAI.V34I07.6693' is not one of 'arxiv', 'bibtex', 'crossref', 'dblp', 'doi', 'folder', 'isbn', 'lib', 'pdf2arxivid', 'pdf2doi', 'pmid', 'yaml', 'zenodo'.

2025-12-07 21:56:15,657 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:56:15,657 - INFO - Script started with args: ['scripts/discover.py', 'test query']
2025-12-07 21:56:15,657 - INFO - Starting search for: test query
2025-12-07 21:56:15,657 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test query&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:56:15,657 - DEBUG - Headers: {}
2025-12-07 21:56:15,657 - DEBUG - Payload: None
2025-12-07 21:56:15,657 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=test query&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:56:15,687 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:56:15,804 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1067d2910>
2025-12-07 21:56:15,804 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1067aa820> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:56:15,928 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1067bcaf0>
2025-12-07 21:56:15,929 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:56:15,929 - DEBUG - send_request_headers.complete
2025-12-07 21:56:15,929 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:56:15,929 - DEBUG - send_request_body.complete
2025-12-07 21:56:15,929 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:56:16,532 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:56:16 GMT'), (b'x-amz-apigw-id', b'VPF9oGYcPHcEYlg='), (b'x-amzn-RequestId', b'c46cd2af-0894-41d1-8443-13e9747c874f'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 b4f99b67248205879b34cb6436fcbd88.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'U0VbzFLZTNBSFwqx1C4_FazW5j1PdzVpUSfzZm5c6XCRNEs1DLp94g==')])
2025-12-07 21:56:16,533 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test+query&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 429 "
2025-12-07 21:56:16,533 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:56:16,533 - DEBUG - receive_response_body.complete
2025-12-07 21:56:16,533 - DEBUG - response_closed.started
2025-12-07 21:56:16,533 - DEBUG - response_closed.complete
2025-12-07 21:56:16,533 - DEBUG - close.started
2025-12-07 21:56:16,534 - DEBUG - close.complete
2025-12-07 21:57:16,917 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:57:16,918 - INFO - Script started with args: ['scripts/discover.py', 'machine learning']
2025-12-07 21:57:16,918 - INFO - Starting search for: machine learning
2025-12-07 21:57:16,918 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=machine learning&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:57:16,918 - DEBUG - Headers: {}
2025-12-07 21:57:16,918 - DEBUG - Payload: None
2025-12-07 21:57:16,918 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=machine learning&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:57:16,939 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:57:17,440 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10b84f790>
2025-12-07 21:57:17,440 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10b827890> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:57:17,569 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10b83c970>
2025-12-07 21:57:17,570 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:57:17,571 - DEBUG - send_request_headers.complete
2025-12-07 21:57:17,571 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:57:17,571 - DEBUG - send_request_body.complete
2025-12-07 21:57:17,571 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:57:18,179 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:57:18 GMT'), (b'x-amz-apigw-id', b'VPGHQEccPHcEYXg='), (b'x-amzn-RequestId', b'04613f53-dc14-4bef-95ad-710da0c2d9a8'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 645f43b8717568c0a4b2c8f32ab504dc.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'w3owFfB5vR5_sq53KbxLQeMzlcZhi3rk2-6OamlbqHbmpoSyQxdAeQ==')])
2025-12-07 21:57:18,180 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 429 "
2025-12-07 21:57:18,180 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:57:18,180 - DEBUG - receive_response_body.complete
2025-12-07 21:57:18,181 - DEBUG - response_closed.started
2025-12-07 21:57:18,181 - DEBUG - response_closed.complete
2025-12-07 21:57:18,182 - DEBUG - close.started
2025-12-07 21:57:18,182 - DEBUG - close.complete
2025-12-07 21:57:48,184 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=machine learning&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:57:48,185 - DEBUG - Headers: {}
2025-12-07 21:57:48,185 - DEBUG - Payload: None
2025-12-07 21:57:48,185 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=machine learning&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:57:48,198 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:57:48,357 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1065b1d30>
2025-12-07 21:57:48,357 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10b827a50> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:57:48,502 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1065b1ac0>
2025-12-07 21:57:48,503 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:57:48,503 - DEBUG - send_request_headers.complete
2025-12-07 21:57:48,504 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:57:48,504 - DEBUG - send_request_body.complete
2025-12-07 21:57:48,504 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:57:50,882 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'59946'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:57:50 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:57:50 GMT'), (b'x-amzn-RequestId', b'9b9a5fc4-64c8-4117-a858-c3e99e58a68c'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'59946'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPGMGEo-vHcEe4g='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 f92e634cae29c615cbe8f686d8333cf6.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'1tpkLkQo4TEbeFYsWtR5umbnJ8ygcHWSdzgiQlNl25pMInDSfjZc2g==')])
2025-12-07 21:57:50,883 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:57:50,883 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:57:51,154 - DEBUG - receive_response_body.complete
2025-12-07 21:57:51,155 - DEBUG - response_closed.started
2025-12-07 21:57:51,155 - DEBUG - response_closed.complete
2025-12-07 21:57:51,156 - DEBUG - close.started
2025-12-07 21:57:51,156 - DEBUG - close.complete
2025-12-07 21:57:51,163 - INFO - Search returned (generator). Processing first 20 results.
2025-12-07 21:57:51,165 - INFO - Invoking FZF subprocess with preview.
2025-12-07 21:57:51,330 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:57:51,330 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpp317uoq9.json']
2025-12-07 21:57:51,894 - INFO - FZF finished. Parsing selections.
2025-12-07 21:57:51,902 - INFO - Selected 0 papers: []
2025-12-07 21:57:51,903 - INFO - Starting search for: machine learning
2025-12-07 21:57:51,904 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=machine learning&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:57:51,904 - DEBUG - Headers: {}
2025-12-07 21:57:51,904 - DEBUG - Payload: None
2025-12-07 21:57:51,904 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=machine learning&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:57:51,914 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:57:52,455 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10b83c970>
2025-12-07 21:57:52,455 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10b827970> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:57:53,333 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10b8e1700>
2025-12-07 21:57:53,333 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:57:53,334 - DEBUG - send_request_headers.complete
2025-12-07 21:57:53,334 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:57:53,334 - DEBUG - send_request_body.complete
2025-12-07 21:57:53,334 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:57:53,962 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:57:53 GMT'), (b'x-amz-apigw-id', b'VPGMzEJzPHcEn1w='), (b'x-amzn-RequestId', b'ff495b7f-67fe-4ce4-ae07-2cb6397d5bfd'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 9a3c643f228eb943137621235dabf790.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'yHkMhzUuMzSGVN8DiLwTXHuBOj-xJ7Y4E24KThrKM95g9N1Nzl6dUA==')])
2025-12-07 21:57:53,962 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 429 "
2025-12-07 21:57:53,963 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:57:53,963 - DEBUG - receive_response_body.complete
2025-12-07 21:57:53,963 - DEBUG - response_closed.started
2025-12-07 21:57:53,964 - DEBUG - response_closed.complete
2025-12-07 21:57:53,964 - DEBUG - close.started
2025-12-07 21:57:53,964 - DEBUG - close.complete
2025-12-07 21:58:09,265 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:58:09,265 - INFO - Script started with args: ['/Users/gabriele/Documents/Github/research/scripts/discover.py', 'test']
2025-12-07 21:58:09,265 - INFO - Starting search for: test
2025-12-07 21:58:09,266 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:58:09,266 - DEBUG - Headers: {}
2025-12-07 21:58:09,266 - DEBUG - Payload: None
2025-12-07 21:58:09,266 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=test&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:58:09,297 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:58:09,373 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10430e8b0>
2025-12-07 21:58:09,373 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1042e6890> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:58:09,492 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1042f8a90>
2025-12-07 21:58:09,492 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:58:09,493 - DEBUG - send_request_headers.complete
2025-12-07 21:58:09,493 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:58:09,493 - DEBUG - send_request_body.complete
2025-12-07 21:58:09,493 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:58:10,036 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:58:10 GMT'), (b'x-amz-apigw-id', b'VPGPYEddvHcEZWg='), (b'x-amzn-RequestId', b'c2d73a3e-e3e2-4ba4-81de-e2b9d4831732'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 fbbaf051f1b6e237fbee09c998e075cc.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'fQDxWfM7Se2zJF2927zqCwR2KvD07glsJ4i9osF0ve1e4pNQt-3AdQ==')])
2025-12-07 21:58:10,037 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 429 "
2025-12-07 21:58:10,037 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:58:10,038 - DEBUG - receive_response_body.complete
2025-12-07 21:58:10,038 - DEBUG - response_closed.started
2025-12-07 21:58:10,038 - DEBUG - response_closed.complete
2025-12-07 21:58:10,039 - DEBUG - close.started
2025-12-07 21:58:10,039 - DEBUG - close.complete
2025-12-07 21:58:23,966 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=machine learning&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:58:23,967 - DEBUG - Headers: {}
2025-12-07 21:58:23,967 - DEBUG - Payload: None
2025-12-07 21:58:23,967 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=machine learning&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:58:23,975 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:58:24,054 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10b9361c0>
2025-12-07 21:58:24,055 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10b827a50> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:58:24,164 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10b88b370>
2025-12-07 21:58:24,165 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:58:24,165 - DEBUG - send_request_headers.complete
2025-12-07 21:58:24,166 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:58:24,166 - DEBUG - send_request_body.complete
2025-12-07 21:58:24,166 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:58:25,662 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'59946'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:58:25 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:58:25 GMT'), (b'x-amzn-RequestId', b'd672068b-5423-4346-abbd-f8fb8133cbda'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'59946'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPGRqGyvPHcER_A='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 15e6afc335dbbf726da2dd95038f862a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'PdpoV6R1hvGLowpp3EC5XzTBze9D0eVVNDTDzpzszONIHU9XBKlRBQ==')])
2025-12-07 21:58:25,662 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:58:25,663 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:58:25,816 - DEBUG - receive_response_body.complete
2025-12-07 21:58:25,816 - DEBUG - response_closed.started
2025-12-07 21:58:25,817 - DEBUG - response_closed.complete
2025-12-07 21:58:25,817 - DEBUG - close.started
2025-12-07 21:58:25,817 - DEBUG - close.complete
2025-12-07 21:58:25,822 - INFO - Search returned (generator). Processing first 20 results.
2025-12-07 21:58:25,823 - INFO - Invoking FZF subprocess with preview.
2025-12-07 21:58:25,971 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:58:25,971 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpg_tbct2p.json']
2025-12-07 21:58:40,041 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:58:40,042 - DEBUG - Headers: {}
2025-12-07 21:58:40,042 - DEBUG - Payload: None
2025-12-07 21:58:40,042 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=test&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:58:40,050 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:58:40,140 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10450b550>
2025-12-07 21:58:40,140 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x104315ac0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:58:40,278 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1040aa040>
2025-12-07 21:58:40,279 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:58:40,279 - DEBUG - send_request_headers.complete
2025-12-07 21:58:40,280 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:58:40,280 - DEBUG - send_request_body.complete
2025-12-07 21:58:40,280 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:58:42,046 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'44662'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:58:41 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:58:41 GMT'), (b'x-amzn-RequestId', b'c48825ca-526a-4d80-85a9-6cc683cc7aba'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'44662'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPGULFXiPHcEvrg='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 0dfe79aed7452aeeec8075594a25c0fe.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'NetDQuCMfPWutdqF-cPyYcMIt2f1HV6dwjI05-Fa_sG09XKRBclhWQ==')])
2025-12-07 21:58:42,046 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:58:42,047 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:58:42,176 - DEBUG - receive_response_body.complete
2025-12-07 21:58:42,177 - DEBUG - response_closed.started
2025-12-07 21:58:42,177 - DEBUG - response_closed.complete
2025-12-07 21:58:42,177 - DEBUG - close.started
2025-12-07 21:58:42,178 - DEBUG - close.complete
2025-12-07 21:58:42,184 - INFO - Search returned (generator). Processing first 20 results.
2025-12-07 21:58:42,186 - INFO - Invoking FZF subprocess with preview.
2025-12-07 21:58:42,319 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:58:42,320 - INFO - Script started with args: ['/Users/gabriele/Documents/Github/research/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpih_7a71_.json']
2025-12-07 21:58:45,791 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:58:45,792 - INFO - Script started with args: ['/Users/gabriele/Documents/Github/research/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpih_7a71_.json']
2025-12-07 21:58:46,165 - INFO - FZF finished. Parsing selections.
2025-12-07 21:58:46,170 - INFO - Selected 1 papers: [('doi', '10.1136/BMJ.315.7109.629')]
2025-12-07 21:58:46,171 - INFO - Entering add_to_library loop.
2025-12-07 21:58:46,173 - INFO - Adding: source=doi, id=10.1136/BMJ.315.7109.629
2025-12-07 21:58:46,175 - DEBUG - Executing: /Users/gabriele/Documents/GitHub/research/.venv/bin/papis --config /Users/gabriele/Documents/Github/research/papis.config add --lib main --from doi 10.1136/BMJ.315.7109.629
2025-12-07 21:58:46,325 - ERROR - CalledProcessError for 10.1136/BMJ.315.7109.629: [ERROR] commands.default: Library 'papers' does not have any folders attached to it. Please create and add the required paths to the configuration file.
Usage: papis add [OPTIONS] [FILES]...

Error: No such option: --lib (Possible options: --git, --link)

2025-12-07 21:58:46,326 - INFO - Starting search for: test
2025-12-07 21:58:46,326 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:58:46,326 - DEBUG - Headers: {}
2025-12-07 21:58:46,326 - DEBUG - Payload: None
2025-12-07 21:58:46,326 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=test&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 21:58:46,329 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 21:58:46,372 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104510ee0>
2025-12-07 21:58:46,372 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10434df90> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 21:58:46,479 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104507fd0>
2025-12-07 21:58:46,480 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 21:58:46,480 - DEBUG - send_request_headers.complete
2025-12-07 21:58:46,480 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 21:58:46,480 - DEBUG - send_request_body.complete
2025-12-07 21:58:46,480 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 21:58:47,578 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'44662'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 20:58:47 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 20:58:47 GMT'), (b'x-amzn-RequestId', b'e7e9ee89-5daf-47d0-9d65-bd8a7ebee9a3'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'44662'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPGVHGWqPHcEAQw='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 946a47118939fc2908c7ce8e46e8d16c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'E2apVKneXrhNa-V1Xni6T8uduvDbWWz-ykOg83qePfVT6zuMYzKZVg==')])
2025-12-07 21:58:47,579 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-07 21:58:47,580 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 21:58:47,736 - DEBUG - receive_response_body.complete
2025-12-07 21:58:47,737 - DEBUG - response_closed.started
2025-12-07 21:58:47,737 - DEBUG - response_closed.complete
2025-12-07 21:58:47,738 - DEBUG - close.started
2025-12-07 21:58:47,738 - DEBUG - close.complete
2025-12-07 21:58:47,741 - INFO - Search returned (generator). Processing first 20 results.
2025-12-07 21:58:47,742 - INFO - Invoking FZF subprocess with preview.
2025-12-07 21:58:47,869 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:58:47,869 - INFO - Script started with args: ['/Users/gabriele/Documents/Github/research/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpguudr5bt.json']
2025-12-07 21:58:50,775 - DEBUG - Using selector: KqueueSelector
2025-12-07 21:58:50,775 - INFO - Script started with args: ['/Users/gabriele/Documents/Github/research/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpguudr5bt.json']
2025-12-07 21:58:51,078 - INFO - FZF finished. Parsing selections.
2025-12-07 21:58:51,080 - INFO - Selected 1 papers: [('doi', '10.1136/BMJ.315.7109.629')]
2025-12-07 21:58:51,082 - INFO - Entering add_to_library loop.
2025-12-07 21:58:51,084 - INFO - Adding: source=doi, id=10.1136/BMJ.315.7109.629
2025-12-07 21:58:51,086 - DEBUG - Executing: /Users/gabriele/Documents/GitHub/research/.venv/bin/papis --config /Users/gabriele/Documents/Github/research/papis.config add --lib main --from doi 10.1136/BMJ.315.7109.629
2025-12-07 21:58:51,218 - ERROR - CalledProcessError for 10.1136/BMJ.315.7109.629: [ERROR] commands.default: Library 'papers' does not have any folders attached to it. Please create and add the required paths to the configuration file.
Usage: papis add [OPTIONS] [FILES]...

Error: No such option: --lib (Possible options: --git, --link)

2025-12-07 22:19:07,327 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:07,327 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', 'test']
2025-12-07 22:19:07,327 - INFO - Starting search for: test
2025-12-07 22:19:07,328 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 22:19:07,328 - DEBUG - Headers: {}
2025-12-07 22:19:07,328 - DEBUG - Payload: None
2025-12-07 22:19:07,328 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=test&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-07 22:19:07,358 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-07 22:19:07,496 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10234d910>
2025-12-07 22:19:07,496 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x102327890> server_hostname='api.semanticscholar.org' timeout=30
2025-12-07 22:19:07,617 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10233aaf0>
2025-12-07 22:19:07,617 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-07 22:19:07,618 - DEBUG - send_request_headers.complete
2025-12-07 22:19:07,618 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-07 22:19:07,618 - DEBUG - send_request_body.complete
2025-12-07 22:19:07,618 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-07 22:19:09,073 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'44662'), (b'Connection', b'keep-alive'), (b'Date', b'Sun, 07 Dec 2025 21:19:08 GMT'), (b'x-amzn-Remapped-Date', b'Sun, 07 Dec 2025 21:19:08 GMT'), (b'x-amzn-RequestId', b'736ba84e-838c-4162-bd9c-ddf49c560ba4'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'44662'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VPJT9HiiPHcEHLA='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 fd1922963c91836bd3bc6df1671c0612.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'YXIBSfibnGK-DygcUlzvxWtcLp0yTFIT7aXR2uvJbBF1w3wfQMzNzQ==')])
2025-12-07 22:19:09,074 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-07 22:19:09,074 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-07 22:19:09,223 - DEBUG - receive_response_body.complete
2025-12-07 22:19:09,224 - DEBUG - response_closed.started
2025-12-07 22:19:09,224 - DEBUG - response_closed.complete
2025-12-07 22:19:09,225 - DEBUG - close.started
2025-12-07 22:19:09,225 - DEBUG - close.complete
2025-12-07 22:19:09,232 - INFO - Search returned (generator). Processing first 20 results.
2025-12-07 22:19:09,233 - INFO - Invoking FZF subprocess with preview.
2025-12-07 22:19:09,357 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:09,357 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:47,193 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:47,193 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '1', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:47,278 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:47,278 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:48,631 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:48,632 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '1', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:48,913 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:48,913 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '2', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:49,140 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:49,140 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '3', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:49,286 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:49,286 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '4', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:49,456 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:49,456 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '5', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:49,624 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:49,624 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '6', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:49,813 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:49,813 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '7', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:49,981 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:49,981 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '8', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:50,149 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:50,149 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '9', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:50,312 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:50,312 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '10', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:50,467 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:50,467 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '11', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:50,624 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:50,624 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '12', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:50,767 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:50,767 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '13', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:50,989 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:50,989 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '14', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:51,148 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:51,149 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '15', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:51,319 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:51,319 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '16', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:51,557 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:51,557 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '17', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:51,896 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:51,896 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '18', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:54,386 - DEBUG - Using selector: KqueueSelector
2025-12-07 22:19:54,386 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research/scripts/discover.py', '--preview', '17', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn8bn_lo8.json']
2025-12-07 22:19:54,471 - INFO - FZF finished. Parsing selections.
2025-12-07 22:19:54,474 - INFO - Selected 1 papers: [('arxiv', '2501.19393')]
2025-12-07 22:19:54,474 - INFO - Entering add_to_library loop.
2025-12-07 22:19:54,475 - INFO - Adding: source=arxiv, id=2501.19393
2025-12-07 22:19:54,476 - DEBUG - Executing: /Users/gabriele/Documents/GitHub/research/.venv/bin/papis --config /Users/gabriele/Documents/GitHub/research/papis.config -l main add --batch --from arxiv 2501.19393
2025-12-07 22:20:00,885 - INFO - Finished adding 2501.19393. Return code: 0
2025-12-07 22:20:01,008 - INFO - Updated master.bib
2025-12-08 13:37:47,585 - DEBUG - Using selector: KqueueSelector
2025-12-08 13:37:47,586 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', 'papere']
2025-12-08 13:37:47,586 - INFO - Starting search for: papere
2025-12-08 13:37:47,586 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=papere&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 13:37:47,586 - DEBUG - Headers: {}
2025-12-08 13:37:47,586 - DEBUG - Payload: None
2025-12-08 13:37:47,586 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=papere&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 13:37:47,615 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-08 13:37:47,689 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x107528bd0>
2025-12-08 13:37:47,690 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1075d6690> server_hostname='api.semanticscholar.org' timeout=30
2025-12-08 13:37:47,713 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x105168c10>
2025-12-08 13:37:47,713 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 13:37:47,713 - DEBUG - send_request_headers.complete
2025-12-08 13:37:47,713 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 13:37:47,713 - DEBUG - send_request_body.complete
2025-12-08 13:37:47,713 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 13:37:48,519 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'34122'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 08 Dec 2025 12:37:48 GMT'), (b'x-amzn-Remapped-Date', b'Mon, 08 Dec 2025 12:37:48 GMT'), (b'x-amzn-RequestId', b'6cead095-c638-4de2-8278-bb45c2461ad5'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'34122'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VRP4dE4EPHcEUMQ='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 f92e634cae29c615cbe8f686d8333cf6.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'kXNxxfa4l7LXLDrZV8jADInCULWmuHasuJbLWYIF04wTR1oqKB-shw==')])
2025-12-08 13:37:48,520 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=papere&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-08 13:37:48,520 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 13:37:48,522 - DEBUG - receive_response_body.complete
2025-12-08 13:37:48,522 - DEBUG - response_closed.started
2025-12-08 13:37:48,522 - DEBUG - response_closed.complete
2025-12-08 13:37:48,522 - DEBUG - close.started
2025-12-08 13:37:48,522 - DEBUG - close.complete
2025-12-08 13:37:48,528 - INFO - Search returned (generator). Processing first 20 results.
2025-12-08 13:37:48,529 - INFO - Invoking FZF subprocess with preview.
2025-12-08 13:37:48,661 - DEBUG - Using selector: KqueueSelector
2025-12-08 13:37:48,661 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpn_sxctj9.json']
2025-12-08 14:02:24,656 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:02:24,656 - INFO - Script started with args: ['scripts/discover.py', 'attention mechanism']
2025-12-08 14:02:24,656 - INFO - Starting unified multi-source search for: attention mechanism
2025-12-08 14:02:24,657 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention mechanism&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:02:24,657 - DEBUG - Headers: {}
2025-12-08 14:02:24,657 - DEBUG - Payload: None
2025-12-08 14:02:24,657 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention mechanism&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:02:24,689 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-08 14:02:24,776 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1090e8910>
2025-12-08 14:02:24,776 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1090a52e0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-08 14:02:24,796 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10395a890>
2025-12-08 14:02:24,797 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:02:24,797 - DEBUG - send_request_headers.complete
2025-12-08 14:02:24,797 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:02:24,797 - DEBUG - send_request_body.complete
2025-12-08 14:02:24,797 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:02:25,381 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 08 Dec 2025 13:02:25 GMT'), (b'x-amz-apigw-id', b'VRTfQFrAPHcESTg='), (b'x-amzn-RequestId', b'49b82823-3a81-4af7-9ae8-43046851faa5'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 b61152e740e230075fb9605e656b9520.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'JxWOz4rjVuDiQ0bblWWfJwOKLidIo5-DgYJ8DZErerhtqq5v0byDDw==')])
2025-12-08 14:02:25,382 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+mechanism&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 429 "
2025-12-08 14:02:25,382 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:02:25,382 - DEBUG - receive_response_body.complete
2025-12-08 14:02:25,382 - DEBUG - response_closed.started
2025-12-08 14:02:25,382 - DEBUG - response_closed.complete
2025-12-08 14:02:25,382 - DEBUG - close.started
2025-12-08 14:02:25,382 - DEBUG - close.complete
2025-12-08 14:02:55,384 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention mechanism&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:02:55,384 - DEBUG - Headers: {}
2025-12-08 14:02:55,384 - DEBUG - Payload: None
2025-12-08 14:02:55,384 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention mechanism&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:02:55,395 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-08 14:02:55,417 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x109222510>
2025-12-08 14:02:55,417 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1090a56d0> server_hostname='api.semanticscholar.org' timeout=30
2025-12-08 14:02:55,442 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x105de68d0>
2025-12-08 14:02:55,442 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:02:55,442 - DEBUG - send_request_headers.complete
2025-12-08 14:02:55,442 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:02:55,442 - DEBUG - send_request_body.complete
2025-12-08 14:02:55,442 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:02:55,630 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 08 Dec 2025 13:02:55 GMT'), (b'x-amz-apigw-id', b'VRTj_EbVvHcEAXQ='), (b'x-amzn-RequestId', b'b54304dd-92a4-4262-87e7-421aadbc43ce'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 65678b013de2c2e6c05b27f6ecb2d78a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'u0fZuU5QhGrSvjD7G45H_UGFO7CCpcj20wqDaXKFSNs2RckQHlX6CA==')])
2025-12-08 14:02:55,631 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+mechanism&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 429 "
2025-12-08 14:02:55,631 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:02:55,631 - DEBUG - receive_response_body.complete
2025-12-08 14:02:55,631 - DEBUG - response_closed.started
2025-12-08 14:02:55,631 - DEBUG - response_closed.complete
2025-12-08 14:02:55,632 - DEBUG - close.started
2025-12-08 14:02:55,632 - DEBUG - close.complete
2025-12-08 14:05:34,879 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:05:34,880 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', 'attention', 'is', 'all', 'you', 'need']
2025-12-08 14:05:34,880 - INFO - Starting unified multi-source search for: attention is all you need
2025-12-08 14:05:34,881 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:05:34,881 - DEBUG - Headers: {}
2025-12-08 14:05:34,881 - DEBUG - Payload: None
2025-12-08 14:05:34,881 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:05:34,905 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-08 14:05:34,993 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x108eb3b90>
2025-12-08 14:05:34,993 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10897d370> server_hostname='api.semanticscholar.org' timeout=30
2025-12-08 14:05:35,015 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x107bb72d0>
2025-12-08 14:05:35,015 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:05:35,015 - DEBUG - send_request_headers.complete
2025-12-08 14:05:35,015 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:05:35,015 - DEBUG - send_request_body.complete
2025-12-08 14:05:35,015 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:05:36,873 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'62409'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 08 Dec 2025 13:05:36 GMT'), (b'x-amzn-Remapped-Date', b'Mon, 08 Dec 2025 13:05:36 GMT'), (b'x-amzn-RequestId', b'897da53f-4d03-4c31-b1a4-29618b64d943'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'62409'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VRT87HHmvHcEVpA='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2c6b43ece241a6b4a6a59e19ffb626fe.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'4yl734zTLWxqgHalSvQVftuNqM2hhcke2mUyKfm5Pt_MRVwqJY4edQ==')])
2025-12-08 14:05:36,874 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-08 14:05:36,874 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:05:36,986 - DEBUG - receive_response_body.complete
2025-12-08 14:05:36,987 - DEBUG - response_closed.started
2025-12-08 14:05:36,987 - DEBUG - response_closed.complete
2025-12-08 14:05:36,987 - DEBUG - close.started
2025-12-08 14:05:36,987 - DEBUG - close.complete
2025-12-08 14:05:36,995 - INFO - Semantic Scholar returned 20 results
2025-12-08 14:06:00,376 - INFO - Searching paper-scraper for: attention is all you need
2025-12-08 14:06:00,379 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:06:39,945 - ERROR - 	Scraper openaccess failed on paper titled 'Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine'.
Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 167, in link_to_pdf
    async with session.get(pdf_link, allow_redirects=True) as r:
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1510, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/utils.py", line 119, in _request
    response = await super()._request(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 639, in _request
    raise err_exc_cls(url)
aiohttp.client_exceptions.InvalidUrlClientError: pdf/main.pdf

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/scraper.py", line 100, in scrape
    result = await asyncio.wait_for(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 277, in __step
    result = coro.send(None)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 302, in openaccess_scraper
    await link_to_pdf(url, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 175, in link_to_pdf
    raise RuntimeError(f"Malformed URL {pdf_link!r} from {url}.") from exc
RuntimeError: Malformed URL 'pdf/main.pdf' from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10025985.
2025-12-08 14:06:40,802 - ERROR - 	Scraper pmc failed on paper titled 'Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine'.
Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1532, in _create_direct_connection
    hosts = await self._resolve_host(host, port, traces=traces)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1148, in _resolve_host
    return await asyncio.shield(resolved_host_task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 349, in __wakeup
    future.result()
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 279, in __step
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1179, in _resolve_host_with_throttle
    addrs = await self._resolver.resolve(host, port, family=self._family)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/resolver.py", line 40, in resolve
    infos = await self._loop.getaddrinfo(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", line 868, in getaddrinfo
    return await self.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 349, in __wakeup
    future.result()
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 974, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno 8] nodename nor servname provided, or not known

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/scraper.py", line 100, in scrape
    result = await asyncio.wait_for(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 279, in __step
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 285, in pmc_scraper
    await pmc_to_pdf(pmc_id, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 215, in pmc_to_pdf
    async with session.get(pdf_url, allow_redirects=True) as r:
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1510, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/utils.py", line 119, in _request
    response = await super()._request(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 779, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 734, in _connect_and_send_request
    conn = await self._connector.connect(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect
    proto = await self._create_connection(req, traces, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1538, in _create_direct_connection
    raise ClientConnectorDNSError(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorDNSError: Cannot connect to host www.ncbi.nlm.nih.govpdf:443 ssl:default [nodename nor servname provided, or not known]
2025-12-08 14:06:42,410 - ERROR - 	Scraper pubmed failed on paper titled 'Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine'.
Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1532, in _create_direct_connection
    hosts = await self._resolve_host(host, port, traces=traces)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1148, in _resolve_host
    return await asyncio.shield(resolved_host_task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 349, in __wakeup
    future.result()
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 279, in __step
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1179, in _resolve_host_with_throttle
    addrs = await self._resolver.resolve(host, port, family=self._family)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/resolver.py", line 40, in resolve
    infos = await self._loop.getaddrinfo(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", line 868, in getaddrinfo
    return await self.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 349, in __wakeup
    future.result()
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 974, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno 8] nodename nor servname provided, or not known

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/scraper.py", line 100, in scrape
    result = await asyncio.wait_for(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 279, in __step
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 293, in pubmed_scraper
    await pubmed_to_pdf(pubmed_id, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 208, in pubmed_to_pdf
    await pmc_to_pdf(pmc_id, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 215, in pmc_to_pdf
    async with session.get(pdf_url, allow_redirects=True) as r:
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1510, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/utils.py", line 119, in _request
    response = await super()._request(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 779, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 734, in _connect_and_send_request
    conn = await self._connector.connect(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect
    proto = await self._create_connection(req, traces, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1538, in _create_direct_connection
    raise ClientConnectorDNSError(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorDNSError: Cannot connect to host www.ncbi.nlm.nih.govpdf:443 ssl:default [nodename nor servname provided, or not known]
2025-12-08 14:06:47,222 - DEBUG - Found paper: Attention is All you Need
2025-12-08 14:06:47,222 - DEBUG - Found paper: Is Space-Time Attention All You Need for Video Understanding?
2025-12-08 14:06:47,222 - DEBUG - Found paper: Attention Is All You Need In Speech Separation
2025-12-08 14:06:47,222 - DEBUG - Found paper: Open-Set Recognition: A Good Closed-Set Classifier is All You Need
2025-12-08 14:06:47,222 - DEBUG - Found paper: Channel Attention Is All You Need for Video Frame Interpolation
2025-12-08 14:06:47,222 - DEBUG - Found paper: Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth
2025-12-08 14:06:47,222 - DEBUG - Found paper: TransGeo: Transformer Is All You Need for Cross-view Image Geo-localization
2025-12-08 14:06:47,222 - DEBUG - Found paper: Fastformer: Additive Attention Can Be All You Need
2025-12-08 14:06:47,222 - DEBUG - Found paper: CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models
2025-12-08 14:06:47,222 - DEBUG - Found paper: CONFORM: Contrast is All You Need For High-Fidelity Text-to-Image Diffusion Models
2025-12-08 14:06:47,222 - DEBUG - Found paper: [CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster
2025-12-08 14:06:47,222 - DEBUG - Found paper: Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation
2025-12-08 14:06:47,222 - DEBUG - Found paper: All You Need Is Feedback: Communication With Block Attention Feedback Codes
2025-12-08 14:06:47,222 - DEBUG - Found paper: Linear attention is (maybe) all you need (to understand transformer optimization)
2025-12-08 14:06:47,222 - DEBUG - Found paper: Attention Is Indeed All You Need: Semantically Attention-Guided Decoding for Data-to-Text NLG
2025-12-08 14:06:47,222 - DEBUG - Found paper: Text-Guided Attention is All You Need for Zero-Shot Robustness in Vision-Language Models
2025-12-08 14:06:47,222 - INFO - Found 16 papers from paper-scraper
2025-12-08 14:06:47,225 - INFO - Paper-scraper returned 16 results
2025-12-08 14:06:47,225 - DEBUG - Merged 20 S2 + 16 PS  31 unique papers (25 had identifiers)
2025-12-08 14:06:47,226 - INFO - Combined 31 unique papers
2025-12-08 14:06:47,227 - INFO - Invoking FZF subprocess with preview.
2025-12-08 14:06:47,471 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:06:47,471 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpw216yqvu.json']
2025-12-08 14:08:06,464 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:08:06,464 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpw216yqvu.json']
2025-12-08 14:08:07,056 - INFO - FZF finished. Parsing selections.
2025-12-08 14:08:07,059 - INFO - Selected 1 papers: [('arxiv', '1706.03762')]
2025-12-08 14:08:07,060 - INFO - Entering add_to_library loop.
2025-12-08 14:08:07,063 - INFO - Adding: source=arxiv, id=1706.03762
2025-12-08 14:08:07,064 - INFO - Fetching PDF from ArXiv: https://arxiv.org/pdf/1706.03762.pdf
2025-12-08 14:08:07,067 - DEBUG - Starting new HTTPS connection (1): arxiv.org:443
2025-12-08 14:08:07,133 - DEBUG - https://arxiv.org:443 "GET /pdf/1706.03762.pdf HTTP/1.1" 301 217
2025-12-08 14:08:07,154 - DEBUG - https://arxiv.org:443 "GET /pdf/1706.03762 HTTP/1.1" 200 2215244
2025-12-08 14:08:07,322 - INFO - PDF downloaded to: /var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpwp7nlyy8.pdf
2025-12-08 14:08:07,327 - DEBUG - Executing: /Users/gabriele/Documents/GitHub/research-agent-cli/.venv/bin/papis --config /Users/gabriele/Documents/GitHub/research-agent-cli/papis.config -l main add --batch --from arxiv 1706.03762 --file /var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpwp7nlyy8.pdf
2025-12-08 14:08:07,508 - ERROR - CalledProcessError for 1706.03762: /Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/isbnlib/registry.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import iter_entry_points
Usage: papis add [OPTIONS] [FILES]...

Error: No such option: --file Did you mean --file-name?

2025-12-08 14:08:20,786 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:08:20,786 - INFO - Script started with args: ['scripts/discover.py', 'test query']
2025-12-08 14:08:20,786 - INFO - Starting unified multi-source search for: test query
2025-12-08 14:08:20,786 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test query&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:08:20,786 - DEBUG - Headers: {}
2025-12-08 14:08:20,786 - DEBUG - Payload: None
2025-12-08 14:08:20,786 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=test query&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:08:20,817 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-08 14:08:20,894 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x104008f10>
2025-12-08 14:08:20,895 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10390d490> server_hostname='api.semanticscholar.org' timeout=30
2025-12-08 14:08:20,917 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x100f8d150>
2025-12-08 14:08:20,917 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:08:20,917 - DEBUG - send_request_headers.complete
2025-12-08 14:08:20,917 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:08:20,917 - DEBUG - send_request_body.complete
2025-12-08 14:08:20,917 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:08:21,427 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 08 Dec 2025 13:08:21 GMT'), (b'x-amz-apigw-id', b'VRUW5FfSPHcEe7Q='), (b'x-amzn-RequestId', b'd31bc817-86f3-471b-9d75-2058e9e85e01'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 785051524e198b20f8b58122667c188e.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'vCb5RHdQPik7vsyRt92l7KO0v8WKstLI01_EbLeaOF98X1eBSvU8pw==')])
2025-12-08 14:08:21,427 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test+query&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 429 "
2025-12-08 14:08:21,427 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:08:21,427 - DEBUG - receive_response_body.complete
2025-12-08 14:08:21,427 - DEBUG - response_closed.started
2025-12-08 14:08:21,427 - DEBUG - response_closed.complete
2025-12-08 14:08:21,427 - DEBUG - close.started
2025-12-08 14:08:21,427 - DEBUG - close.complete
2025-12-08 14:08:51,428 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test query&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:08:51,429 - DEBUG - Headers: {}
2025-12-08 14:08:51,429 - DEBUG - Payload: None
2025-12-08 14:08:51,429 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=test query&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:08:51,441 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-08 14:08:51,461 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10402c610>
2025-12-08 14:08:51,461 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10390d490> server_hostname='api.semanticscholar.org' timeout=30
2025-12-08 14:08:51,483 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x102335b10>
2025-12-08 14:08:51,484 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:08:51,484 - DEBUG - send_request_headers.complete
2025-12-08 14:08:51,484 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:08:51,484 - DEBUG - send_request_body.complete
2025-12-08 14:08:51,484 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:08:51,978 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 08 Dec 2025 13:08:51 GMT'), (b'x-amz-apigw-id', b'VRUbrHquPHcER2A='), (b'x-amzn-RequestId', b'91899a37-ed1f-4113-923c-6237c40e6969'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 5a897fa3742273380e3e2532c7dadcb6.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'FrBmUOF7tqmpcuL0vMLdcH_BKSm2-mTi5IIO7Oi8VXMeslkNaH0G3A==')])
2025-12-08 14:08:51,978 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test+query&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 429 "
2025-12-08 14:08:51,978 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:08:51,978 - DEBUG - receive_response_body.complete
2025-12-08 14:08:51,978 - DEBUG - response_closed.started
2025-12-08 14:08:51,978 - DEBUG - response_closed.complete
2025-12-08 14:08:51,979 - DEBUG - close.started
2025-12-08 14:08:51,979 - DEBUG - close.complete
2025-12-08 14:09:21,980 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test query&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:09:21,980 - DEBUG - Headers: {}
2025-12-08 14:09:21,980 - DEBUG - Payload: None
2025-12-08 14:09:21,980 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=test query&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:09:21,986 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-08 14:09:22,007 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10403ee50>
2025-12-08 14:09:22,008 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10390d490> server_hostname='api.semanticscholar.org' timeout=30
2025-12-08 14:09:22,033 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1040461d0>
2025-12-08 14:09:22,033 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:09:22,033 - DEBUG - send_request_headers.complete
2025-12-08 14:09:22,033 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:09:22,033 - DEBUG - send_request_body.complete
2025-12-08 14:09:22,033 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:09:22,536 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 08 Dec 2025 13:09:22 GMT'), (b'x-amz-apigw-id', b'VRUgcHrPPHcEQmQ='), (b'x-amzn-RequestId', b'217c95fb-a6a3-4d46-b48a-88f53819c589'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 f23ba2c965ce44072e54ea2301ccf406.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'mcxMb3TgwynVVmGcwvuMkuwKoPIXqHyNGnJokmUkscpVl-08WpE8Dg==')])
2025-12-08 14:09:22,537 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=test+query&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 429 "
2025-12-08 14:09:22,537 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:09:22,538 - DEBUG - receive_response_body.complete
2025-12-08 14:09:22,538 - DEBUG - response_closed.started
2025-12-08 14:09:22,538 - DEBUG - response_closed.complete
2025-12-08 14:09:22,538 - DEBUG - close.started
2025-12-08 14:09:22,539 - DEBUG - close.complete
2025-12-08 14:09:49,599 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:09:49,599 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', 'attention', 'is', 'all', 'you', 'need']
2025-12-08 14:09:49,599 - INFO - Starting unified multi-source search for: attention is all you need
2025-12-08 14:09:49,599 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:09:49,599 - DEBUG - Headers: {}
2025-12-08 14:09:49,599 - DEBUG - Payload: None
2025-12-08 14:09:49,599 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention is all you need&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:09:49,624 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-08 14:09:49,697 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1033f9110>
2025-12-08 14:09:49,697 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1030e1490> server_hostname='api.semanticscholar.org' timeout=30
2025-12-08 14:09:49,721 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x102547d50>
2025-12-08 14:09:49,721 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:09:49,722 - DEBUG - send_request_headers.complete
2025-12-08 14:09:49,722 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:09:49,722 - DEBUG - send_request_body.complete
2025-12-08 14:09:49,722 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:09:51,443 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'62409'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 08 Dec 2025 13:09:51 GMT'), (b'x-amzn-Remapped-Date', b'Mon, 08 Dec 2025 13:09:51 GMT'), (b'x-amzn-RequestId', b'd53c6bbb-e9eb-4656-a45e-99a9eaead410'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'62409'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VRUkxG0avHcEWug='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2f9d40e1286737e2a1a91819dee481a8.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'mb_W5VIeyI0Aw_mK4dw9RhkMTRmjz1bNX_21-mrIMJPEPdvMPubRjg==')])
2025-12-08 14:09:51,444 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=attention+is+all+you+need&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-08 14:09:51,444 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:09:51,549 - DEBUG - receive_response_body.complete
2025-12-08 14:09:51,550 - DEBUG - response_closed.started
2025-12-08 14:09:51,550 - DEBUG - response_closed.complete
2025-12-08 14:09:51,550 - DEBUG - close.started
2025-12-08 14:09:51,550 - DEBUG - close.complete
2025-12-08 14:09:51,554 - INFO - Semantic Scholar returned 20 results
2025-12-08 14:09:51,555 - INFO - Searching paper-scraper for: attention is all you need
2025-12-08 14:09:51,556 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:09:56,130 - WARNING - Hit a service limit per status 429 with message {"message": "Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form", "code": "429"}, sleeping 0.11-sec before retry 1.
2025-12-08 14:10:00,141 - WARNING - Hit a service limit per status 429 with message {"message": "Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form", "code": "429"}, sleeping 0.28-sec before retry 2.
2025-12-08 14:10:03,828 - WARNING - Hit a service limit per status 429 with message {"message": "Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form", "code": "429"}, sleeping 0.42-sec before retry 3.
2025-12-08 14:10:08,129 - WARNING - Hit a service limit per status 429 with message {"message": "Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form", "code": "429"}, sleeping 0.81-sec before retry 4.
2025-12-08 14:10:11,751 - WARNING - Hit a service limit per status 429 with message {"message": "Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form", "code": "429"}, sleeping 1.61-sec before retry 5.
2025-12-08 14:10:15,761 - ERROR - Error searching paper-scraper: Failed to avoid a service limit across 5 retries.
2025-12-08 14:10:15,761 - INFO - Paper-scraper returned 0 results
2025-12-08 14:10:15,762 - DEBUG - Merged 20 S2 + 0 PS  20 unique papers (19 had identifiers)
2025-12-08 14:10:15,765 - INFO - Combined 20 unique papers
2025-12-08 14:10:15,767 - INFO - Invoking FZF subprocess with preview.
2025-12-08 14:10:15,968 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:10:15,968 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpmeaukf51.json']
2025-12-08 14:10:18,057 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:10:18,057 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '1', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpmeaukf51.json']
2025-12-08 14:10:19,280 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:10:19,281 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpmeaukf51.json']
2025-12-08 14:10:20,781 - INFO - FZF finished. Parsing selections.
2025-12-08 14:10:20,783 - INFO - Selected 1 papers: [('arxiv', '2010.13154')]
2025-12-08 14:10:20,784 - INFO - Entering add_to_library loop.
2025-12-08 14:10:20,787 - INFO - Adding: source=arxiv, id=2010.13154
2025-12-08 14:10:20,787 - INFO - Fetching PDF from ArXiv: https://arxiv.org/pdf/2010.13154.pdf
2025-12-08 14:10:20,790 - DEBUG - Starting new HTTPS connection (1): arxiv.org:443
2025-12-08 14:10:20,871 - DEBUG - https://arxiv.org:443 "GET /pdf/2010.13154.pdf HTTP/1.1" 301 217
2025-12-08 14:10:20,896 - DEBUG - https://arxiv.org:443 "GET /pdf/2010.13154 HTTP/1.1" 200 271339
2025-12-08 14:10:20,956 - INFO - PDF downloaded to: /var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpax15kxg7.pdf
2025-12-08 14:10:20,960 - DEBUG - Executing: /Users/gabriele/Documents/GitHub/research-agent-cli/.venv/bin/papis --config /Users/gabriele/Documents/GitHub/research-agent-cli/papis.config -l main add --batch --from arxiv 2010.13154 --file-name /var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpax15kxg7.pdf
2025-12-08 14:10:22,081 - INFO - Finished adding 2010.13154. Return code: 0
2025-12-08 14:10:22,185 - INFO - Updated master.bib
2025-12-08 14:29:55,446 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:29:55,446 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', 'ask', 'what', 'is', 'a', 'transformer']
2025-12-08 14:29:55,446 - INFO - Starting unified multi-source search for: ask what is a transformer
2025-12-08 14:29:55,447 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=ask what is a transformer&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:29:55,447 - DEBUG - Headers: {}
2025-12-08 14:29:55,447 - DEBUG - Payload: None
2025-12-08 14:29:55,447 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=ask what is a transformer&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:29:55,474 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-08 14:29:55,572 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103f2d7d0>
2025-12-08 14:29:55,572 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x103ac1490> server_hostname='api.semanticscholar.org' timeout=30
2025-12-08 14:29:55,595 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103d570d0>
2025-12-08 14:29:55,595 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:29:55,596 - DEBUG - send_request_headers.complete
2025-12-08 14:29:55,596 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:29:55,596 - DEBUG - send_request_body.complete
2025-12-08 14:29:55,596 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:29:56,152 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'', [(b'Content-Type', b'application/json'), (b'Content-Length', b'174'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 08 Dec 2025 13:29:56 GMT'), (b'x-amz-apigw-id', b'VRXhMH1JPHcEjtg='), (b'x-amzn-RequestId', b'84c39527-31e0-49c9-b7a4-826e1eeb2169'), (b'x-amzn-ErrorType', b'TooManyRequestsException'), (b'X-Cache', b'Error from cloudfront'), (b'Via', b'1.1 495353e55c32c4243e459034dba6126a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'jzi_kNuqHvqO-l79eRcKGCQsYzDvOa5oqtbyTZxzR1PBkmswssrWeQ==')])
2025-12-08 14:29:56,153 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=ask+what+is+a+transformer&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 429 "
2025-12-08 14:29:56,153 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:29:56,153 - DEBUG - receive_response_body.complete
2025-12-08 14:29:56,153 - DEBUG - response_closed.started
2025-12-08 14:29:56,153 - DEBUG - response_closed.complete
2025-12-08 14:29:56,154 - DEBUG - close.started
2025-12-08 14:29:56,154 - DEBUG - close.complete
2025-12-08 14:30:26,155 - DEBUG - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=ask what is a transformer&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:30:26,156 - DEBUG - Headers: {}
2025-12-08 14:30:26,156 - DEBUG - Payload: None
2025-12-08 14:30:26,156 - DEBUG - cURL command: curl -X GET https://api.semanticscholar.org/graph/v1/paper/search?query=ask what is a transformer&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=20
2025-12-08 14:30:26,166 - DEBUG - connect_tcp.started host='api.semanticscholar.org' port=443 local_address=None timeout=30 socket_options=None
2025-12-08 14:30:26,187 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103f6d490>
2025-12-08 14:30:26,188 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x103ac1490> server_hostname='api.semanticscholar.org' timeout=30
2025-12-08 14:30:26,209 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103d64f90>
2025-12-08 14:30:26,210 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:30:26,210 - DEBUG - send_request_headers.complete
2025-12-08 14:30:26,210 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:30:26,211 - DEBUG - send_request_body.complete
2025-12-08 14:30:26,211 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:30:27,167 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Content-Length', b'58643'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 08 Dec 2025 13:30:27 GMT'), (b'x-amzn-Remapped-Date', b'Mon, 08 Dec 2025 13:30:26 GMT'), (b'x-amzn-RequestId', b'589966ec-bd34-4ac6-a7ea-53042b8df970'), (b'Access-Control-Allow-Origin', b'*'), (b'x-amzn-Remapped-Content-Length', b'58643'), (b'x-amzn-Remapped-Connection', b'keep-alive'), (b'x-amz-apigw-id', b'VRXl7G8wPHcENsQ='), (b'x-amzn-Remapped-Server', b'gunicorn'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 6f1a76f707ac2d807864095749c2cb20.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'sBtE1HsnDsUp5XyyVUYfzPE7Ydd4MNHlWzZrV1agvW4WbQOfZcf4Bg==')])
2025-12-08 14:30:27,168 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=ask+what+is+a+transformer&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=20 "HTTP/1.1 200 OK"
2025-12-08 14:30:27,168 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:30:27,320 - DEBUG - receive_response_body.complete
2025-12-08 14:30:27,320 - DEBUG - response_closed.started
2025-12-08 14:30:27,321 - DEBUG - response_closed.complete
2025-12-08 14:30:27,321 - DEBUG - close.started
2025-12-08 14:30:27,322 - DEBUG - close.complete
2025-12-08 14:30:27,327 - INFO - Semantic Scholar returned 16 results
2025-12-08 14:30:27,327 - INFO - Searching paper-scraper for: ask what is a transformer
2025-12-08 14:30:27,328 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:30:32,280 - ERROR - 	Scraper pubmed failed on paper titled 'Exploring AI-chatbots capability to suggest surgical planning in ophthalmology: ChatGPT versus Google Gemini analysis of retinal detachment cases'.
Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/scraper.py", line 100, in scrape
    result = await asyncio.wait_for(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 277, in __step
    result = coro.send(None)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 293, in pubmed_scraper
    await pubmed_to_pdf(pubmed_id, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 205, in pubmed_to_pdf
    raise RuntimeError(f"No PMC ID found for PubMed ID {pubmed_id}.")
RuntimeError: No PMC ID found for PubMed ID 38448201.
2025-12-08 14:30:32,676 - ERROR - 	Scraper pmc failed on paper titled 'Recommended antibiotic treatment agreement between infectious diseases specialists and ChatGPT'.
Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1532, in _create_direct_connection
    hosts = await self._resolve_host(host, port, traces=traces)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1148, in _resolve_host
    return await asyncio.shield(resolved_host_task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 349, in __wakeup
    future.result()
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 279, in __step
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1179, in _resolve_host_with_throttle
    addrs = await self._resolver.resolve(host, port, family=self._family)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/resolver.py", line 40, in resolve
    infos = await self._loop.getaddrinfo(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", line 868, in getaddrinfo
    return await self.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 349, in __wakeup
    future.result()
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 974, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno 8] nodename nor servname provided, or not known

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/scraper.py", line 100, in scrape
    result = await asyncio.wait_for(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 279, in __step
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 285, in pmc_scraper
    await pmc_to_pdf(pmc_id, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 215, in pmc_to_pdf
    async with session.get(pdf_url, allow_redirects=True) as r:
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1510, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/utils.py", line 119, in _request
    response = await super()._request(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 779, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 734, in _connect_and_send_request
    conn = await self._connector.connect(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect
    proto = await self._create_connection(req, traces, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1538, in _create_direct_connection
    raise ClientConnectorDNSError(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorDNSError: Cannot connect to host www.ncbi.nlm.nih.govpdf:443 ssl:default [nodename nor servname provided, or not known]
2025-12-08 14:30:33,730 - ERROR - 	Scraper pubmed failed on paper titled 'A Quantitative Assessment of ChatGPT as a Neurosurgical Triaging Tool'.
Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/scraper.py", line 100, in scrape
    result = await asyncio.wait_for(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 277, in __step
    result = coro.send(None)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 293, in pubmed_scraper
    await pubmed_to_pdf(pubmed_id, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 205, in pubmed_to_pdf
    raise RuntimeError(f"No PMC ID found for PubMed ID {pubmed_id}.")
RuntimeError: No PMC ID found for PubMed ID 38353523.
2025-12-08 14:30:35,724 - ERROR - 	Scraper pubmed failed on paper titled 'ChatGPT-coauthored conversation between a pediatrician and a taxi driver'.
Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/scraper.py", line 100, in scrape
    result = await asyncio.wait_for(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 277, in __step
    result = coro.send(None)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 293, in pubmed_scraper
    await pubmed_to_pdf(pubmed_id, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 205, in pubmed_to_pdf
    raise RuntimeError(f"No PMC ID found for PubMed ID {pubmed_id}.")
RuntimeError: No PMC ID found for PubMed ID 37016002.
2025-12-08 14:30:41,341 - ERROR - 	Scraper pubmed failed on paper titled 'Assessment of Appearance-related Questions About Breast Reconstruction Generated by Chat Generative Pre-trained Transformer'.
Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1532, in _create_direct_connection
    hosts = await self._resolve_host(host, port, traces=traces)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1148, in _resolve_host
    return await asyncio.shield(resolved_host_task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 349, in __wakeup
    future.result()
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 279, in __step
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1179, in _resolve_host_with_throttle
    addrs = await self._resolver.resolve(host, port, family=self._family)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/resolver.py", line 40, in resolve
    infos = await self._loop.getaddrinfo(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", line 868, in getaddrinfo
    return await self.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 349, in __wakeup
    future.result()
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 974, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno 8] nodename nor servname provided, or not known

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/scraper.py", line 100, in scrape
    result = await asyncio.wait_for(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 279, in __step
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 293, in pubmed_scraper
    await pubmed_to_pdf(pubmed_id, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 208, in pubmed_to_pdf
    await pmc_to_pdf(pmc_id, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 215, in pmc_to_pdf
    async with session.get(pdf_url, allow_redirects=True) as r:
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1510, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/utils.py", line 119, in _request
    response = await super()._request(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 779, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 734, in _connect_and_send_request
    conn = await self._connector.connect(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect
    proto = await self._create_connection(req, traces, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1538, in _create_direct_connection
    raise ClientConnectorDNSError(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorDNSError: Cannot connect to host www.ncbi.nlm.nih.govpdf:443 ssl:default [nodename nor servname provided, or not known]
2025-12-08 14:30:43,153 - ERROR - 	Scraper openaccess failed on paper titled 'Assessment of Appearance-related Questions About Breast Reconstruction Generated by Chat Generative Pre-trained Transformer'.
Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/scraper.py", line 100, in scrape
    result = await asyncio.wait_for(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 277, in __step
    result = coro.send(None)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 302, in openaccess_scraper
    await link_to_pdf(url, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 168, in link_to_pdf
    r.raise_for_status()
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 636, in raise_for_status
    raise ClientResponseError(
aiohttp.client_exceptions.ClientResponseError: 404, message='Not Found', url='https://www.doi.org/plasreconsurg/Documents/Journal%20Supplement%20Policy%204-30-21.pdf'
2025-12-08 14:30:43,828 - ERROR - 	Scraper pmc failed on paper titled 'Assessment of Appearance-related Questions About Breast Reconstruction Generated by Chat Generative Pre-trained Transformer'.
Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1532, in _create_direct_connection
    hosts = await self._resolve_host(host, port, traces=traces)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1148, in _resolve_host
    return await asyncio.shield(resolved_host_task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 349, in __wakeup
    future.result()
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 279, in __step
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1179, in _resolve_host_with_throttle
    addrs = await self._resolver.resolve(host, port, family=self._family)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/resolver.py", line 40, in resolve
    infos = await self._loop.getaddrinfo(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", line 868, in getaddrinfo
    return await self.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 349, in __wakeup
    future.result()
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 974, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno 8] nodename nor servname provided, or not known

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/scraper.py", line 100, in scrape
    result = await asyncio.wait_for(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 279, in __step
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 285, in pmc_scraper
    await pmc_to_pdf(pmc_id, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 215, in pmc_to_pdf
    async with session.get(pdf_url, allow_redirects=True) as r:
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1510, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/utils.py", line 119, in _request
    response = await super()._request(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 779, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 734, in _connect_and_send_request
    conn = await self._connector.connect(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect
    proto = await self._create_connection(req, traces, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1538, in _create_direct_connection
    raise ClientConnectorDNSError(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorDNSError: Cannot connect to host www.ncbi.nlm.nih.govpdf:443 ssl:default [nodename nor servname provided, or not known]
2025-12-08 14:30:45,344 - ERROR - 	Scraper pubmed failed on paper titled 'Recommended antibiotic treatment agreement between infectious diseases specialists and ChatGPT'.
Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1532, in _create_direct_connection
    hosts = await self._resolve_host(host, port, traces=traces)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1148, in _resolve_host
    return await asyncio.shield(resolved_host_task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 349, in __wakeup
    future.result()
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 279, in __step
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1179, in _resolve_host_with_throttle
    addrs = await self._resolver.resolve(host, port, family=self._family)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/resolver.py", line 40, in resolve
    infos = await self._loop.getaddrinfo(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", line 868, in getaddrinfo
    return await self.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 349, in __wakeup
    future.result()
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 974, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno 8] nodename nor servname provided, or not known

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/scraper.py", line 100, in scrape
    result = await asyncio.wait_for(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 279, in __step
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 293, in pubmed_scraper
    await pubmed_to_pdf(pubmed_id, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 208, in pubmed_to_pdf
    await pmc_to_pdf(pmc_id, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 215, in pmc_to_pdf
    async with session.get(pdf_url, allow_redirects=True) as r:
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1510, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/utils.py", line 119, in _request
    response = await super()._request(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 779, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 734, in _connect_and_send_request
    conn = await self._connector.connect(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect
    proto = await self._create_connection(req, traces, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/connector.py", line 1538, in _create_direct_connection
    raise ClientConnectorDNSError(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorDNSError: Cannot connect to host www.ncbi.nlm.nih.govpdf:443 ssl:default [nodename nor servname provided, or not known]
2025-12-08 14:30:45,470 - ERROR - 	Scraper openaccess failed on paper titled 'Algorithmic logic-based verification'.
Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/scraper.py", line 100, in scrape
    result = await asyncio.wait_for(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 277, in __step
    result = coro.send(None)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 302, in openaccess_scraper
    await link_to_pdf(url, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 134, in link_to_pdf
    r.raise_for_status()
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 636, in raise_for_status
    raise ClientResponseError(
aiohttp.client_exceptions.ClientResponseError: 403, message='Forbidden', url='https://dl.acm.org/doi/pdf/10.1145/2766189.2766195?download=true'
2025-12-08 14:30:48,065 - ERROR - 	Scraper openaccess failed on paper titled 'Reflections on the Spiritual Dynamics of the Bah Faith'.
Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 154, in get_pdf
    return search_pdf_link(html_text, epdf=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/utils.py", line 209, in search_pdf_link
    raise NoPDFLinkError("No PDF link found.")
paperscraper.exceptions.NoPDFLinkError: No PDF link found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 159, in link_to_pdf
    pdf_link = get_pdf()
               ^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 156, in get_pdf
    return search_pdf_link(html_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/utils.py", line 209, in search_pdf_link
    raise NoPDFLinkError("No PDF link found.")
paperscraper.exceptions.NoPDFLinkError: No PDF link found.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/scraper.py", line 100, in scrape
    result = await asyncio.wait_for(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 489, in wait_for
    return fut.result()
           ^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 277, in __step
    result = coro.send(None)
             ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 302, in openaccess_scraper
    await link_to_pdf(url, path, session)
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/paperscraper/lib.py", line 161, in link_to_pdf
    raise RuntimeError(f"No PDF link found for {url}.") from exc
RuntimeError: No PDF link found for https://doi.org/10.31581/jbs-1.1.2(1988).
2025-12-08 14:30:48,084 - DEBUG - Found paper: Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives
2025-12-08 14:30:48,084 - DEBUG - Found paper: Testing Components of the Attention Schema Theory in Artificial Neural Networks
2025-12-08 14:30:48,084 - DEBUG - Found paper: Persistent crying in babies
2025-12-08 14:30:48,084 - INFO - Found 3 papers from paper-scraper
2025-12-08 14:30:48,086 - INFO - Paper-scraper returned 3 results
2025-12-08 14:30:48,087 - DEBUG - Merged 16 S2 + 3 PS  17 unique papers (12 had identifiers)
2025-12-08 14:30:48,092 - INFO - Combined 17 unique papers
2025-12-08 14:30:48,094 - INFO - Invoking FZF subprocess with preview.
2025-12-08 14:30:48,351 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:30:48,351 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:09,615 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:09,615 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '4', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:11,806 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:11,806 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '4', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:11,971 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:11,971 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '0', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:53,940 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:53,941 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '1', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:54,138 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:54,138 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '2', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:54,310 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:54,310 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '3', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:54,570 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:54,570 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '4', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:54,784 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:54,784 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '5', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:54,988 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:54,988 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '6', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:55,158 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:55,158 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '7', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:55,326 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:55,326 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '8', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:55,522 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:55,522 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '10', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:55,708 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:55,708 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '11', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:55,879 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:55,879 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '12', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:56,049 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:56,049 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '13', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:56,218 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:56,218 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '14', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:56,389 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:56,390 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '15', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:56,558 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:56,558 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '16', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:59,047 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:59,047 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '15', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:59,440 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:59,440 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '14', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:59,670 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:59,670 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '13', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:31:59,975 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:31:59,976 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '12', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:00,321 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:00,321 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '11', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:00,656 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:00,656 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '10', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:01,515 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:01,515 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '11', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:01,764 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:01,764 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '12', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:05,715 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:05,715 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '11', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:06,052 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:06,052 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '10', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:09,021 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:09,022 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '9', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:09,960 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:09,960 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '8', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:10,126 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:10,126 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '7', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:10,292 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:10,292 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '6', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:10,740 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:10,740 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '5', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:12,739 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:12,739 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '4', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:13,230 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:13,230 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '5', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:15,826 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:15,826 - INFO - Script started with args: ['/Users/gabriele/Documents/GitHub/research-agent-cli/scripts/discover.py', '--preview', '4', '/var/folders/yk/4_ysm4397c313kwrc2q92l0w0000gn/T/tmpfswzf67o.json']
2025-12-08 14:32:15,910 - INFO - FZF finished. Parsing selections.
2025-12-08 14:32:15,911 - INFO - Selected 1 papers: [('doi', '10.17726/philit.2024.2.6')]
2025-12-08 14:32:15,911 - INFO - Entering add_to_library loop.
2025-12-08 14:32:15,912 - INFO - Adding: source=doi, id=10.17726/philit.2024.2.6
2025-12-08 14:32:15,912 - INFO - Querying Unpaywall API: https://api.unpaywall.org/v2/10.17726/philit.2024.2.6?email=research@example.com
2025-12-08 14:32:15,913 - DEBUG - Starting new HTTPS connection (1): api.unpaywall.org:443
2025-12-08 14:32:16,380 - DEBUG - https://api.unpaywall.org:443 "GET /v2/10.17726/philit.2024.2.6?email=research@example.com HTTP/1.1" 422 154
2025-12-08 14:32:16,380 - ERROR - Failed to fetch PDF from Unpaywall for DOI 10.17726/philit.2024.2.6: 422 Client Error: Unprocessable Entity for url: https://api.unpaywall.org/v2/10.17726/philit.2024.2.6?email=research@example.com
2025-12-08 14:32:16,381 - INFO - PDF fetch failed: no sources available
2025-12-08 14:32:16,383 - DEBUG - Executing: /Users/gabriele/Documents/GitHub/research-agent-cli/.venv/bin/papis --config /Users/gabriele/Documents/GitHub/research-agent-cli/papis.config -l main add --batch --from doi 10.17726/philit.2024.2.6
2025-12-08 14:32:19,497 - INFO - Finished adding 10.17726/philit.2024.2.6. Return code: 0
2025-12-08 14:32:19,598 - INFO - Updated master.bib
2025-12-08 14:32:26,384 - DEBUG - Using AiohttpTransport...
2025-12-08 14:32:26,384 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:32:26,384 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:32:26,413 - DEBUG - connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2025-12-08 14:32:26,469 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10683a410>
2025-12-08 14:32:26,469 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1067c9a30> server_hostname='raw.githubusercontent.com' timeout=5
2025-12-08 14:32:26,492 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1061ee090>
2025-12-08 14:32:26,492 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:32:26,492 - DEBUG - send_request_headers.complete
2025-12-08 14:32:26,492 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:32:26,492 - DEBUG - send_request_body.complete
2025-12-08 14:32:26,492 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:32:26,510 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'52540'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3cd138251572ce0662c39b6cee3c3fb9dc040f283d3f3d4afe7b0952ce409f78"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'39EE:1415:4A44EF:5A1F90:69367C9F'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 08 Dec 2025 13:32:26 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-lin1730076-LIN'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'7'), (b'X-Timer', b'S1765200747.576216,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'01650f11e141e71700c04a4080421ebe663583ac'), (b'Expires', b'Mon, 08 Dec 2025 13:37:26 GMT'), (b'Source-Age', b'263')])
2025-12-08 14:32:26,511 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:32:26,528 - DEBUG - receive_response_body.complete
2025-12-08 14:32:26,528 - DEBUG - response_closed.started
2025-12-08 14:32:26,528 - DEBUG - response_closed.complete
2025-12-08 14:32:26,528 - DEBUG - close.started
2025-12-08 14:32:26,528 - DEBUG - close.complete
2025-12-08 14:32:26,796 - DEBUG - [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2025-12-08 14:32:26,896 - DEBUG - Using AiohttpTransport...
2025-12-08 14:32:26,896 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:32:26,896 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:32:26,898 - DEBUG - Using AiohttpTransport...
2025-12-08 14:32:26,898 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:32:26,898 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:32:26,898 - DEBUG - Using AiohttpTransport...
2025-12-08 14:32:26,898 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:32:26,898 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:32:26,898 - DEBUG - Using AiohttpTransport...
2025-12-08 14:32:26,898 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:32:26,898 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:32:27,199 - INFO - Configured paper-qa with Gemini: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:27,201 - DEBUG - Adding PDF: /Users/gabriele/Documents/GitHub/research-agent-cli/library/70cd4cf2090b16a8d00040b736650432/1706.03762v7.pdf
2025-12-08 14:32:27,201 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:27,556 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'custom_llm_provider': None}
2025-12-08 14:32:27,556 - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-12-08 14:32:27,556 - DEBUG - added/updated model=8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be in litellm.model_cost: 8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be
2025-12-08 14:32:27,557 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:27,557 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:27,557 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:27,557 - DEBUG - added/updated model=gemini/gemini-2.0-flash-exp in litellm.model_cost: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:27,557 - DEBUG - 
Initialized Model List ['gemini/gemini-2.0-flash-exp']
2025-12-08 14:32:27,557 - INFO - Routing strategy: simple-shuffle
2025-12-08 14:32:27,557 - DEBUG - Inside async function with retries.
2025-12-08 14:32:27,557 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x120112b10>>, num_retries - 3
2025-12-08 14:32:27,557 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:27,557 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:27,557 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:27,557 - DEBUG - retrieve cooldown models: []
2025-12-08 14:32:27,557 - DEBUG - async cooldown deployments: []
2025-12-08 14:32:27,558 - DEBUG - cooldown_deployments: []
2025-12-08 14:32:27,558 - DEBUG - cooldown deployments: []
2025-12-08 14:32:27,558 - DEBUG - 

2025-12-08 14:32:27,558 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:32:27,558 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\nGoogle Brain\navaswani@google.com\nNoam Shazeer\nGoogle Brain\nnoam@google.com\nNiki Parmar\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\nGoogle Research\nusz@google.com\nLlion Jones\nGoogle Research\nllion@google.com\nAidan N. Gomez \nUniversity of Toronto\naidan@cs.toronto.edu\nukasz Kaiser\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin \nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\nWork performed while at Google Brain.\nWork performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, c\n\nCitation:"}], caching=False, client=None, n=1, stream=False, litellm_trace_id='a2354102-5d53-4090-a500-0b20abea11bc', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:32:27,558 - DEBUG - 

2025-12-08 14:32:27,558 - DEBUG - self.optional_params: {}
2025-12-08 14:32:27,558 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:32:27,558 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:32:27,560 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:27,560 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:32:27,560 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\nGoogle Brain\navaswani@google.com\nNoam Shazeer\nGoogle Brain\nnoam@google.com\nNiki Parmar\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\nGoogle Research\nusz@google.com\nLlion Jones\nGoogle Research\nllion@google.com\nAidan N. Gomez \nUniversity of Toronto\naidan@cs.toronto.edu\nukasz Kaiser\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin \nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\nWork performed while at Google Brain.\nWork performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, c\n\nCitation:"}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:32:27,560 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:27,561 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:32:27,561 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:27,561 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:27,561 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:27,561 - DEBUG - Using AiohttpTransport...
2025-12-08 14:32:27,561 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:32:27,561 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:32:27,561 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\nGoogle Brain\navaswani@google.com\nNoam Shazeer\nGoogle Brain\nnoam@google.com\nNiki Parmar\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\nGoogle Research\nusz@google.com\nLlion Jones\nGoogle Research\nllion@google.com\nAidan N. Gomez \nUniversity of Toronto\naidan@cs.toronto.edu\nukasz Kaiser\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin \nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\nWork performed while at Google Brain.\nWork performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, c\n\nCitation:"}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:32:29,038 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Vaswani, Ashish, et al. \"Attention Is All You Need.\" *31st Conference on Neural Information Processing Systems (NIPS 2017)*, arXiv, 2 Aug. 2023. arXiv:1706.03762v7.\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.022742537781596184
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 1161,
    "candidatesTokenCount": 64,
    "totalTokenCount": 1225,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 1161
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 64
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "bNM2abukAeq0xN8Pq6nC6Qc"
}



2025-12-08 14:32:29,041 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:29,041 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:29,042 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:29,042 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:29,042 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,043 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,043 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,043 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,043 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,043 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,044 - DEBUG - response_cost: 0.0
2025-12-08 14:32:29,044 - DEBUG - response_cost: 0.0
2025-12-08 14:32:29,044 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:32:29,045 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:29,045 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,045 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,045 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,045 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,045 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,045 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:32:29,046 - DEBUG - Async Response: ModelResponse(id='bNM2abukAeq0xN8Pq6nC6Qc', created=1765200747, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Vaswani, Ashish, et al. "Attention Is All You Need." *31st Conference on Neural Information Processing Systems (NIPS 2017)*, arXiv, 2 Aug. 2023. arXiv:1706.03762v7.\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=64, prompt_tokens=1161, total_tokens=1225, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=64, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=1161, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:32:29,046 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:29,047 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:32:29,047 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,047 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:29,047 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,047 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,047 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,048 - DEBUG - Inside async function with retries.
2025-12-08 14:32:29,048 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x120112b10>>, num_retries - 3
2025-12-08 14:32:29,048 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:29,048 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:29,048 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:29,048 - DEBUG - retrieve cooldown models: []
2025-12-08 14:32:29,049 - DEBUG - async cooldown deployments: []
2025-12-08 14:32:29,049 - DEBUG - cooldown_deployments: []
2025-12-08 14:32:29,049 - DEBUG - cooldown deployments: []
2025-12-08 14:32:29,049 - DEBUG - 

2025-12-08 14:32:29,049 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:32:29,049 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Vaswani, Ashish, et al. "Attention Is All You Need." *31st Conference on Neural Information Processing Systems (NIPS 2017)*, arXiv, 2 Aug. 2023. arXiv:1706.03762v7.\n\n\nCitation JSON:'}], caching=False, client=None, n=1, stream=False, litellm_trace_id='cc171c21-0b57-44db-ab92-25066f20a4d2', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:32:29,049 - DEBUG - 

2025-12-08 14:32:29,049 - DEBUG - self.optional_params: {}
2025-12-08 14:32:29,049 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:32:29,049 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:32:29,050 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:29,050 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x12039b250>>
2025-12-08 14:32:29,050 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:32:29,050 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Vaswani, Ashish, et al. "Attention Is All You Need." *31st Conference on Neural Information Processing Systems (NIPS 2017)*, arXiv, 2 Aug. 2023. arXiv:1706.03762v7.\n\n\nCitation JSON:'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:32:29,050 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:29,051 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:32:29,051 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:29,051 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,052 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:29,052 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:29,053 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,053 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Vaswani, Ashish, et al. "Attention Is All You Need." *31st Conference on Neural Information Processing Systems (NIPS 2017)*, arXiv, 2 Aug. 2023. arXiv:1706.03762v7.\n\n\nCitation JSON:'}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:32:29,054 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:32:29,054 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:29,055 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,055 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,055 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:32:29,055 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:32:29,055 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:29,055 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,055 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,056 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,056 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,056 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,769 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"title\": \"Attention Is All You Need\",\n  \"authors\": [\n    \"Ashish Vaswani\",\n    \"et al.\"\n  ],\n  \"doi\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.0062891682799981561
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 117,
    "candidatesTokenCount": 49,
    "totalTokenCount": 166,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 117
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 49
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "bdM2adWoC5eQvdIP463D0Qw"
}



2025-12-08 14:32:29,770 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:29,771 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:29,771 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:29,771 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,771 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:29,771 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,772 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,772 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,772 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,772 - DEBUG - response_cost: 0.0
2025-12-08 14:32:29,772 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,772 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:32:29,772 - DEBUG - response_cost: 0.0
2025-12-08 14:32:29,773 - DEBUG - Async Response: ModelResponse(id='bdM2adWoC5eQvdIP463D0Qw', created=1765200749, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='```json\n{\n  "title": "Attention Is All You Need",\n  "authors": [\n    "Ashish Vaswani",\n    "et al."\n  ],\n  "doi": null\n}\n```', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=49, prompt_tokens=117, total_tokens=166, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=49, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=117, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:32:29,773 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:29,773 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:32:29,773 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,773 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:29,773 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,774 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,774 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:32:29,774 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,774 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:29,775 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,775 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,802 - DEBUG - Attempting to populate metadata query: {'authors': ['Ashish Vaswani', 'et al.'], 'title': 'Attention Is All You Need', 'session': <aiohttp.client.ClientSession object at 0x1201b9810>} via providers=[<paperqa.clients.crossref.CrossrefProvider object at 0x1206f9490>, <paperqa.clients.semantic_scholar.SemanticScholarProvider object at 0x1206f9d10>] processors=[<paperqa.clients.journal_quality.JournalQualityPostProcessor object at 0x114bbb110>]
2025-12-08 14:32:29,802 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1169da650>>
2025-12-08 14:32:29,802 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:29,803 - WARNING - CROSSREF_MAILTO environment variable not set. Crossref API rate limits may apply.
2025-12-08 14:32:29,803 - WARNING - CROSSREF_API_KEY environment variable not set. Crossref API rate limits may apply.
2025-12-08 14:32:29,803 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:29,803 - WARNING - SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.
2025-12-08 14:32:29,803 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:32:29,804 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:29,804 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,804 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,804 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:32:29,804 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:32:29,804 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:29,804 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,804 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:29,805 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,805 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:29,805 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:31,499 - DEBUG - connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2025-12-08 14:32:31,518 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12254a9d0>
2025-12-08 14:32:31,518 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1201768d0> server_hostname='raw.githubusercontent.com' timeout=5
2025-12-08 14:32:31,543 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1248bcc50>
2025-12-08 14:32:31,543 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:32:31,543 - DEBUG - send_request_headers.complete
2025-12-08 14:32:31,543 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:32:31,543 - DEBUG - send_request_body.complete
2025-12-08 14:32:31,543 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:32:31,801 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'52540'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3cd138251572ce0662c39b6cee3c3fb9dc040f283d3f3d4afe7b0952ce409f78"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'8389:10C5B:28F09C4:2ED595D:6936D36F'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 08 Dec 2025 13:32:31 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-lin1730068-LIN'), (b'X-Cache', b'MISS'), (b'X-Cache-Hits', b'0'), (b'X-Timer', b'S1765200752.626308,VS0,VE176'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'4b2ef9065ce51569c74aa7227745fc3e8ac28f9c'), (b'Expires', b'Mon, 08 Dec 2025 13:37:31 GMT'), (b'Source-Age', b'0')])
2025-12-08 14:32:31,802 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:32:31,821 - DEBUG - receive_response_body.complete
2025-12-08 14:32:31,821 - DEBUG - response_closed.started
2025-12-08 14:32:31,821 - DEBUG - response_closed.complete
2025-12-08 14:32:31,822 - DEBUG - close.started
2025-12-08 14:32:31,822 - DEBUG - close.complete
2025-12-08 14:32:31,830 - DEBUG - 

2025-12-08 14:32:31,830 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:32:31,830 - DEBUG - [92mlitellm.aembedding(model='text-embedding-3-small', input=['Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\nGoogle Brain\navaswani@google.com\nNoam Shazeer\nGoogle Brain\nnoam@google.com\nNiki Parmar\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\nGoogle Research\nusz@google.com\nLlion Jones\nGoogle Research\nllion@google.com\nAidan N. Gomez \nUniversity of Toronto\naidan@cs.toronto.edu\nukasz Kaiser\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin \nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\nWork performed while at Google Brain.\nWork performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, c', 'elve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packe', 'ith all keys, divide each by dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V) = softmax(QKT\ndk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1dk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q  k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni )\nWhere the projections are parameter matricesWQ\ni  Rdmodeldk , WK\ni  Rdmodeldk , WV\ni  Rdmodeldv\nand WO  Rhdvdmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n In "encoder-decoder attention" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to ) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedd', 'okens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodel.\n5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2  d) O(1) O(1)\nRecurrent O(n  d2) O(n) O(n)\nConvolutional O(k  n  d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r  n  d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2 to 10000  2. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi  Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6length n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k  n  d + n  d2). Even with k = n, ', 'tween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k  n  d + n  d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with 1 = 0.9, 2 = 0.98 and  = 109. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d0.5\nmodel  min(step_num0.5, step_num  warmup_steps1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0  1020\nGNMT + RL [38] 24.6 39.92 2.3  1019 1.4  1020\nConvS2S [9] 25.16 40.46 9.6  1018 1.5  1020\nMoE [32] 26.03 40.56 2.0  1019 1.2  1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0  1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8  1020 1.1  1021\nConvS2S Ensemble [9] 26.36 41.29 7.7  1019 1.2  1021\nTransformer (base model) 27.3 38.1 3.3  1018\nTransformer (big) 28.4 41.8 2.3  1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty  = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results an', 'th penalty  = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dv Pdrop ls\ntrain PPL BLEU params\nsteps (dev) (dev) 106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and  = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN seque', 'in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, aglar Glehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):17351780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Beng', 'ization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152159. ACL, June 2006.\n[27] Ankur Parikh, Oscar Tckstrm, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):19291958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 24402448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 31043112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Googles neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434443. ACL, August 2013.\n12Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb making, completing the phrase making...more difficult. Attentions here shown only for\nthe word making. Different colors represent different heads. Best viewed in color.\n13Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\n', 'ct\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word its for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15'], dimensions=1536, num_retries=3, timeout=120)[0m
2025-12-08 14:32:31,832 - DEBUG - 

2025-12-08 14:32:31,832 - DEBUG - self.optional_params: {}
2025-12-08 14:32:31,832 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:32:31,832 - DEBUG - self.optional_params: {'dimensions': 1536}
2025-12-08 14:32:31,833 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://api.openai.com/v1 \
-d '{'model': 'text-embedding-3-small', 'input': ['Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\nGoogle Brain\navaswani@google.com\nNoam Shazeer\nGoogle Brain\nnoam@google.com\nNiki Parmar\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\nGoogle Research\nusz@google.com\nLlion Jones\nGoogle Research\nllion@google.com\nAidan N. Gomez \nUniversity of Toronto\naidan@cs.toronto.edu\nukasz Kaiser\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin \nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\nWork performed while at Google Brain.\nWork performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, c', 'elve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packe', 'ith all keys, divide each by dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V) = softmax(QKT\ndk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1dk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q  k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni )\nWhere the projections are parameter matricesWQ\ni  Rdmodeldk , WK\ni  Rdmodeldk , WV\ni  Rdmodeldv\nand WO  Rhdvdmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n In "encoder-decoder attention" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to ) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedd', 'okens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodel.\n5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2  d) O(1) O(1)\nRecurrent O(n  d2) O(n) O(n)\nConvolutional O(k  n  d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r  n  d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2 to 10000  2. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi  Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6length n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k  n  d + n  d2). Even with k = n, ', 'tween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k  n  d + n  d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with 1 = 0.9, 2 = 0.98 and  = 109. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d0.5\nmodel  min(step_num0.5, step_num  warmup_steps1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0  1020\nGNMT + RL [38] 24.6 39.92 2.3  1019 1.4  1020\nConvS2S [9] 25.16 40.46 9.6  1018 1.5  1020\nMoE [32] 26.03 40.56 2.0  1019 1.2  1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0  1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8  1020 1.1  1021\nConvS2S Ensemble [9] 26.36 41.29 7.7  1019 1.2  1021\nTransformer (base model) 27.3 38.1 3.3  1018\nTransformer (big) 28.4 41.8 2.3  1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty  = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results an', 'th penalty  = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dv Pdrop ls\ntrain PPL BLEU params\nsteps (dev) (dev) 106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and  = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN seque', 'in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, aglar Glehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):17351780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Beng', 'ization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152159. ACL, June 2006.\n[27] Ankur Parikh, Oscar Tckstrm, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):19291958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 24402448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 31043112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Googles neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434443. ACL, August 2013.\n12Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb making, completing the phrase making...more difficult. Attentions here shown only for\nthe word making. Different colors represent different heads. Best viewed in color.\n13Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\n', 'ct\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word its for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15'], 'dimensions': 1536}'
[0m

2025-12-08 14:32:31,834 - DEBUG - Using AiohttpTransport...
2025-12-08 14:32:31,835 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:32:31,835 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:32:31,835 - DEBUG - RAW RESPONSE:
The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


2025-12-08 14:32:31,835 - DEBUG - Logging Details: logger_fn - None | callable(logger_fn) - False
2025-12-08 14:32:31,843 - DEBUG - Logging Details LiteLLM-Failure Call: [<bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x120112b10>>]
2025-12-08 14:32:31,844 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:32:31,844 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:32:31,847 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:32:31,847 - ERROR - Error adding /Users/gabriele/Documents/GitHub/research-agent-cli/library/70cd4cf2090b16a8d00040b736650432/1706.03762v7.pdf: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable LiteLLM Retried: 3 times
2025-12-08 14:32:31,847 - DEBUG - Adding PDF: /Users/gabriele/Documents/GitHub/research-agent-cli/library/4603a874ea67c1b582ccff73ff5f82f2/2501.19393v3.pdf
2025-12-08 14:32:31,847 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:32,721 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'custom_llm_provider': None}
2025-12-08 14:32:32,721 - DEBUG - model=8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be, custom_llm_provider=None has no input_cost_per_token in model_cost_map. Defaulting to 0.
2025-12-08 14:32:32,721 - DEBUG - model=8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be, custom_llm_provider=None has no output_cost_per_token in model_cost_map. Defaulting to 0.
2025-12-08 14:32:32,721 - DEBUG - model_info: {'key': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': None, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': None, 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:32,722 - DEBUG - added/updated model=8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be in litellm.model_cost: 8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be
2025-12-08 14:32:32,722 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:32,722 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:32,722 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:32,722 - DEBUG - added/updated model=gemini/gemini-2.0-flash-exp in litellm.model_cost: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:32,722 - DEBUG - 
Initialized Model List ['gemini/gemini-2.0-flash-exp']
2025-12-08 14:32:32,722 - INFO - Routing strategy: simple-shuffle
2025-12-08 14:32:32,722 - DEBUG - Inside async function with retries.
2025-12-08 14:32:32,722 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x120ca3290>>, num_retries - 3
2025-12-08 14:32:32,722 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:32,722 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:32,722 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:32,722 - DEBUG - retrieve cooldown models: []
2025-12-08 14:32:32,722 - DEBUG - async cooldown deployments: []
2025-12-08 14:32:32,722 - DEBUG - cooldown_deployments: []
2025-12-08 14:32:32,722 - DEBUG - cooldown deployments: []
2025-12-08 14:32:32,722 - DEBUG - 

2025-12-08 14:32:32,722 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:32:32,722 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\ns1: Simple test-time scaling\nNiklas Muennighoff*134 Zitong Yang*1 Weijia Shi*23 Xiang Lisa Li*1 Li Fei-Fei1 Hannaneh Hajishirzi23\nLuke Zettlemoyer2 Percy Liang1 Emmanuel Cands1 Tatsunori Hashimoto1\nAbstract\nTest-time scaling is a promising new approach to\nlanguage modeling that uses extra test-time com-\nputetoimproveperformance. Recently,OpenAIs\no1 model showed this capability but did not pub-\nlicly share its methodology, leading to many repli-\ncation efforts. We seek the simplest approach to\nachievetest-timescalingandstrongreasoningper-\nformance. First, we curate a small datasets1K\nof 1,000 questions paired with reasoning traces\nrelying on three criteria we validate through abla-\ntions: difficulty,diversity,andquality. Second,we\ndevelop budget forcing to control test-time com-\npute by forcefully terminating the models think-\ning process or lengthening it by appending Wait\nmultiple times to the models generation when it\ntries to end. This can lead the model to double-\ncheck its answer, often fixing incorrect reasoning\nsteps. After supervised finetuning the Qwen2.5-\n32B-Instruct language model ons1Kand equip-\nping it with budget forcing, our models1-32B ex-\nceeds o1-preview on competition math questions\nby up to 27% (MATH and AIME24). Further,\nscaling s1-32B with budget forcing allows extrap-\nolating beyond its performance without test-time\nintervention: from 50% to 57% on AIME24. Our\nmodel, data, and code are open-source athttps:\n//github.com/simplescaling/s1.\n1. Introduction\nPerformance improvements of language models (LMs) over\nthe past years have largely relied on scaling up train-time\ncompute using large-scale self-supervised pretraining (Ka-\nplan et al., 2020; Hoffmann et al., 2022). The creation of\nthese powerful models has set the stage for a new scaling\nparadigm built on top of them:test-time scaling. The aim\n*Equal contribution. ZY and NM started the project. WS, NM\nand ZY collected the prompts, XL, ZY and NM, built the data\npipeline, LZ and WS proposed using a 1K subset and ZY and\nNM built budget forcing.1 Stanford University.2 University of\nWashington, Seattle.3 Allen Institute for AI.4 Contextual AI.\n512 2048\n65\n75\n85\n95Accuracy (%)\nMathematical\nProblem Solving\n(MATH500)\n512 2048 8192\n0\n20\n40\n60\nCompetition\nMath\n(AIME24)\n1024 4096\n40\n50\n60\nPhD-Level\nScience Questions\n(GPQA Diamond)\nAverage thinking time (tokens)\nFigure 1.Test-time scaling with s1-32B.We benchmarks1-32B\non reasoning-intensive tasks and vary test-time compute.\nof this approach is to increase the compute at test time to\nget better results. There has been much work exploring this\nidea (Snell et al., 2024; Welleck et al., 2024), and the via-\nbility of this paradigm was recently validated by OpenAI\no1 (OpenAI, 2024). o1 has demonstrated strong reasoning\nperformance with consistent gains from scaling test-time\ncompute. OpenAI describes their approach as using large-\nscalereinforcementlearning(RL)implyingtheuseofsizable\namounts of data (OpenAI, 2024). This has led to various\nattempts to replicate their models relying on techniques like\nMonte Carlo Tree Search (Gao et al., 2024b; Zhang et al.,\n2024a), multi-agent approaches (Qin et al., 2024), and oth-\ners (Wang et al., 2024a; Huang et al., 2024b; 2025). Among\nthese approaches, DeepSeek R1 (DeepSeek-AI et al., 2025)\nhas successfully replicated o1-level performance, also em-\nploying reinforcement learning via millions of samples and\nmultiple training stages. However, despite the large num-\nber of o1 replication attempts, none have openly replicated\na clear test-time scaling behavior. Thus, we ask: what is\nthe simplest approach to achieve both test-time scaling and\nstrong reasoning performance?\nWeshowthattrainingononly1,000sampleswithnext-token\nprediction and controlling thinking duration via a simple\ntest-time technique we refer to asbudget forcingleads to\na strong reasoning model that scales in performance with\nmore test-time compute. Specifically, we constructs1K,\nwhich consists of 1,000 carefully curated questions paired\nwith reasoning traces and answers distilled from Gemini\nThinking Experimental (Google, 2024). We perform super-\nvised fine-tuning (SFT) of an off-the-shelf pretrained model\n1\narXiv:2501.19393v3  [cs.CL]  1 Mar 2025s1: Simple test-time scaling\non our small dataset requiring just 26 minutes of training\non 16 H100 GPUs. After training, we control the amount\nof test-time compute our model spends usingbudget forc-\ning: (I) If the model generates more thinking tokens than\na desired limit, we forcefully end the thinking process by\nappending an end-of-thinking token delimiter. Ending the\nthinking this way makes the model transition to generating\nitsanswer. (II)Ifwewantthemodeltospendmoretest-time\ncompute on a problem, we suppress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our mode\n\nCitation:"}], caching=False, client=None, n=1, stream=False, litellm_trace_id='2fafc998-1ded-4915-a7ff-b53f807a48ae', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:32:32,722 - DEBUG - 

2025-12-08 14:32:32,722 - DEBUG - self.optional_params: {}
2025-12-08 14:32:32,723 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:32:32,723 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:32:32,723 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:32,723 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:32:32,723 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\ns1: Simple test-time scaling\nNiklas Muennighoff*134 Zitong Yang*1 Weijia Shi*23 Xiang Lisa Li*1 Li Fei-Fei1 Hannaneh Hajishirzi23\nLuke Zettlemoyer2 Percy Liang1 Emmanuel Cands1 Tatsunori Hashimoto1\nAbstract\nTest-time scaling is a promising new approach to\nlanguage modeling that uses extra test-time com-\nputetoimproveperformance. Recently,OpenAIs\no1 model showed this capability but did not pub-\nlicly share its methodology, leading to many repli-\ncation efforts. We seek the simplest approach to\nachievetest-timescalingandstrongreasoningper-\nformance. First, we curate a small datasets1K\nof 1,000 questions paired with reasoning traces\nrelying on three criteria we validate through abla-\ntions: difficulty,diversity,andquality. Second,we\ndevelop budget forcing to control test-time com-\npute by forcefully terminating the models think-\ning process or lengthening it by appending Wait\nmultiple times to the models generation when it\ntries to end. This can lead the model to double-\ncheck its answer, often fixing incorrect reasoning\nsteps. After supervised finetuning the Qwen2.5-\n32B-Instruct language model ons1Kand equip-\nping it with budget forcing, our models1-32B ex-\nceeds o1-preview on competition math questions\nby up to 27% (MATH and AIME24). Further,\nscaling s1-32B with budget forcing allows extrap-\nolating beyond its performance without test-time\nintervention: from 50% to 57% on AIME24. Our\nmodel, data, and code are open-source athttps:\n//github.com/simplescaling/s1.\n1. Introduction\nPerformance improvements of language models (LMs) over\nthe past years have largely relied on scaling up train-time\ncompute using large-scale self-supervised pretraining (Ka-\nplan et al., 2020; Hoffmann et al., 2022). The creation of\nthese powerful models has set the stage for a new scaling\nparadigm built on top of them:test-time scaling. The aim\n*Equal contribution. ZY and NM started the project. WS, NM\nand ZY collected the prompts, XL, ZY and NM, built the data\npipeline, LZ and WS proposed using a 1K subset and ZY and\nNM built budget forcing.1 Stanford University.2 University of\nWashington, Seattle.3 Allen Institute for AI.4 Contextual AI.\n512 2048\n65\n75\n85\n95Accuracy (%)\nMathematical\nProblem Solving\n(MATH500)\n512 2048 8192\n0\n20\n40\n60\nCompetition\nMath\n(AIME24)\n1024 4096\n40\n50\n60\nPhD-Level\nScience Questions\n(GPQA Diamond)\nAverage thinking time (tokens)\nFigure 1.Test-time scaling with s1-32B.We benchmarks1-32B\non reasoning-intensive tasks and vary test-time compute.\nof this approach is to increase the compute at test time to\nget better results. There has been much work exploring this\nidea (Snell et al., 2024; Welleck et al., 2024), and the via-\nbility of this paradigm was recently validated by OpenAI\no1 (OpenAI, 2024). o1 has demonstrated strong reasoning\nperformance with consistent gains from scaling test-time\ncompute. OpenAI describes their approach as using large-\nscalereinforcementlearning(RL)implyingtheuseofsizable\namounts of data (OpenAI, 2024). This has led to various\nattempts to replicate their models relying on techniques like\nMonte Carlo Tree Search (Gao et al., 2024b; Zhang et al.,\n2024a), multi-agent approaches (Qin et al., 2024), and oth-\ners (Wang et al., 2024a; Huang et al., 2024b; 2025). Among\nthese approaches, DeepSeek R1 (DeepSeek-AI et al., 2025)\nhas successfully replicated o1-level performance, also em-\nploying reinforcement learning via millions of samples and\nmultiple training stages. However, despite the large num-\nber of o1 replication attempts, none have openly replicated\na clear test-time scaling behavior. Thus, we ask: what is\nthe simplest approach to achieve both test-time scaling and\nstrong reasoning performance?\nWeshowthattrainingononly1,000sampleswithnext-token\nprediction and controlling thinking duration via a simple\ntest-time technique we refer to asbudget forcingleads to\na strong reasoning model that scales in performance with\nmore test-time compute. Specifically, we constructs1K,\nwhich consists of 1,000 carefully curated questions paired\nwith reasoning traces and answers distilled from Gemini\nThinking Experimental (Google, 2024). We perform super-\nvised fine-tuning (SFT) of an off-the-shelf pretrained model\n1\narXiv:2501.19393v3  [cs.CL]  1 Mar 2025s1: Simple test-time scaling\non our small dataset requiring just 26 minutes of training\non 16 H100 GPUs. After training, we control the amount\nof test-time compute our model spends usingbudget forc-\ning: (I) If the model generates more thinking tokens than\na desired limit, we forcefully end the thinking process by\nappending an end-of-thinking token delimiter. Ending the\nthinking this way makes the model transition to generating\nitsanswer. (II)Ifwewantthemodeltospendmoretest-time\ncompute on a problem, we suppress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our mode\n\nCitation:"}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:32:32,723 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:32,723 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:32:32,723 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:32,723 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:32,723 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:32,723 - DEBUG - Using AiohttpTransport...
2025-12-08 14:32:32,723 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:32:32,723 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:32:32,724 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\ns1: Simple test-time scaling\nNiklas Muennighoff*134 Zitong Yang*1 Weijia Shi*23 Xiang Lisa Li*1 Li Fei-Fei1 Hannaneh Hajishirzi23\nLuke Zettlemoyer2 Percy Liang1 Emmanuel Cands1 Tatsunori Hashimoto1\nAbstract\nTest-time scaling is a promising new approach to\nlanguage modeling that uses extra test-time com-\nputetoimproveperformance. Recently,OpenAIs\no1 model showed this capability but did not pub-\nlicly share its methodology, leading to many repli-\ncation efforts. We seek the simplest approach to\nachievetest-timescalingandstrongreasoningper-\nformance. First, we curate a small datasets1K\nof 1,000 questions paired with reasoning traces\nrelying on three criteria we validate through abla-\ntions: difficulty,diversity,andquality. Second,we\ndevelop budget forcing to control test-time com-\npute by forcefully terminating the models think-\ning process or lengthening it by appending Wait\nmultiple times to the models generation when it\ntries to end. This can lead the model to double-\ncheck its answer, often fixing incorrect reasoning\nsteps. After supervised finetuning the Qwen2.5-\n32B-Instruct language model ons1Kand equip-\nping it with budget forcing, our models1-32B ex-\nceeds o1-preview on competition math questions\nby up to 27% (MATH and AIME24). Further,\nscaling s1-32B with budget forcing allows extrap-\nolating beyond its performance without test-time\nintervention: from 50% to 57% on AIME24. Our\nmodel, data, and code are open-source athttps:\n//github.com/simplescaling/s1.\n1. Introduction\nPerformance improvements of language models (LMs) over\nthe past years have largely relied on scaling up train-time\ncompute using large-scale self-supervised pretraining (Ka-\nplan et al., 2020; Hoffmann et al., 2022). The creation of\nthese powerful models has set the stage for a new scaling\nparadigm built on top of them:test-time scaling. The aim\n*Equal contribution. ZY and NM started the project. WS, NM\nand ZY collected the prompts, XL, ZY and NM, built the data\npipeline, LZ and WS proposed using a 1K subset and ZY and\nNM built budget forcing.1 Stanford University.2 University of\nWashington, Seattle.3 Allen Institute for AI.4 Contextual AI.\n512 2048\n65\n75\n85\n95Accuracy (%)\nMathematical\nProblem Solving\n(MATH500)\n512 2048 8192\n0\n20\n40\n60\nCompetition\nMath\n(AIME24)\n1024 4096\n40\n50\n60\nPhD-Level\nScience Questions\n(GPQA Diamond)\nAverage thinking time (tokens)\nFigure 1.Test-time scaling with s1-32B.We benchmarks1-32B\non reasoning-intensive tasks and vary test-time compute.\nof this approach is to increase the compute at test time to\nget better results. There has been much work exploring this\nidea (Snell et al., 2024; Welleck et al., 2024), and the via-\nbility of this paradigm was recently validated by OpenAI\no1 (OpenAI, 2024). o1 has demonstrated strong reasoning\nperformance with consistent gains from scaling test-time\ncompute. OpenAI describes their approach as using large-\nscalereinforcementlearning(RL)implyingtheuseofsizable\namounts of data (OpenAI, 2024). This has led to various\nattempts to replicate their models relying on techniques like\nMonte Carlo Tree Search (Gao et al., 2024b; Zhang et al.,\n2024a), multi-agent approaches (Qin et al., 2024), and oth-\ners (Wang et al., 2024a; Huang et al., 2024b; 2025). Among\nthese approaches, DeepSeek R1 (DeepSeek-AI et al., 2025)\nhas successfully replicated o1-level performance, also em-\nploying reinforcement learning via millions of samples and\nmultiple training stages. However, despite the large num-\nber of o1 replication attempts, none have openly replicated\na clear test-time scaling behavior. Thus, we ask: what is\nthe simplest approach to achieve both test-time scaling and\nstrong reasoning performance?\nWeshowthattrainingononly1,000sampleswithnext-token\nprediction and controlling thinking duration via a simple\ntest-time technique we refer to asbudget forcingleads to\na strong reasoning model that scales in performance with\nmore test-time compute. Specifically, we constructs1K,\nwhich consists of 1,000 carefully curated questions paired\nwith reasoning traces and answers distilled from Gemini\nThinking Experimental (Google, 2024). We perform super-\nvised fine-tuning (SFT) of an off-the-shelf pretrained model\n1\narXiv:2501.19393v3  [cs.CL]  1 Mar 2025s1: Simple test-time scaling\non our small dataset requiring just 26 minutes of training\non 16 H100 GPUs. After training, we control the amount\nof test-time compute our model spends usingbudget forc-\ning: (I) If the model generates more thinking tokens than\na desired limit, we forcefully end the thinking process by\nappending an end-of-thinking token delimiter. Ending the\nthinking this way makes the model transition to generating\nitsanswer. (II)Ifwewantthemodeltospendmoretest-time\ncompute on a problem, we suppress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our mode\n\nCitation:"}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:32:33,876 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Muennighoff, Niklas, et al. \"s1: Simple Test-Time Scaling.\" *arXiv*, 1 Mar. 2025, arxiv.org/abs/2501.19393v3.\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.023861061643671105
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 1471,
    "candidatesTokenCount": 54,
    "totalTokenCount": 1525,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 1471
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 54
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "cdM2afPpBpWixN8P06or"
}



2025-12-08 14:32:33,877 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:33,878 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:33,878 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:33,878 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:33,879 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:33,879 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:33,879 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:33,879 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:33,879 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:33,879 - DEBUG - response_cost: 0.0
2025-12-08 14:32:33,880 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:33,880 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:32:33,880 - DEBUG - response_cost: 0.0
2025-12-08 14:32:33,880 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:33,880 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:33,881 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:33,881 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:33,881 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:33,881 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:33,881 - DEBUG - Async Response: ModelResponse(id='cdM2afPpBpWixN8P06or', created=1765200752, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Muennighoff, Niklas, et al. "s1: Simple Test-Time Scaling." *arXiv*, 1 Mar. 2025, arxiv.org/abs/2501.19393v3.\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=54, prompt_tokens=1471, total_tokens=1525, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=54, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=1471, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:32:33,881 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:32:33,882 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:32:33,882 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:33,882 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:33,882 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:33,882 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:33,882 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:33,882 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:33,883 - DEBUG - Inside async function with retries.
2025-12-08 14:32:33,883 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x120ca3290>>, num_retries - 3
2025-12-08 14:32:33,883 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:33,883 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:33,883 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:33,883 - DEBUG - retrieve cooldown models: []
2025-12-08 14:32:33,883 - DEBUG - async cooldown deployments: []
2025-12-08 14:32:33,883 - DEBUG - cooldown_deployments: []
2025-12-08 14:32:33,883 - DEBUG - cooldown deployments: []
2025-12-08 14:32:33,883 - DEBUG - 

2025-12-08 14:32:33,883 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:32:33,883 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Muennighoff, Niklas, et al. "s1: Simple Test-Time Scaling." *arXiv*, 1 Mar. 2025, arxiv.org/abs/2501.19393v3.\n\n\nCitation JSON:'}], caching=False, client=None, n=1, stream=False, litellm_trace_id='94e31305-6b85-43ea-a7c8-4e56716eff6b', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:32:33,884 - DEBUG - 

2025-12-08 14:32:33,884 - DEBUG - self.optional_params: {}
2025-12-08 14:32:33,884 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:32:33,884 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:32:33,884 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:33,884 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x120770f50>>
2025-12-08 14:32:33,885 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:32:33,885 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:33,885 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Muennighoff, Niklas, et al. "s1: Simple Test-Time Scaling." *arXiv*, 1 Mar. 2025, arxiv.org/abs/2501.19393v3.\n\n\nCitation JSON:'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:32:33,885 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:33,885 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:33,885 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:32:33,885 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:33,885 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:33,886 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:33,886 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Muennighoff, Niklas, et al. "s1: Simple Test-Time Scaling." *arXiv*, 1 Mar. 2025, arxiv.org/abs/2501.19393v3.\n\n\nCitation JSON:'}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:32:34,725 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"title\": \"s1: Simple Test-Time Scaling\",\n  \"authors\": [\n    \"Niklas Muennighoff\"\n  ],\n  \"doi\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.0023679534594217935
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 107,
    "candidatesTokenCount": 48,
    "totalTokenCount": 155,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 107
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 48
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "ctM2aZCKAZ2jxN8Pkd7w4Aw"
}



2025-12-08 14:32:34,727 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:34,727 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:34,727 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:34,728 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:34,728 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:34,728 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:34,728 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:34,728 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:34,728 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:34,729 - DEBUG - response_cost: 0.0
2025-12-08 14:32:34,729 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:34,729 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:32:34,729 - DEBUG - response_cost: 0.0
2025-12-08 14:32:34,730 - DEBUG - Async Response: ModelResponse(id='ctM2aZCKAZ2jxN8Pkd7w4Aw', created=1765200753, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='```json\n{\n  "title": "s1: Simple Test-Time Scaling",\n  "authors": [\n    "Niklas Muennighoff"\n  ],\n  "doi": null\n}\n```', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=48, prompt_tokens=107, total_tokens=155, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=48, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=107, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:32:34,730 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:34,730 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:32:34,730 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:34,731 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:34,731 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:34,731 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:34,731 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:32:34,731 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:34,731 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:34,732 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:34,732 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:34,762 - DEBUG - Attempting to populate metadata query: {'authors': ['Niklas Muennighoff'], 'title': 's1: Simple Test-Time Scaling', 'session': <aiohttp.client.ClientSession object at 0x120a68990>} via providers=[<paperqa.clients.crossref.CrossrefProvider object at 0x120771ed0>, <paperqa.clients.semantic_scholar.SemanticScholarProvider object at 0x120773750>] processors=[<paperqa.clients.journal_quality.JournalQualityPostProcessor object at 0x120771c50>]
2025-12-08 14:32:34,762 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x120773310>>
2025-12-08 14:32:34,762 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:34,763 - WARNING - SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.
2025-12-08 14:32:34,763 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:36,926 - DEBUG - connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2025-12-08 14:32:36,951 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12090af90>
2025-12-08 14:32:36,951 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x120ddbf50> server_hostname='raw.githubusercontent.com' timeout=5
2025-12-08 14:32:36,979 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12090b010>
2025-12-08 14:32:36,979 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:32:36,979 - DEBUG - send_request_headers.complete
2025-12-08 14:32:36,979 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:32:36,979 - DEBUG - send_request_body.complete
2025-12-08 14:32:36,979 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:32:37,003 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'52540'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3cd138251572ce0662c39b6cee3c3fb9dc040f283d3f3d4afe7b0952ce409f78"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'8389:10C5B:28F09C4:2ED595D:6936D36F'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 08 Dec 2025 13:32:37 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-lin1730080-LIN'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'1'), (b'X-Timer', b'S1765200757.064090,VS0,VE2'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'991bb8f89699a00d65571111d81d745b0971eed7'), (b'Expires', b'Mon, 08 Dec 2025 13:37:37 GMT'), (b'Source-Age', b'5')])
2025-12-08 14:32:37,004 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:32:37,026 - DEBUG - receive_response_body.complete
2025-12-08 14:32:37,026 - DEBUG - response_closed.started
2025-12-08 14:32:37,026 - DEBUG - response_closed.complete
2025-12-08 14:32:37,026 - DEBUG - close.started
2025-12-08 14:32:37,026 - DEBUG - close.complete
2025-12-08 14:32:37,031 - DEBUG - 

2025-12-08 14:32:37,031 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:32:37,031 - DEBUG - [92mlitellm.aembedding(model='text-embedding-3-small', input=['s1: Simple test-time scaling\nNiklas Muennighoff*134 Zitong Yang*1 Weijia Shi*23 Xiang Lisa Li*1 Li Fei-Fei1 Hannaneh Hajishirzi23\nLuke Zettlemoyer2 Percy Liang1 Emmanuel Cands1 Tatsunori Hashimoto1\nAbstract\nTest-time scaling is a promising new approach to\nlanguage modeling that uses extra test-time com-\nputetoimproveperformance. Recently,OpenAIs\no1 model showed this capability but did not pub-\nlicly share its methodology, leading to many repli-\ncation efforts. We seek the simplest approach to\nachievetest-timescalingandstrongreasoningper-\nformance. First, we curate a small datasets1K\nof 1,000 questions paired with reasoning traces\nrelying on three criteria we validate through abla-\ntions: difficulty,diversity,andquality. Second,we\ndevelop budget forcing to control test-time com-\npute by forcefully terminating the models think-\ning process or lengthening it by appending Wait\nmultiple times to the models generation when it\ntries to end. This can lead the model to double-\ncheck its answer, often fixing incorrect reasoning\nsteps. After supervised finetuning the Qwen2.5-\n32B-Instruct language model ons1Kand equip-\nping it with budget forcing, our models1-32B ex-\nceeds o1-preview on competition math questions\nby up to 27% (MATH and AIME24). Further,\nscaling s1-32B with budget forcing allows extrap-\nolating beyond its performance without test-time\nintervention: from 50% to 57% on AIME24. Our\nmodel, data, and code are open-source athttps:\n//github.com/simplescaling/s1.\n1. Introduction\nPerformance improvements of language models (LMs) over\nthe past years have largely relied on scaling up train-time\ncompute using large-scale self-supervised pretraining (Ka-\nplan et al., 2020; Hoffmann et al., 2022). The creation of\nthese powerful models has set the stage for a new scaling\nparadigm built on top of them:test-time scaling. The aim\n*Equal contribution. ZY and NM started the project. WS, NM\nand ZY collected the prompts, XL, ZY and NM, built the data\npipeline, LZ and WS proposed using a 1K subset and ZY and\nNM built budget forcing.1 Stanford University.2 University of\nWashington, Seattle.3 Allen Institute for AI.4 Contextual AI.\n512 2048\n65\n75\n85\n95Accuracy (%)\nMathematical\nProblem Solving\n(MATH500)\n512 2048 8192\n0\n20\n40\n60\nCompetition\nMath\n(AIME24)\n1024 4096\n40\n50\n60\nPhD-Level\nScience Questions\n(GPQA Diamond)\nAverage thinking time (tokens)\nFigure 1.Test-time scaling with s1-32B.We benchmarks1-32B\non reasoning-intensive tasks and vary test-time compute.\nof this approach is to increase the compute at test time to\nget better results. There has been much work exploring this\nidea (Snell et al., 2024; Welleck et al., 2024), and the via-\nbility of this paradigm was recently validated by OpenAI\no1 (OpenAI, 2024). o1 has demonstrated strong reasoning\nperformance with consistent gains from scaling test-time\ncompute. OpenAI describes their approach as using large-\nscalereinforcementlearning(RL)implyingtheuseofsizable\namounts of data (OpenAI, 2024). This has led to various\nattempts to replicate their models relying on techniques like\nMonte Carlo Tree Search (Gao et al., 2024b; Zhang et al.,\n2024a), multi-agent approaches (Qin et al., 2024), and oth-\ners (Wang et al., 2024a; Huang et al., 2024b; 2025). Among\nthese approaches, DeepSeek R1 (DeepSeek-AI et al., 2025)\nhas successfully replicated o1-level performance, also em-\nploying reinforcement learning via millions of samples and\nmultiple training stages. However, despite the large num-\nber of o1 replication attempts, none have openly replicated\na clear test-time scaling behavior. Thus, we ask: what is\nthe simplest approach to achieve both test-time scaling and\nstrong reasoning performance?\nWeshowthattrainingononly1,000sampleswithnext-token\nprediction and controlling thinking duration via a simple\ntest-time technique we refer to asbudget forcingleads to\na strong reasoning model that scales in performance with\nmore test-time compute. Specifically, we constructs1K,\nwhich consists of 1,000 carefully curated questions paired\nwith reasoning traces and answers distilled from Gemini\nThinking Experimental (Google, 2024). We perform super-\nvised fine-tuning (SFT) of an off-the-shelf pretrained model\n1\narXiv:2501.19393v3  [cs.CL]  1 Mar 2025s1: Simple test-time scaling\non our small dataset requiring just 26 minutes of training\non 16 H100 GPUs. After training, we control the amount\nof test-time compute our model spends usingbudget forc-\ning: (I) If the model generates more thinking tokens than\na desired limit, we forcefully end the thinking process by\nappending an end-of-thinking token delimiter. Ending the\nthinking this way makes the model transition to generating\nitsanswer. (II)Ifwewantthemodeltospendmoretest-time\ncompute on a problem, we suppress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our mode', 'ress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our models1-\n32Bexhibits test-time scaling (Figure 1). Further,s1-32Bis\nthe most sample-efficient reasoning model and outperforms\nclosed-source models like OpenAIs o1-preview (Figure 2).\nWe conduct extensive ablation experiments targeting (a) our\nselection of 1,000 (1K) reasoning samples and (b) our test-\ntime scaling. For (a), we find that jointly incorporating\ndifficulty, diversity, and quality measures into our selec-\ntion algorithm is important. Random selection, selecting\nsamples with the longest reasoning traces, or only selecting\nmaximally diverse samples all lead to significantly worse\nperformance (around30% on AIME24 on average). Train-\ning on our full data pool of 59K examples, a superset of\ns1K, does not offer substantial gains over our 1K selection.\nThis highlights the importance of careful data selection and\nechoes prior findings for instruction tuning (Zhou et al.,\n2023). For(b), we define desiderata for test-time scaling\nmethods to compare different approaches. Budget forcing\nleads to the best scaling as it has perfect controllability with\na clear positive slope leading to strong performance.\nInsummary,ourcontributionsare: Wedevelopsimplemeth-\nods for creating a sample-efficient reasoning dataset (2)\nand test-time scaling (3); Based on these we builds1-32B\nwhich is competitive with o1-preview (4); We ablate sub-\ntleties of data (5.1) and test-time scaling (5.2). We end\nwith a discussion to motivate future work on simple rea-\nsoning (6). Our code, model, and data are open-source at\nhttps://github.com/simplescaling/s1.\n2. Reasoning data curation to create s1K\nIn this section, we describe our process for creating a large\ndataset first in 2.1 and then filtering it down tos1Kin 2.2.\n2.1. Initial collection of 59K samples\nWecollectaninitial59,029questionsfrom16sourcesfollow-\ning three guiding principles.Quality: Datasets should be\nhigh-quality; we always inspect samples and ignore datasets\nwith, e.g., poor formatting;Difficulty: Datasets should be\nchallenging and require significant reasoning effort;Diver-\nsity: Datasetsshouldstemfromvariousfieldstocoverdiffer-\nent reasoning tasks. We collect datasets of two categories:\nCuration of existing datasetsOur largest source is Nu-\nminaMATH (LI et al., 2024) with 30,660 mathematical\nproblems from online websites. We also include histori-\ncal AIME problems (1983-2021). To enhance diversity,\nwe add OlympicArena (Huang et al., 2024a) with 4,250\nquestions spanning Astronomy, Biology, Chemistry, Com-\nputer Science, Geography, Mathematics, and Physics from\nvarious Olympiads. OmniMath (Gao et al., 2024a) adds\n4,238 competition-level mathematics problems. We also\ninclude 2,385 problems from AGIEval (Zhong et al., 2023),\nwhich features questions from standardized tests like SAT\nand LSAT, covering English, Law, and Logic. We refer to\nTable 7 in C for our other sources.\nNew datasets in quantitative reasoning To comple-\nment these existing datasets, we create two original\ndatasets. s1-prob consists of 182 questions from the prob-\nability section of Stanford Universitys Statistics Depart-\nments PhD Qualifying Exams (https://statistics.\nstanford.edu), accompanied by handwritten solutions\nthat cover difficult proofs. The probability qualifying exam\nis held yearly and requires professional-level mathemati-\ncal problem-solving. s1-teasers comprises 23 challenging\nbrain-teasers commonly used in interview questions for\nquantitative trading positions. Each sample consists of a\nproblem and solution taken from PuzzledQuant (https:\n//www.puzzledquant.com/). Weonlytakeexamples\nwith the highest difficulty level ("Hard").\nFor each question, we generate a reasoning trace and solu-\ntion using the Google Gemini Flash Thinking API (Google,\n2024)extractingitsreasoningtraceandresponse. Thisyields\n59K triplets of a question, generated reasoning trace, and\ngenerated solution. Examples from our dataset are in D.2.\nWe decontaminate all samples against our evaluation ques-\ntions (MATH500, GPQA Diamond, AIME24; C.5) using\n8-grams and deduplicate the data.\n2.2. Final selection of 1K samples\nWe could directly train on our pool of 59K questions, how-\never, our goal is to find thesimplest approach with minimal\nresources. Thus, we go through three stages of filtering to\narriveataminimalsetof1,000samplesrelyingonourthree\nguiding data principles: Quality, Difficulty, and Diversity.\nQuality We first remove any questions where we ran into\nanyAPIerrorsreducingourdatasetto54,116samples. Next,\nwefilteroutlow-qualityexamplesbycheckingiftheycontain\nany string patterns with formatting issues, such as ASCII\nart diagrams, non-existent image references, or inconsistent\nquestionnumberingreducingourdatasetto51,581examples.\n2s1: Simple test-time scaling\nGeometry', '-qualityexamplesbycheckingiftheycontain\nany string patterns with formatting issues, such as ASCII\nart diagrams, non-existent image references, or inconsistent\nquestionnumberingreducingourdatasetto51,581examples.\n2s1: Simple test-time scaling\nGeometry Number\ntheory\nCombin\natorics\nReal\nfunctions\nBiology\nComplex\nfunctions\nQuantum\ntheory\nField\ntheory\nCalculus of\nvariations\nDifference\nequations\nElectro\ndynamics\nGroup\ntheory\nLinear\nalgebra\nProbability\ntheory\nAlgebraic\nsystems\nMechanics\nThermo\ndynamics\nDifferential\nequations\nComputer\nscience\nNumerical\nanalysis\nCalculus\nAlgebraic\nstructures\nAstronomy\nDynamical\nsystems\nStatistical\nmechanics\nOperations\nresearch\nMath\nematics\neducation\nMeasure\ntheory\nConvex\ngeometry\nFluid\nmechanics\nAlgebraic\ngeometry\nStatistics General\ntopology\nEconomics\nAssociative\nrings\nGeneral\nrelativity\nDifferential\ngeometry\nMath\nematical\nlogic\nPartial\ndifferential\nequations\nInformation\ntheory\nSolid\nmech\nanics\nFunctional\nanalysis\nSpecial\nfunctions\nComm\nutative\nalgebra\nIntegral\nequations\nIntegral\ntransform\nApproxi\nmation\ntheory\nPotential\ntheory\nHarmonic\nanalysis\nControl\ntheory\nGeo\nphysics\n1000 17000 800000 N/A\nNumber of Examples\n80\n85\n90\n95\n100MATH500 Accuracy (%)\ns1\nr1-distill\nSky-T1\nQwQ\nBespoke-Stratos\no1-preview\nMost sample-efficient\nFigure 2.s1Kands1-32B. (left)s1Kisadatasetof1,000high-quality,diverse,anddifficultquestionswithreasoningtraces. (right)s1-32B,\na 32B parameter model finetuned ons1K is on the sample-efficiency frontier. See Table 1 for details on other models.\nFrom this pool, we identify 384 samples for our final 1,000\nsamples from datasets that we perceive as high-quality and\nnot in need of further filtering (see C.4 for details).\nDifficulty Fordifficulty, weusetwoindicators: modelper-\nformance and reasoning trace length. We evaluate two mod-\nels on each question: Qwen2.5-7B-Instruct and Qwen2.5-\n32B-Instruct (Qwen et al., 2024), with correctness assessed\nby Claude 3.5 Sonnet comparing each attempt against the\nreference solution (see C.3 for the grading protocol). We\nmeasure the token length of each reasoning trace to indicate\nproblem difficulty using the Qwen2.5 tokenizer. This relies\non the assumption that more difficult problems require more\nthinkingtokens. Basedonthegrading,weremovequestions\nthat either Qwen2.5-7B-Instruct or Qwen2.5-32B-Instruct\ncan solve correctly and thus may be too easy. By using two\nmodels we reduce the likelihood of an easy sample slipping\nthrough our filtering due to a rare mistake on an easy ques-\ntion of one of the models. This brings our total samples\ndown to 24,496, setting the stage for the next round of sub-\nsampling based on diversity. While filtering with these two\nmodels may be optimized for our setup as we will also use\nQwen2.5-32B-Instruct as our model to finetune, the idea of\nmodel-based filtering generalizes to other setups.\nDiversity To quantify diversity, we classify questions into\ndomainsusingClaude3.5SonnetbasedontheMathematics\nSubject Classification (MSC) system (e.g., geometry, com-\nbinatorics, etc.) from the American Mathematical Society.1\nThe taxonomy focuses on topics in mathematics but also\nincludes other sciences such as biology, physics, and eco-\n1https://mathscinet.ams.org/mathscinet/\nmsc/msc2020.html\nnomics. Toselectourfinalexamplesfromthepoolof24,496\nquestions, we first choose one domain uniformly at random.\nThen, we sample one problem from this domain according\ntoadistributionthatfavorslongerreasoningtraces(seeC.4\nfordetails)asmotivatedin Difficulty. Werepeatthisprocess\nuntil we have 1,000 total samples spanning 50 domains.\nIn 5.1, we will show that using our three criteria in combi-\nnation is important, as only relying on quality, diversity, or\ndifficulty in isolation leads to worse datasets. Some distilled\ngenerations are incorrect, which we allow in our data as we\nfocusoncapturingthereasoningprocessratherthanentirely\ncorrect solutions. Our grader (C.3) deems 53.6% correct\nin s1Kand 63.0% in our follow-ups1K-1.1(see A).\n3. Test-time scaling\n3.1. Method\nWe classify test-time scaling methods into1) Sequential,\nwherelatercomputationsdependonearlierones(e.g.,along\nreasoningtrace),and 2)Parallel,wherecomputationsrunin-\ndependently(e.g.,majorityvoting)(Snelletal.,2024;Brown\netal.,2024). Wefocusonsequentialscalingasintuitivelywe\nbelieve it should scale better, since later computations can\nbuild on intermediate results, allowing for deeper reasoning\nanditerativerefinement. Weproposenewsequentialscaling\nmethods and ways to benchmark them.\nBudget forcing We propose a simple decoding-time in-\ntervention by forcing a maximum and/or minimum number\nof thinking tokens. Specifically, we enforce a maximum\ntoken count by simply appending the end-of-thinking token\ndelimiter and optionally Final Answer: to early exit\n3s1: Simple test-time scaling\nHo w many r in r aspberry ?\nLet \' s br eak do wn the pr ocess of counting the letter \' r \' in the \nwor d " r aspberry " ...\n* First letter: \' r \' - This is an \' r \', count = 1.\n* Secon', 'nally Final Answer: to early exit\n3s1: Simple test-time scaling\nHo w many r in r aspberry ?\nLet \' s br eak do wn the pr ocess of counting the letter \' r \' in the \nwor d " r aspberry " ...\n* First letter: \' r \' - This is an \' r \', count = 1.\n* Second letter: \' a \' - Not an \' r \', count r emains 1 ...\n* Sixth letter: \' e \' - Not an \' r \', count r emains 1.\n* Se venth letter: \' r \' - This is an \' r \', count = 2.\n* Eighth letter: \'y \' - Not an \' r \', count r emains 2 ...\nThe number of \' r \' s in " r aspberry " is 2.\n, let \' s r e-r ead the question car efully . It asks "Ho w many r \nin r aspberry ?" ... * r - a - s - p - b - e - r - r - y ... * First \' r \' ... * \nSecond \' r \' ... * Thir d \' r \' ... Count = 3 ...\nW ait\nMy initial answer of 2 was incorr ect due to a quick r eading of \nthe wor d. Final Answer: The final answer is\nQuestion\nReasoning tr ace\nResponse\nFigure 3.Budget forcing with s1-32B.The model tries to stop\nafter ...is 2., but we suppress the end-of-thinking token delimiter\ninstead appending Wait leadings1-32Bto self-correct its answer.\nthe thinking stage and make the model provide its current\nbest answer. To enforce a minimum, we suppress the gener-\nation of the end-of-thinking token delimiter and optionally\nappend the string Wait to the models current reasoning\ntrace to encourage the model to reflect on its current gen-\neration. Figure 3 contains an example of how this simple\napproach can lead the model to arrive at a better answer.\nBaselines We benchmark budget forcing with:(I) Con-\nditional length-control methods, which rely on telling the\nmodel in the prompt how long it should generate for. We\ngroup them by granularity into (a) Token-conditional con-\ntrol: We specify an upper bound of thinking tokens in the\nprompt; (b) Step-conditional control: We specify an upper\nbound of thinking steps, where each step is around 100 to-\nkens; (c) Class-conditional control: We write two generic\npromptsthattellthemodeltoeitherthinkforashortorlong\namount of time (see E.1 for details).(II) Rejection sam-\npling, which samples until a generation fits a predetermined\ncompute budget. This oracle captures the posterior over\nresponses conditioned on its length.\n3.2. Metrics\nWe establish a set of desiderata as evaluation metrics to\nmeasure test-time scaling across methods. Importantly, we\ndo not only care about the accuracy a method can achieve\nbut also its controllability and test-time scaling slope. For\neach method we consider, we run a set of evaluations\n\ue22d varying test-time compute on a fixed benchmark, e.g.\nAIME24. This produces a piece-wise linear function\nwith compute as the x-axis measured in thinking tokens and\naccuracyasthey-axis(seeFigure1,wheretherightmostdot\nfor AIME24 corresponds to(7320)=57% ). We measure\nthree metrics:\nControl = 1\n|\ue22d|\n\n\ue22d\n(min   max) (1)\nwhere min,max refer to a pre-specified minimum and max-\nimum amount of test-time compute; in our case thinking\ntokens. We usually only constrainmax. As tokens gener-\nated correspond to the amount of test-time compute spent,\nthis metric measures the extent to which a method allows\ncontrollability over the use of that test-time compute. We\nreport it as a percentage with 100% being perfect control.\nScaling = 1(|\ue22d|\n2\n)\n\n,\ue22d\n>\n() ()\n (2)\nScalingistheaverageslopeofthepiece-wiselinearfunction.\nIt must be positive for useful methods and larger is better.\nPerformance =max\n\ue22d\n() (3)\nPerformance is simply the maximum performance the\nmethod achieves on the benchmark. A method with mono-\ntonically increasing scaling achieves 100% performance on\nany benchmark in the limit. However, the methods we in-\nvestigate eventually flatten out or further scaling fails due to\ncontrol or context window limitations.\n4. Results\n4.1. Setup\nTraining We perform supervised finetuning on Qwen2.5-\n32B-Instruct usings1Kto obtain our models1-32Busing\nbasic hyperparameters outlined in D. Finetuning took 26\nminutes on 16 NVIDIA H100 GPUs with PyTorch FSDP.\nEvaluation We select three representative reasoning\nbenchmarks widely used in the field:AIME24(of America,\n2024)has30problemsthatwereusedinthe2024American\nInvitational Mathematics Examination (AIME) held from\nJanuary 31  February 1, 2024. AIME tests mathematical\nproblem-solving with arithmetic, algebra, counting, geome-\ntry, number theory, probability, and other secondary school\nmathtopics. High-scoringhighschoolstudentsinthetestare\ninvitedtoparticipateintheUnitedStatesofAmericaMathe-\nmaticsOlympiad(USAMO).AllAIMEanswersareintegers\nranging from000to999, inclusive. Some AIME problems\nrely on figures that we provide to our model using the vec-\ntor graphics language Asymptote as it cannot take image\ninputs. MATH500(Hendrycks et al., 2021) is a benchmark\nof competition math problems of varying difficulty. We\nevaluate on the same 500 samples selected by OpenAI in\nprior work (Lightman et al., 2023).GPQA Diamond(Rein\net al., 2023) consists of 198 PhD-level science questions\n4s1: Simple test-time scaling\nF', ' competition math problems of varying difficulty. We\nevaluate on the same 500 samples selected by OpenAI in\nprior work (Lightman et al., 2023).GPQA Diamond(Rein\net al., 2023) consists of 198 PhD-level science questions\n4s1: Simple test-time scaling\nFor cing 2048/4096 max thinking \ntokens\nIgnoring end-of-thinking 2x/4x/6x and \nappending  W ait  thus for cing it to continue \nr easoning when it tries to stop\n(a) Sequential scaling via budget forcing\nPar allel scaling via Majority \nvoting with base model \n(Qwen2.5-32B-Instruct)\nSequential scaling \nvia For cing with s1\n60% accur acy (b) Parallel scaling via majority voting\nFigure 4.Sequential and parallel test-time scaling.(a): Budget forcing shows clear scaling trends and extrapolates to some extent. For\nthe three rightmost dots, we prevent the model from stopping its thinking 2/4/6 times, each time appending Wait to its current reasoning\ntrace. (b): For Qwen2.5-32B-Instruct we perform 64 evaluations for each sample with a temperature of 1 and visualize the performance\nwhen majority voting across 2, 4, 8, 16, 32, and 64 of these.\nfromBiology,ChemistryandPhysics. ExpertswithPhDsin\nthe corresponding domains only achieved 69.7% on GPQA\nDiamond (OpenAI, 2024). When we write GPQA in the\ncontext of evaluation in this work, we always refer to the\nDiamond subset. We build on the lm-evaluation-harness\nframework (Gao et al., 2021; Biderman et al., 2024). Unless\notherwise specified, we evaluate with a temperature of 0\n(greedy) and measure accuracy (equivalent to pass@1).\nOther models We benchmarks1-32Bagainst: OpenAI\no1 series(OpenAI, 2024), closed-source models that pop-\nularized test-time scaling;DeepSeek r1 series(DeepSeek-\nAI et al., 2025), open-weight reasoning models with up to\no1-level performance; QwensQwQ-32B-preview(Team,\n2024), a 32B open-weight reasoning model without dis-\nclosed methodology;Sky-T1-32B-Preview(Team, 2025)\nandBespoke-32B(Labs, 2025), open models with open rea-\nsoningdatadistilledfromQwQ-32B-previewandr1; Google\nGemini 2.0 Flash Thinking Experimental(Google, 2024),\nthe API that we distill from. As it has no official evaluation\nscores, we use the Gemini API to benchmark it ourselves.\nHowever, the recitation error of the Gemini API makes\nevaluation challenging.2 We circumvent this, by manually\ninsertingall30AIME24questionsinitswebinterfacewhere\ntheerrordoesnotappear. However,weleaveoutMATH500\n(500 questions) and GPQA Diamond (198 questions), thus\nthey are N.A. in Table 1. Our model,s1-32B, is fully open\nincluding weights, reasoning data, and code.\n2https://github.com/google/\ngenerative-ai-docs/issues/257\nTable 1.s1-32B is a strong open reasoning model.We evaluate\ns1-32B, Qwen, and Gemini (some entries are unknown (N.A.), see\n4). Other results are from the respective reports (Qwen et al.,\n2024; Team, 2024; OpenAI, 2024; DeepSeek-AI et al., 2025; Labs,\n2025; Team, 2025). # ex. = number examples used for reasoning\nfinetuning; BF = budget forcing. See A for our betters1.1 model.\nModel # ex. AIME\n2024\nMATH\n500\nGPQA\nDiamond\nAPI only\no1-preview N.A. 44.6 85.5 73.3\no1-mini N.A. 70.0 90.0 60.0\no1 N.A. 74.4 94.8 77.3\nGemini 2.0 N.A. 60.0 N.A. N.A.Flash Think.\nOpen Weights\nQwen2.5- N.A. 26.7 84.0 49.032B-Instruct\nQwQ-32B N.A. 50.0 90.6 54.5\nr1 800K 79.8 97.3 71.5\nr1-distill 800K 72.6 94.3 62.1\nOpen Weights and Open Data\nSky-T1 17K 43.3 82.4 56.8\nBespoke-32B 17K 63.3 93.0 58.1\ns1 w/o BF 1K 50.0 92.6 56.6\ns1-32B 1K 56.7 93.0 59.6\n5s1: Simple test-time scaling\n4.2. Performance\nTest-time scaling Figure 1 shows the performance ofs1-\n32Bwithbudgetforcingscaleswithmoretest-timecompute.\nInFigure4(left),weexpandtheplotfromFigure1(middle)\nshowing that while we can improve AIME24 performance\nusing our budget forcing technique (3) and more test-time\ncomputeitdoeseventuallyflattenoutatsixtimes. Suppress-\ningtheend-of-thinkingtokendelimitertoooftencanleadthe\nmodel into repetitive loops instead of continued reasoning.\nIn Figure 4 (right), we show that after training Qwen2.5-\n32B-Instruct on our 1,000 samples to produces1-32Band\nequipping it with the simple budget forcing technique, it\noperates in a different scaling paradigm. Scaling test-time\ncomputeonthebasemodelviamajorityvotingcannotcatch\nup with the performance ofs1-32B which validates our\nintuition from 3 that sequential scaling is more effective\nthan parallel. We provide example generations ofs1-32B in\nFigure 5.\nSample-efficiency In Figure 2 (right) and Table 1 we\ncompare s1-32B with other models. We find that s1-\n32B is the most sample-efficient open data reasoning\nmodel. It performs significantly better than our base model\n(Qwen2.5-32B-Instruct) despite just training it on an ad-\nditional 1,000 samples. The concurrently released r1-32B\nshows stronger performance thans1-32B while also only\nusingSFT(DeepSeek-AIetal.,2025). However,itistrained\non 800more reasoning samples. It is an open question\nwhether one can achieve their performance with just 1,000\nsamples. Finally, our model nearly matches Gemin', 'rformance thans1-32B while also only\nusingSFT(DeepSeek-AIetal.,2025). However,itistrained\non 800more reasoning samples. It is an open question\nwhether one can achieve their performance with just 1,000\nsamples. Finally, our model nearly matches Gemini 2.0\nThinking on AIME24. As the data fors1-32B is distilled\nfrom Gemini 2.0, this shows our distillation procedure was\nlikely effective.\n5. Ablations\n5.1. Data Quantity, Diversity, and Difficulty\nIn 2 we outlined our three guiding principles in curating\ns1K: Quality, Difficulty, and Diversity. Here we test the\nimportanceofcombiningthemandtheoverallefficacyofour\nselection. Only Quality (1K-random): After obtaining our\nhigh-quality reasoning chains from Gemini, we select 1,000\nsamplesatrandom;notrelyingonourdifficultyanddiversity\nfiltering at all. Table 2 shows this approach performs much\nworse thans1K across all benchmarks. Only Diversity\n(1K-diverse): For this dataset, we sample uniformly across\ndomains to maximize diversity disregarding any notion of\ndifficulty. This approach also leads to poor performance\nsimilar to 1K-random.Only Difficulty (1K-longest): Here\nwerelyononeofourdifficultyindicatorsintroducedin2by\nselectingthe1,000sampleswiththelongestreasoningtraces.\nThis approach significantly boosts GPQA performance but\noverall still falls short of usings1K.Maximize Quantity:\nTable 2.s1K data ablations.We budget force (BF) a maximum\nof around 30,000 thinking tokens for all scores in this table. This\nperforms slightly better than the scores without BF (Table 1) as\nit allows the model to finish with a best guess when stuck in an\ninfinite loop. We report 95% paired bootstrap confidence intervals\nfor differences relative to thes1K model using 10,000 bootstrap\nsamples. E.g., the interval [-13%, 20%] means that, with 95% con-\nfidence, the true difference between 59K-full ands1Kis between\n-13% and +20%. If the entire interval is negative, e.g. [-27%, -3%],\nwe can confidently say that the performance is worse thans1K.\nModel AIME\n2024\nMATH\n500\nGPQA\nDiamond\n1K-random 36.7 90.6 52.0\n[-26.7%, -3.3%] [-4.8%, 0.0%] [-12.6%, 2.5%]\n1K-diverse 26.7 91.2 54.6\n[-40.0%, -10.0%] [-4.0%, 0.2%] [-10.1%, 5.1%]\n1K-longest 33.3 90.4 59.6\n[-36.7%, 0.0%] [-5.0%, -0.2%] [-5.1%, 10.1%]\n59K-full 53.3 92.8 58.1\n[-13.3%, 20.0%] [-2.6%, 2.2%] [-6.6%, 8.6%]\ns1K 50.0 93.0 57.6\nFinally, we compare with just training on all of our 59K\nsamples, a superset of all the 1K-sample versions. This\nleads to a strong model but uses much more resources. To\nfinetune on 59K samples, we use 394 H100 GPU hours\nwhiles1-32Bonly required 7 H100 GPU hours. Moreover,\nrelyingonlyon s1Kisextremelycompetitiveasshownin2.\nOverall, combining all three criteria Quality, Difficulty,\nDiversity  via our methodology in 2 is key for sample-\nefficient reasoning training.\n5.2. Test-time scaling methods\nTable 3.Ablations on methods to scale test-time compute on\nAIME24. |\ue22d| refers to the number of evaluation runs used to\nestimate the properties; thus a higher value indicates more robust-\nness. Bold indicates our chosen method and the best values. BF\n= budget forcing, TCC/SCC/CCC = token/step/class-conditional\ncontrol, RS = rejection sampling.\nMethod Control Scaling Performance |\ue22d|\nBF 100% 15 56.7 5\nTCC 40% -24 40.0 5\nTCC + BF 100% 13 40.0 5\nSCC 60% 3 36.7 5\nSCC + BF 100% 6 36.7 5\nCCC 50% 25 36.7 2\nRS 100% -35 40.0 5\n6s1: Simple test-time scaling\n[AIME24]\nAlice and Bob play the following game. A stack of\ntokens lies before them. The players take turns\nwith Alice going first. On each turn, the player\nremoves either1token or4tokens from the stack.\nWhoever removes the last token wins. Find the\nnumber of positive integersless than or equal to\n2024for which there exists a strategy for Bob that\nguarantees that Bob will win the game regardless\nof Alices play.\nThe problem asks for the number of positive integers\n 2024such that Bob has a winning strategy. This\nis a typical impartial game that can be analyzed using\nthe Sprague-Grundy theorem or by directly finding\nthe winning and losing positions. Let be the set of\nwinning positions (number of tokenssuch that the\nplayer whose turn it is can force a win) andbe the\nset of losing positions. [...]\nLet()be the statement "the player whose turn it is\nwhen there aretokens has a winning strategy.(0)\nis false (the player whose turn it is and faces0tokens\nhas already lost). Consider the small values of:\n = 1: The player whose turn it is can take1token\nand win. So(1)is true.1 .\n = 2: The player whose turn it is can take1token,\nleaving1token for the opponent. Since1 , the\nopponent wins. So(2)is false.2 .\n = 3: The player whose turn it is can take1token,\nleaving2tokens for the opponent. Since2 , the\nplayer whose turn it is wins. So(3)is true. 3 \n.[...]\nWe are looking for the number of positive integers\n 2024for which Bob has a winning strategy. This\nmeans that if the game starts withtokens, and Alice\ngoes first, Bob can win. This happens if and only if\nis a losing pos', 'So(3)is true. 3 \n.[...]\nWe are looking for the number of positive integers\n 2024for which Bob has a winning strategy. This\nmeans that if the game starts withtokens, and Alice\ngoes first, Bob can win. This happens if and only if\nis a losing position (because if, then Alices\nfirst move must lead to a winning position for Bob).\n[...]\nLets try to find a pattern in the losing positions\n={0,2,5,7,10,12,15,17,20,22,...}. Differences\nbetween consecutive terms: 2,3,2,3,2,3,2,3,2,...\nThe pattern of differences is alternating2,3. [...]\nThe losing positions are of the form5or 5+2,\nwhere  0. We need to find the number of positive\nintegers 2024thatareinthesetoflosingpositions\n. [...]\nCase 1:  = 5, where  1. 1  5  2024\n15    20245 = 404.8Since is an integer,\n1  404. The number of values ofis404.\nCase 2: = 5+2, where  0. 5+2  2024\n5  2022  20225 = 404.4Since is an in-\nteger, 0    404. The number of values ofis\n4040+1=405 .\nThe total number of positive integers  2024that\nare losing positions is404+405= 809 .\n[MATH500]\nFind the projection ofonto  =(2\n6\n3\n)\nif=8.\nTheprojectionofvector  ontovector\n is given by the formula:\nproj =  \n2\nWearegiven  =\n(2\n6\n3\n)\nand  =8.\nFirst, we need to find the magnitude\nsquared of, which is2.\n2 = =22+62+32 =4+36+9=49 .\nNow we can plug the values into the\nprojection formula:\nproj = 8\n49\nSubstitute the vector:\nproj = 8\n49\n(2\n6\n3\n)\nMultiply the scalar by each compo-\nnent of the vector:\nproj =\n\n\n\n\n8\n49 2\n8\n49 6\n8\n49 3\n\n\n\n\n=\n\n\n\n\n16\n4948\n4924\n49\n\n\n\n\nFinal Answer: The final answer is\n\n\n\n\n16\n4948\n4924\n49\n\n\n\n\n[GPQA]\nA quantum mechanical particle of\nmassmoves in two dimensions in\nthe following potential, as a function\nof the polar coordinates(,):\n(,)= 1\n22 + 3\n22cos2()\nFind the energy spectrum. Hint:\nWrite the potential in Cartesian co-\nordinates.\nThe problem asks for the energy spec-\ntrum of a particle in a given 2D po-\ntential. The potential is given in polar\ncoordinates(,):\n(,)= 1\n22 + 3\n22cos2().\nWe can rewrite the potential as:\n(,)= 1\n22(1+3cos 2()).\nThe Hamiltonian for a particle in 2D\nis given by:\n = 2\n2\n(\n2\n2 + 2\n2\n)\n+(,).\nIn polar coordinates(,), the Lapla-\ncian operator is:\n2 = 1\n\n\n\n(\n \n\n)\n+ 1\n2\n2\n2.\nSo the Hamiltonian in polar coordi-\nnates is:\n = 2\n2\n[\n1\n\n\n\n(\n \n\n)\n+ 1\n2\n2\n2\n]\n+(,).\nThe time-independent Schrdinger\nequation is = . We can try\nto separate variables by assuming a so-\nlution of the form(,)= ()().\n[...]\nThis confirms the energy spectrum is:\n =\n\n\n\n(\n2 + + 3\n2\n)\n.\nFinal Answer:  =\n\n\n\n\n(\n2 + + 3\n2\n)\n.\nFigure 5.Example model outputs.We pick a question from AIME24 (left), MATH500 (middle), and GPQA (right), where our model\ngenerates the correct answer. The black text is the prompt, the light blue text is the reasoning trace, and the blue text is the answer of\ns1-32B. The gray ellipsis [...] indicates that the text was trimmed to fit this page, but the generated text is actually longer.\n7s1: Simple test-time scaling\nBudget forcing In Table 3 we compare the test-time scal-\ning methods we have introduced in 3. Overall, we find that\nbudget forcingprovides perfect control, good scaling, and\nleads to our best AIME24 score. Thus, this is the method\nwe use fors1-32B in Figure 1 and in 4. In Table 4, we\ncomparedifferentstringsforextrapolatingperformance. We\nfind that Wait generally gives the best performance.\nClass-conditional controlWe provide benchmark scores\nfor this method in E.1 and summarize three findings here:\n(1)Token-conditionalcontrolfailswithoutbudgetforcing,as\nour model cannot reliably count tokens - even when trained\nto do so.(2) Under step-conditional control, the model gen-\nerates a similar total number of tokens when given different\nstep targets, as the model goes from few steps with many\ntokens per step, to many steps with few tokens in each step.\nThus, the model learns to hack its way around the com-\npute constraint making the controllability of this method\nmediocre. (3) Class-conditional control can work - telling a\nmodel to simply think longer can increase its test-time com-\npute and performance, which leads good scaling in Table 3.\nTable 4.Budget forcing extrapolation ablations.We compare\nignoring the end-of-thinking delimiter twice and appending none\nor various strings.\nModel AIME\n2024\nMATH\n500\nGPQA\nDiamond\nNo extrapolation 50.0 93.0 57.6\n2x without string 50.0 90.2 55.1\n2x Alternatively 50.0 92.2 59.6\n2x Hmm 50.0 93.0 59.6\n2x Wait 53.3 93.0 59.6\nRejectionsampling Surprisingly,wefindthatsimplysam-\npling until the generation fits a specific length leads to an\ninverse scaling trend as depicted in Figure 6. In E.2 we in-\n3072 4096 5120\nAverage thinking time (tokens)\n22\n27\n32\n37\n42Accuracy (%)\nFigure 6.Rejection sampling on AIME24 with s1-32B.We sam-\nplewithatemperatureof1untilallgenerationshavelessthan(from\nleft to right) 3500, 4000, 5000,', 'in Figure 6. In E.2 we in-\n3072 4096 5120\nAverage thinking time (tokens)\n22\n27\n32\n37\n42Accuracy (%)\nFigure 6.Rejection sampling on AIME24 with s1-32B.We sam-\nplewithatemperatureof1untilallgenerationshavelessthan(from\nleft to right) 3500, 4000, 5000, 8000, and 16000 thinking tokens\nrequiring an average of 655, 97, 8, 3, 2, and 1 tries per sample.\nspectaquestion,whichwasansweredcorrectlybythemodel\nwhen rejection sampling for 4000, but not for the 8000\ntokensetting. Inthe  4000settingthemodeldirectlyjumps\nto the correct approach, while for the 8000setting it back-\ntracks a lot. We hypothesize that there is a correlation such\nthat shorter generations tend to be the ones where the model\nwas on the right track from the start, whereas longer ones\ntend to be ones where the model made mistakes and thus\nbacktracks or questions itself. This leads to longer samples\noften being wrong when rejection sampling and thus the\ninverse scaling trend.\n6. Discussion and related work\n6.1. Sample-efficient reasoning\nModels There are a number of concurrent efforts to build\nmodelsthatreplicatetheperformanceofo1(OpenAI,2024).\nFor example, DeepSeek-r1 and k1.5 (DeepSeek-AI et al.,\n2025; Team et al., 2025) are built with reinforcement learn-\ning methods, while others rely on SFT using tens of thou-\nsands of distilled examples (Team, 2025; Xu et al., 2025;\nLabs, 2025). We show that SFT on only 1,000 examples\nsuffices to build a competitive reasoning model matching\no1-preview and produces a model that lies on the pareto\nfrontier (Figure 2). Further, we introduce budget forcing\nwhich combined with our reasoning model leads to the first\nreproduction of OpenAIs test-time scaling curves (OpenAI,\n2024). Why does supervised finetuning on just 1,000 sam-\nples lead to such performance gains? We hypothesize that\nthe model is already exposed to large amounts of reason-\ning data during pretraining which spans trillions of tokens.\nThus, the ability to perform reasoning is already present in\nour model. Our sample-efficient finetuning stage just acti-\nvatesitandwescaleitfurtherattesttimewithbudgetforcing.\nThis is similar to the "Superficial Alignment Hypothesis"\npresented in LIMA (Zhou et al., 2023), where the authors\nfind that 1,000 examples can be sufficient to align a model\nto adhere to user preferences.\nBenchmarks and methods To evaluate and push the lim-\nits of these models, increasingly challenging benchmarks\nhave been introduced, such as Olympiad-level science com-\npetitions (He et al., 2024; Jain et al., 2024; Zhong et al.,\n2023)andothers(Srivastavaetal.,2023;Glazeretal.,2024;\nSu et al., 2024; Kim et al., 2024; Phan et al., 2025). To\nenhancemodelsperformanceonreasoning-relatedtasks,re-\nsearchers have pursued several strategies: Prior works have\nexplored continuing training language models on special-\nized corpora related to mathematics and science (Azerbayev\netal.,2023;Yangetal.,2024),sometimesevensynthetically\ngenerated data (Yu et al., 2024). Others have developed\ntraining methodologies specifically aimed at reasoning per-\nformance (Zelikman et al., 2022; 2024; Luo et al., 2025;\n8s1: Simple test-time scaling\nYuanetal.,2025;Wuetal.,2024a). Anothersignificantline\nof work focuses on prompting-based methods to elicit and\nimprove reasoning abilities, including methods like Chain-\nof-Thought prompting (Wei et al., 2023; Yao et al., 2023a;b;\nBi et al., 2023; Fu et al., 2023; Zhang et al., 2024b; Xi-\nang et al., 2025; Hu et al., 2024; Diao et al., 2024). These\ncombined efforts aim to advance the reasoning ability of\nlanguage models, enabling them to handle more complex\nand abstract tasks effectively.\n6.2. Test-time scaling\nMethods Asweintroducein3,wedifferentiatetwometh-\nodstoscaletest-timecompute: parallelandsequential. The\nformer relies on multiple solution attempts generated in par-\nallel and selecting the best outcome via specific criteria.\nThese criteria include choosing the most frequent response\nfor majority voting or the best response based on an exter-\nnal reward for Best-of-N (Brown et al., 2024; Irvine et al.,\n2023; Levi, 2024). Unlike repeated sampling, previous se-\nquential scaling methods let the model generate solution\nattempts sequentially based on previous attempts, allowing\nit to refine each attempt based on previous outcomes (Snell\net al., 2024; Hou et al., 2025; Lee et al., 2025). Tree-based\nsearch methods (Gandhi et al., 2024; Wu et al., 2024b) offer\na hybrid approach between sequential and parallel scaling,\nsuch as Monte-Carlo Tree Search (MCTS) (Liu et al., 2024;\nZhang et al., 2023; Zhou et al., 2024; Choi et al., 2023) and\nguided beam search (Xie et al., 2023).REBASE (Wu et al.,\n2024b) employs a process reward model to balance exploita-\ntion and pruning during tree search. Empirically,REBASE\nhas been shown to outperform sampling-based methods and\nMCTS (Wu et al., 2024b). Reward models (Lightman et al.,\n2023; Wang et al., 2024b;c) play a key role in these meth-\nods. They come in two variants: outcome reward models\nand p', 'Empirically,REBASE\nhas been shown to outperform sampling-based methods and\nMCTS (Wu et al., 2024b). Reward models (Lightman et al.,\n2023; Wang et al., 2024b;c) play a key role in these meth-\nods. They come in two variants: outcome reward models\nand process reward models. Outcome reward models (Xin\net al., 2024; Ankner et al., 2024) assign a score to complete\nsolutions and are particularly useful in Best-of-N selection,\nwhile process reward models (Lightman et al., 2023; Wang\net al., 2024b; Wu et al., 2024b) assess individual reasoning\nsteps and are effective in guiding tree-based search methods.\nLimits to further test-time scalingWe have shown that\nbudget forcing allows extrapolating test-time compute in\n4, e.g., improving AIME24 performance from 50% to 57%.\nHowever, it has two key limitations when scaling further: it\neventuallyflattens out(Figure 4), and thecontext window\nof the underlying language model constrains it. Despite\nthese constraints, our work shows test-time scaling across a\nwide range of accuracies (Figure 1), partly because scaling\ndown test-time compute behaves predictably and does not\nsuffer from these constraints.\nContinuing test-time scaling will require approaches that\ncan further extrapolate test-time compute. How can we get\n2048 8192 32768 131072\nAverage thinking time (tokens)\n30\n40\n50\n60Accuracy (%)\nREBASE\nMajority\nSequential\nscaling\nFigure 7.Scalingfurtherwithparallelscalingmethods. Allmet-\nrics averaged over the 30 questions in AIME24. Average thinking\ntokensfor REBASE donotaccountfortheadditionalcomputefrom\nthe reward model. For sequential scaling, we prompt the model\nto use up to (from left to right) 32, 64, 256, and 512 steps. For\nREBASE and majority voting we generate 16 parallel trajectories\nto aggregate across.\nsuch extrapolation? There may be improvements to budget\nforcing such as rotating through different strings, not only\nWait, or combining it with frequency penalties or higher\ntemperature to avoid repetitive loops. An exciting direc-\ntion for future work is also researching whether applying\nbudget forcing to a reasoning model trained with reinforce-\nmentlearningyieldsbetterextrapolation; orifRLallowsfor\nnew ways of test-time scaling beyond budget forcing. Our\nwork definestheright metrics (3.2) Control, Scaling, and\nPerformance  to enable future research and progress on\nextrapolating test-time compute.\nParallel scaling as a solutionParallel scaling offers one\nsolution to the limits of sequential scaling, thus we aug-\nment our sequentially scaled model with two methods:(I)\nMajority voting: After generating solutions, the final\nsolution is the most frequent one across generations;(II)\nTree search viaREBASE:We use theREBASE process\nreward model, which is initialized from LLaMA-34B and\nfurther finetuned on a synthetic process reward modeling\ndataset (Wu et al., 2024b). We then aggregate the solutions\ngenerated byREBASE via majority voting. As shown in\nFigure7,augmentingourmodelwith REBASE scalesbetter\nthan majority voting, and even sequential scaling in this sce-\nnario. However,REBASE requires an additional forward\npassateachstepfortherewardmodeladdingsomecomputa-\ntionoverhead. Forsequentialscaling,whenpromptedtouse\nup to 512 steps, for 12 out of the 30 evaluation questions the\nmodelgeneratesaresponsethatexceedsthecontextwindow\nleading to a large performance drop. Overall, we find that\nthese parallel scaling methods complement sequential scal-\ning thus they offer an avenue for scaling test-time compute\neven further; beyond fixed context windows.\n9s1: Simple test-time scaling\nImpact Statement\nLanguage models with strong reasoning capabilities have\nthe potential to greatly enhance human productivity, from\nassisting in complex decision-making to driving scientific\nbreakthroughs. However,recentadvancesinreasoning,such\nas OpenAIs o1 and DeepSeeks r1, lack transparency, limit-\ning broader research progress. Our work aims to push the\nfrontier of reasoning in a fully open manner, fostering in-\nnovation and collaboration to accelerate advancements that\nultimately benefit society.\nAcknowledgements\nWethankRyanMartenforgeneratingtracesfromDeepSeek\nr1 for s1.1 using Bespoke Curator (Marten et al., 2025).\nThisworkwaspartlyconductedusingtheStanfordMarlowe\nGPUcluster(Kapferetal.,2025),madepossiblebyfinancial\nsupport from Stanford University. We thank Alexander M.\nRush, Andrew Ilyas, Banghua Zhu, Chenglei Si, Chunting\nZhou, John Yang, Ludwig Schmidt, Samy Jelassi, Suhas\nKotha,TengyuMa,XuechenLi,YuSun,andYueZhangfor\nvery constructive discussions.\nReferences\nAnkner, Z., Paul, M., Cui, B., Chang, J. D., and Am-\nmanabrolu, P. Critique-out-loud reward models, 2024.\nURL https://arxiv.org/abs/2408.11791.\nArora, D., Singh, H. G., and Mausam. Have llms advanced\nenough? a challenging problem solving benchmark for\nlarge language models, 2023. URLhttps://arxiv.\norg/abs/2305.15074.\nAzerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D.,\nMcAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and\nWell', 'llms advanced\nenough? a challenging problem solving benchmark for\nlarge language models, 2023. URLhttps://arxiv.\norg/abs/2305.15074.\nAzerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D.,\nMcAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and\nWelleck, S. Llemma: An open language model for math-\nematics, 2023.\nBi, Z., Zhang, N., Jiang, Y., Deng, S., Zheng, G., and\nChen, H. When do program-of-thoughts work for reason-\ning?,2023. URLhttps://arxiv.org/abs/2308.\n15452.\nBiderman, S., Schoelkopf, H., Sutawika, L., Gao, L., Tow,\nJ., Abbasi, B., Aji, A. F., Ammanamanchi, P. S., Black,\nS., Clive, J., DiPofi, A., Etxaniz, J., Fattori, B., Forde,\nJ. Z., Foster, C., Hsu, J., Jaiswal, M., Lee, W. Y., Li, H.,\nLovering, C., Muennighoff, N., Pavlick, E., Phang, J.,\nSkowron, A., Tan, S., Tang, X., Wang, K. A., Winata,\nG. I., Yvon, F., and Zou, A. Lessons from the trenches on\nreproducible evaluation of language models, 2024.\nBrown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., R,\nC., and Mirhoseini, A. Large language monkeys: Scaling\ninference compute with repeated sampling, 2024. URL\nhttps://arxiv.org/abs/2407.21787.\nCesista, F.L. Multimodalstructuredgeneration: Cvprs2nd\nmmfm challenge technical report, 2024. URLhttps:\n//arxiv.org/abs/2406.11403.\nChen, W., Yin, M., Ku, M., Lu, P., Wan, Y., Ma, X.,\nXu, J., Wang, X., and Xia, T. Theoremqa: A theorem-\ndriven question answering dataset, 2023. URLhttps:\n//arxiv.org/abs/2305.12524.\nChoi,S.,Fang,T.,Wang,Z.,andSong,Y. Kcts: Knowledge-\nconstrained tree search decoding with token-level halluci-\nnation detection, 2023. URLhttps://arxiv.org/\nabs/2310.09044.\nDeepSeek-AI,Guo,D.,Yang,D.,Zhang,H.,Song,J.,Zhang,\nR., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X.,\nYu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao,\nZ., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu,\nC., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D.,\nChen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao,\nG., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang,\nH., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J.,\nLi, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai,\nJ. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao,\nK., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L.,\nZhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang,\nM., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian,\nN., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q.,\nGe, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin,\nR. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang,\nS., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu,\nS., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W.,\nZhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang,\nW., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X.,\nNie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X.,\nLi, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen,\nX., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X.,\nShan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y.,\nXu,Y., Li,Y., Zhao,Y., Sun,Y., Wang, Y.,Yu, Y.,Zhang,\nY., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan,\nY., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong,\nY., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y.,\nZhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng,\nY., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z.,\nRen, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao,\nZ., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li,\nZ., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang,\nZ., and Zhang, Z. Deepseek-r1: Incentivizing reasoning\ncapability in llms via reinforcement learning, 2025. URL\nhttps://arxiv.org/abs/2501.12948.\nDiao, S., Wang, P., Lin, Y., Pan, R., Liu, X., and Zhang,\nT. Active prompting with chain-of-thought for large lan-\n10s1: Simple test-time scaling\nguage models, 2024. URL https://arxiv.org/\nabs/2302.12246.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A.,\nLetman, A., Mathur, A., Schelten, A., Yang, A., Fan, A.,\nGoyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravanku-\nmar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A.,\nRodriguez, A., Gregerson, A., et al. The llama 3 herd\nof models, 2024. URLhttps://arxiv.org/abs/\n2407.21783.\nFu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot,\nT. Complexity-based prompting for multi-step reason-\ning, 2023. URLhttps://arxiv.org/abs/2210.\n00720.\nGandhi,K.,Lee,D.,Grand,G.,Liu,M.,Cheng,W.,Sharma,\nA.,andGoodman,N.D. Streamofsearch(sos): Learning\nto search in language, 2024. URLhttps://arxiv.\norg/abs/2404.03683.\nGao, B., Song, F., Yang, Z., Cai, Z., Miao, Y., Dong, Q., Li,\nL., Ma, C., Chen, L., Xu, R., Tang, Z., Wang, B., Zan, D.,\nQuan, S., Zhang, G., Sha, L., Zhang, Y., Ren, X., Liu, T.,\nand Chang, B. Omni-math: A universal olympiad level\nmathematic benchmark for large language models, 2024a.\nURL https://arxiv.org/abs/2410.07985.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,\nC., Golding, L., Hsu, J., McDonell, K., Muennighoff, N.,\nPhang, J., Reynolds, L., Tang, E., Thite, A., Wang, B.,\n', 'k for large language models, 2024a.\nURL https://arxiv.org/abs/2410.07985.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,\nC., Golding, L., Hsu, J., McDonell, K., Muennighoff, N.,\nPhang, J., Reynolds, L., Tang, E., Thite, A., Wang, B.,\nWang,K.,andZou,A.Aframeworkforfew-shotlanguage\nmodel evaluation, September 2021. URLhttps://\ndoi.org/10.5281/zenodo.5371628.\nGao,Z.,Niu,B.,He,X.,Xu,H.,Liu,H.,Liu,A.,Hu,X.,and\nWen, L. Interpretable contrastive monte carlo tree search\nreasoning,2024b. URLhttps://arxiv.org/abs/\n2410.01707.\nGlazer, E., Erdil, E., Besiroglu, T., Chicharro, D., Chen,\nE., Gunning, A., Olsson, C. F., Denain, J.-S., Ho, A.,\nde Oliveira Santos, E., Jrviniemi, O., Barnett, M., San-\ndler, R., Vrzala, M., Sevilla, J., Ren, Q., Pratt, E., Levine,\nL., Barkley, G., Stewart, N., Grechuk, B., Grechuk, T.,\nEnugandla,S.V.,andWildon,M. Frontiermath: Abench-\nmark for evaluating advanced mathematical reasoning in\nai, 2024. URL https://arxiv.org/abs/2411.\n04872.\nGoogle. Gemini 2.0 flash thinking mode (gemini-2.0-\nflash-thinking-exp-1219), December 2024. URL\nhttps://cloud.google.com/vertex-ai/\ngenerative-ai/docs/thinking-mode.\nGroeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney,\nR., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I.,\nWang, Y., Arora, S., Atkinson, D., Authur, R., Chandu,\nK.R., Cohan,A., Dumas,J., Elazar, Y., Gu,Y., Hessel,J.,\nKhot,T.,Merrill,W.,Morrison,J.,Muennighoff,N.,Naik,\nA., Nam, C., Peters, M. E., Pyatkin, V., Ravichander, A.,\nSchwenk,D.,Shah,S.,Smith,W.,Strubell,E.,Subramani,\nN., Wortsman, M., Dasigi, P., Lambert, N., Richardson,\nK.,Zettlemoyer,L.,Dodge,J.,Lo,K.,Soldaini,L.,Smith,\nN. A., and Hajishirzi, H. Olmo: Accelerating the science\nof language models, 2024.\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J.,\nHu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L.,\nLiu, Z., and Sun, M. Olympiadbench: A challeng-\ning benchmark for promoting agi with olympiad-level\nbilingual multimodal scientific problems, 2024. URL\nhttps://arxiv.org/abs/2402.14008.\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS.,Tang,E.,Song,D.,andSteinhardt,J. Measuringmath-\nematical problem solving with the math dataset, 2021.\nURL https://arxiv.org/abs/2103.03874.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai,T.,Rutherford,E.,deLasCasas,D.,Hendricks,L.A.,\nWelbl, J., Clark, A., Hennigan, T., Noland, E., Millican,\nK., van den Driessche, G., Damoc, B., Guy, A., Osindero,\nS., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and\nSifre, L. Training compute-optimal large language mod-\nels, 2022. URLhttps://arxiv.org/abs/2203.\n15556.\nHou, Z., Lv, X., Lu, R., Zhang, J., Li, Y., Yao, Z., Li, J.,\nTang, J., and Dong, Y. Advancing language model rea-\nsoning through reinforcement learning and inference scal-\ning, 2025. URLhttps://arxiv.org/abs/2501.\n11651.\nHu, Y., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettle-\nmoyer, L., Smith, N. A., and Krishna, R. Visual sketch-\npad: Sketchingasavisualchainofthoughtformultimodal\nlanguage models, 2024. URLhttps://arxiv.org/\nabs/2406.09403.\nHuang, Z., Wang, Z., Xia, S., Li, X., Zou, H., Xu, R., Fan,\nR.-Z., Ye, L., Chern, E., Ye, Y., Zhang, Y., Yang, Y., Wu,\nT.,Wang,B.,Sun,S.,Xiao,Y.,Li,Y.,Zhou,F.,Chern,S.,\nQin, Y., Ma, Y., Su, J., Liu, Y., Zheng, Y., Zhang, S., Lin,\nD., Qiao, Y., and Liu, P. Olympicarena: Benchmarking\nmulti-discipline cognitive reasoning for superintelligent\nai, 2024a. URLhttps://arxiv.org/abs/2406.\n12753.\nHuang, Z., Zou, H., Li, X., Liu, Y., Zheng, Y., Chern, E.,\nXia, S., Qin, Y., Yuan, W., and Liu, P. O1 replication\njourney  part 2: Surpassing o1-preview through simple\ndistillation, big progress or bitter lesson?, 2024b. URL\nhttps://arxiv.org/abs/2411.16489.\n11s1: Simple test-time scaling\nHuang, Z., Geng, G., Hua, S., Huang, Z., Zou, H., Zhang,\nS., Liu, P., and Zhang, X. O1 replication journey  part 3:\nInference-time scaling for medical reasoning, 2025. URL\nhttps://arxiv.org/abs/2501.06458.\nIrvine, R., Boubert, D., Raina, V., Liusie, A., Zhu, Z.,\nMudupalli, V., Korshuk, A., Liu, Z., Cremer, F., As-\nsassi, V., Beauchamp, C.-C., Lu, X., Rialan, T., and\nBeauchamp, W. Rewarding chatbots for real-world en-\ngagement with millions of users, 2023. URLhttps:\n//arxiv.org/abs/2303.06135.\nJain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang,\nT., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I.\nLivecodebench: Holistic and contamination free evalu-\nation of large language models for code, 2024. URL\nhttps://arxiv.org/abs/2403.07974.\nKapfer, C., Stine, K., Narasimhan, B., Mentzel, C., and\nCandes,E. Marlowe: Stanfordsgpu-basedcomputational\ninstrument, January 2025. URLhttps://doi.org/\n10.5281/zenodo.14751899.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J.,\nand Amodei, D. Scaling laws for neural language mod-\nels, 2020. URLhttps://arxiv.org/abs/2001.\n08361.\nKim,E.,Suk,J.,Kim,S.,Muennighoff,N.,Kim,D.,andOh,\nA. Llm-as-an-interviewer: Beyond static testing through\ndynamic llm ev', 'y, S., Radford, A., Wu, J.,\nand Amodei, D. Scaling laws for neural language mod-\nels, 2020. URLhttps://arxiv.org/abs/2001.\n08361.\nKim,E.,Suk,J.,Kim,S.,Muennighoff,N.,Kim,D.,andOh,\nA. Llm-as-an-interviewer: Beyond static testing through\ndynamic llm evaluation, 2024. URLhttps://arxiv.\norg/abs/2412.10424.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Effi-\ncientmemorymanagementforlargelanguagemodelserv-\ning with pagedattention, 2023. URLhttps://arxiv.\norg/abs/2309.06180.\nLabs, B. Bespoke-stratos: The unreasonable effectiveness\nof reasoning distillation, 2025. URL https://hf.\nco/bespokelabs/Bespoke-Stratos-32B. Ac-\ncessed: 2025-01-22.\nLee, K.-H., Fischer, I., Wu, Y.-H., Marwood, D., Baluja,\nS., Schuurmans, D., and Chen, X. Evolving deeper llm\nthinking, 2025. URL https://arxiv.org/abs/\n2501.09891.\nLevi, N. A simple model of inference scaling laws, 2024.\nURL https://arxiv.org/abs/2410.16377.\nLI, J., Beeching, E., Tunstall, L., Lipkin, B., Solet-\nskyi, R., Huang, S. C., Rasul, K., Yu, L., Jiang, A.,\nShen, Z., Qin, Z., Dong, B., Zhou, L., Fleureau, Y.,\nLample, G., and Polu, S. Numinamath, 2024. URL\nhttps://github.com/project-numina/\naimo-progress-prize/blob/main/report/\nnumina_dataset.pdf.\nLightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker,\nB., Lee, T., Leike, J., Schulman, J., Sutskever, I., and\nCobbe, K. Lets verify step by step, 2023. URLhttps:\n//arxiv.org/abs/2305.20050.\nLing,W.,Yogatama,D.,Dyer,C.,andBlunsom,P. Program\ninduction by rationale generation : Learning to solve and\nexplain algebraic word problems, 2017. URLhttps:\n//arxiv.org/abs/1705.04146.\nLiu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang,\nY. Logiqa: A challenge dataset for machine reading com-\nprehension with logical reasoning, 2020. URLhttps:\n//arxiv.org/abs/2007.08124.\nLiu, J., Cohen, A., Pasunuru, R., Choi, Y., Hajishirzi,\nH., and Celikyilmaz, A. Dont throw away your value\nmodel! generatingmorepreferabletextwithvalue-guided\nmonte-carlo tree search decoding, 2024. URLhttps:\n//arxiv.org/abs/2309.15028.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization, 2019.\nLuo,H.,Sun,Q.,Xu,C.,Zhao,P.,Lou,J.,Tao,C.,Geng,X.,\nLin, Q., Chen, S., Tang, Y., and Zhang, D. Wizardmath:\nEmpowering mathematical reasoning for large language\nmodels via reinforced evol-instruct, 2025. URLhttps:\n//arxiv.org/abs/2308.09583.\nMarten, R., Vu, T., Ji, C. C.-J., Sharma, K., Pimpalgaonkar,\nS., Dimakis, A., and Sathiamoorthy, M. Curator: A tool\nfor synthetic data creation.https://github.com/\nbespokelabsai/curator, January 2025.\nMuennighoff, N., Soldaini, L., Groeneveld, D., Lo, K., Mor-\nrison,J.,Min,S.,Shi,W.,Walsh,P.,Tafjord,O.,Lambert,\nN., Gu, Y., Arora, S., Bhagia, A., Schwenk, D., Wadden,\nD., Wettig, A., Hui, B., Dettmers, T., Kiela, D., Farhadi,\nA., Smith, N. A., Koh, P. W., Singh, A., and Hajishirzi, H.\nOlmoe: Open mixture-of-experts language models, 2024.\nURL https://arxiv.org/abs/2409.02060.\nof America, M. A. Aime, February 2024. URLhttps:\n//artofproblemsolving.com/wiki/index.\nphp/AIME_Problems_and_Solutions/.\nOpenAI. Learning to reason with llms, Septem-\nber 2024. URL https://openai.com/index/\nlearning-to-reason-with-llms/ .\nOpenAI. Openaio3-mini,2025. URLhttps://openai.\ncom/index/openai-o3-mini/. Accessed: 2025-\n02-24.\n12s1: Simple test-time scaling\nPhan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Shi,\nS., Choi, M., Agrawal, A., Chopra, A., et al. Humanitys\nlast exam, 2025. URLhttps://arxiv.org/abs/\n2501.14249.\nQin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye,\nY., Yuan, W., Liu, H., Li, Y., and Liu, P. O1 replication\njourney: A strategic progress report  part 1, 2024. URL\nhttps://arxiv.org/abs/2410.18982.\nQwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng,\nB., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H.,\nYang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J.,\nLin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li,\nM., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li,\nT., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y.,\nWan,Y.,Liu,Y.,Cui,Z.,Zhang,Z.,andQiu,Z. Qwen2.5\ntechnical report, 2024. URLhttps://arxiv.org/\nabs/2412.15115.\nRein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y.,\nDirani, J., Michael, J., and Bowman, S. R. Gpqa: A\ngraduate-level google-proof q&a benchmark, 2023. URL\nhttps://arxiv.org/abs/2311.12022.\nShi, Q., Tang, M., Narasimhan, K., and Yao, S. Can lan-\nguagemodels solveolympiad programming?, 2024. URL\nhttps://arxiv.org/abs/2404.10952.\nSnell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-\ntime compute optimally can be more effective than scal-\ning model parameters, 2024. URLhttps://arxiv.\norg/abs/2408.03314.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., et al. Beyond the imitation game:\nQuantifyingandextrapolatingthecapabilitiesoflanguage\nmodels, 2023.\nSu,H.,Yen,H.,Xia,M.,Shi,W.,Muennighoff,N.,yuWang,\nH.,Liu,H.,Shi,Q', ' M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., et al. Beyond the imitation game:\nQuantifyingandextrapolatingthecapabilitiesoflanguage\nmodels, 2023.\nSu,H.,Yen,H.,Xia,M.,Shi,W.,Muennighoff,N.,yuWang,\nH.,Liu,H.,Shi,Q.,Siegel,Z.S.,Tang,M.,Sun,R.,Yoon,\nJ., Arik, S. O., Chen, D., and Yu, T. Bright: A realis-\ntic and challenging benchmark for reasoning-intensive\nretrieval, 2024. URL https://arxiv.org/abs/\n2407.12883.\nSun,L.,Han,Y.,Zhao,Z.,Ma,D.,Shen,Z.,Chen,B.,Chen,\nL., and Yu, K. Scieval: A multi-level large language\nmodel evaluation benchmark for scientific research, 2024.\nURL https://arxiv.org/abs/2308.13149.\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li,\nC., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang,\nD., Yuan, E., Lu, E., Tang, F., Sung, F., Wei, G., Lai, G.,\nGuo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H.,\nYao, H., Zhao, H., Lu, H., Li, H., Yu, H., Gao, H., Zheng,\nH., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J.,\nZhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S.,\nMa, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W.,\nXiong, W., He, W., Huang, W., Wu, W., He, W., Wei, X.,\nJia,X.,Wu,X.,Xu,X.,Zu,X.,Zhou,X.,Pan,X.,Charles,\nY.,Li,Y.,Hu,Y.,Liu,Y.,Chen,Y.,Wang,Y.,Liu,Y.,Qin,\nY., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y.,\nZhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang, Z., Wang, Z.,\nYang,Z.,Huang,Z.,Huang,Z.,Xu,Z.,andYang,Z. Kimi\nk1.5: Scaling reinforcement learning with llms, 2025.\nURL https://arxiv.org/abs/2501.12599.\nTeam, N. Sky-t1: Fully open-source reasoning\nmodel with o1-preview performance in $450 budget,\n2025. URL https://novasky-ai.github.io/\nposts/sky-t1. Accessed: 2025-01-09.\nTeam, Q. Qwq: Reflect deeply on the boundaries of the\nunknown, November 2024. URLhttps://qwenlm.\ngithub.io/blog/qwq-32b-preview/.\nWang, J., Meng, F., Liang, Y., and Zhou, J. Drt-o1: Op-\ntimized deep reasoning translation via long chain-of-\nthought, 2024a. URLhttps://arxiv.org/abs/\n2412.17498.\nWang, P., Li, L., Shao, Z., Xu, R. X., Dai, D., Li, Y., Chen,\nD., Wu, Y., and Sui, Z. Math-shepherd: Verify and rein-\nforcellmsstep-by-stepwithouthumanannotations,2024b.\nURL https://arxiv.org/abs/2312.08935.\nWang, S., Liu, Z., Zhong, W., Zhou, M., Wei, Z., Chen, Z.,\nand Duan, N. From lsat: The progress and challenges\nof complex reasoning, 2021. URLhttps://arxiv.\norg/abs/2108.00648.\nWang, Z., Dong, Y., Delalleau, O., Zeng, J., Shen, G., Egert,\nD., Zhang, J. J., Sreedhar, M. N., and Kuchaiev, O. Help-\nsteer2: Open-source dataset for training top-performing\nreward models, 2024c. URLhttps://arxiv.org/\nabs/2406.08673.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter,\nB., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-\nthought prompting elicits reasoning in large language\nmodels, 2023. URL https://arxiv.org/abs/\n2201.11903.\nWelleck,S.,Bertsch,A.,Finlayson,M.,Schoelkopf,H.,Xie,\nA., Neubig, G., Kulikov, I., and Harchaoui, Z. From de-\ncoding to meta-generation: Inference-time algorithms for\nlarge language models, 2024. URLhttps://arxiv.\norg/abs/2406.16838.\n13s1: Simple test-time scaling\nWu, T., Lan, J., Yuan, W., Jiao, J., Weston, J., and\nSukhbaatar, S. Thinking llms: General instruction fol-\nlowing with thought generation, 2024a. URLhttps:\n//arxiv.org/abs/2410.10630.\nWu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y. Infer-\nence scaling laws: An empirical analysis of compute-\noptimalinferenceforproblem-solvingwithlanguagemod-\nels,2024b. URLhttps://arxiv.org/abs/2408.\n00724.\nXiang, V., Snell, C., Gandhi, K., Albalak, A., Singh, A.,\nBlagden, C., Phung, D., Rafailov, R., Lile, N., Mahan,\nD., Castricato, L., Franken, J.-P., Haber, N., and Finn, C.\nTowards system 2 reasoning in llms: Learning how to\nthink with meta chain-of-thought, 2025. URLhttps:\n//arxiv.org/abs/2501.04682.\nXie, Y., Kawaguchi, K., Zhao, Y., Zhao, X., Kan, M.-Y., He,\nJ., and Xie, Q. Self-evaluation guided beam search for\nreasoning, 2023. URLhttps://arxiv.org/abs/\n2305.00633.\nXin, H., Guo, D., Shao, Z., Ren, Z., Zhu, Q., Liu, B., Ruan,\nC., Li, W., and Liang, X. Deepseek-prover: Advancing\ntheorem proving in llms through large-scale synthetic\ndata,2024. URLhttps://arxiv.org/abs/2405.\n14333.\nXu, H., Wu, X., Wang, W., Li, Z., Zheng, D., Chen, B.,\nHu, Y., Kang, S., Ji, J., Zhang, Y., Guo, Z., Yang, Y.,\nZhang,M.,andZhang,D. Redstar: Doesscalinglong-cot\ndata unlock better slow-reasoning systems?, 2025. URL\nhttps://arxiv.org/abs/2501.11284.\nYang, Z., Band, N., Li, S., Cands, E., and Hashimoto, T.\nSynthetic continued pretraining, 2024. URLhttps:\n//arxiv.org/abs/2409.07431.\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao,\nY., and Narasimhan, K. Tree of thoughts: Deliberate\nproblemsolvingwithlargelanguagemodels,2023a. URL\nhttps://arxiv.org/abs/2305.10601.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK., and Cao, Y. React: Synergizing reasoning and acting\nin language models, 2023b. URLhttps://arxiv.\norg/abs/2210', 'msolvingwithlargelanguagemodels,2023a. URL\nhttps://arxiv.org/abs/2305.10601.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK., and Cao, Y. React: Synergizing reasoning and acting\nin language models, 2023b. URLhttps://arxiv.\norg/abs/2210.03629.\nYe, Y., Huang, Z., Xiao, Y., Chern, E., Xia, S., and Liu, P.\nLimo: Less is more for reasoning, 2025a. URLhttps:\n//arxiv.org/abs/2502.03387.\nYe, Y., Xiao, Y., Mi, T., and Liu, P. Aime-preview: A\nrigorous and immediate evaluation framework for ad-\nvanced mathematical reasoning. https://github.\ncom/GAIR-NLP/AIME-Preview, 2025b. GitHub\nrepository.\nYu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok,\nJ.T.,Li,Z.,Weller,A.,andLiu,W. Metamath: Bootstrap\nyour own mathematical questions for large language mod-\nels, 2024. URLhttps://arxiv.org/abs/2309.\n12284.\nYuan, S., Chen, Z., Xi, Z., Ye, J., Du, Z., and Chen, J.\nAgent-r: Training language model agents to reflect via\niterative self-training, 2025. URLhttps://arxiv.\norg/abs/2501.11425.\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. D. Star:\nBootstrapping reasoning with reasoning, 2022. URL\nhttps://arxiv.org/abs/2203.14465.\nZelikman, E., Harik, G., Shao, Y., Jayasiri, V., Haber, N.,\nand Goodman, N. D. Quiet-star: Language models can\nteach themselves to think before speaking, 2024. URL\nhttps://arxiv.org/abs/2403.09629.\nZhang, H. and Chen, C. Test-time compute scaling laws,\n2024. URLhttps://github.com/hughbzhang/\no1_inference_scaling_laws.\nZhang, S., Chen, Z., Shen, Y., Ding, M., Tenenbaum, J. B.,\nand Gan, C. Planning with large language models for\ncode generation, 2023. URLhttps://arxiv.org/\nabs/2303.05510.\nZhang, Y., Wu, S., Yang, Y., Shu, J., Xiao, J., Kong, C., and\nSang, J. o1-coder: an o1 replication for coding, 2024a.\nURL https://arxiv.org/abs/2412.00154.\nZhang, Y., Yang, J., Yuan, Y., and Yao, A. C.-C. Cumula-\ntive reasoning with large language models, 2024b. URL\nhttps://arxiv.org/abs/2308.04371.\nZhong, H., Xiao, C., Tu, C., Zhang, T., Liu, Z., and Sun, M.\nJec-qa: A legal-domain question answering dataset, 2019.\nURL https://arxiv.org/abs/1911.12011.\nZhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang,\nY., Saied, A., Chen, W., and Duan, N. Agieval: A\nhuman-centric benchmark for evaluating foundation mod-\nels, 2023. URLhttps://arxiv.org/abs/2304.\n06364.\nZhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H.,\nand Wang, Y.-X. Language agent tree search unifies rea-\nsoning acting and planning in language models, 2024.\nURL https://arxiv.org/abs/2310.04406.\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M.,\nZettlemoyer, L., and Levy, O. Lima: Less is more for\nalignment, 2023. URLhttps://arxiv.org/abs/\n2305.11206.\n14s1: Simple test-time scaling\nContents\n1 Introduction 1\n2 Reasoning data curation to create s1K 2\n2.1 Initial collection of 59K samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n2.2 Final selection of 1K samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n3 Test-time scaling 3\n3.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n3.2 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n4 Results 4\n4.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n4.2 Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n5 Ablations 6\n5.1 Data Quantity, Diversity, and Difficulty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n5.2 Test-time scaling methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n6 Discussion and related work 8\n6.1 Sample-efficient reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n6.2 Test-time scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\nA s1.1 16\nB Evaluation determinism 16\nC s1K details 17\nC.1 s1K summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nC.2 Dataset composition for full 59K questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nC.3 s1K grading prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC.4 s1K diversity selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC.5 Decontamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nD Training details 21\nD.1 Training Ablations: Sequence length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nD.2 Training Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nE Test-time sca', 'D.1 Training Ablations: Sequence length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nD.2 Training Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nE Test-time scaling details 35\nE.1 Sequential scaling ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\nE.2 Examples for rejection sampling ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nF Version Control 46\n15s1: Simple test-time scaling\nA. s1.1\nSeven days after our release of s1, we released s1.1. We regenerated traces for our 1,000 samples ins1Kusing DeepSeek\nr1 (DeepSeek-AI et al., 2025) to creates1K-1.1. We use the same training procedure to train our models1.1. Other updates\nsince our launch include the release of o3 (OpenAI, 2025), LIMO (Ye et al., 2025a), and AIME 2025. We consider all these\nnew developments in Table 5. We find that s1.1 performs significantly better than s1. We also tried distilling from Claude\n3.7, which led to worse performance than from r1 (not reported).\nTable 5.s1-32B is an open and sample-efficient reasoning model.We evaluates1-32B, Qwen, and Gemini (some entries are unknown\n(N.A.), see 4). Other results are from the respective reports (Qwen et al., 2024; Team, 2024; OpenAI, 2024; 2025; DeepSeek-AI et al.,\n2025; Labs, 2025; Team, 2025) except for AIME 2025 (Ye et al., 2025b). # ex. = number examples used for reasoning finetuning; BF =\nbudget forcing.\nModel # Examples MATH500 GPQA AIME 2024 AIME 2025\nAPI only\no3-mini-low N/A 95.8 70.6 56.3 42.1\no3-mini-medium N/A 97.3 76.8 75.8 70.4\no3-mini-high N/A 97.9 79.7 83.8 80.9\nOpen Weights\nQwQ-32B N.A. 90.6 54.5 46.7 32.7\nr1 800K 07.3 71.5 79.8 70.0\nr1-distill-Llama-70B 800K 94.5 65.2 57.1 56.3\nr1-distill-Qwen-14B 800K 93.9 59.1 61.7 48.0\nr1-distill-Qwen-32B 800K 94.3 62.1 58.3 49.6\nOpen Weights and Open Data\nLIMO 817 94.8 66.7 56.3 44.6\ns1 w/o BF 1K 92.6 56.6 50.0 26.7\ns1 with Budget Forcing Wait 1x 1K 92.8 59.6 53.3 30.0\ns1 with Budget Forcing Wait 2x 1K 93.0 59.6 53.3 33.3\ns1 with Budget Forcing Wait 4x 1K 92.2 58.6 56.7 36.7\ns1.1 w/o BF 1K 94.4 60.6 56.7 50.0\ns1.1 with Budget Forcing Wait 1x 1K 95.4 62.6 56.7 50.0\ns1.1 with Budget Forcing Wait 2x 1K 95.4 63.6 56.7 50.0\nB. Evaluation determinism\nWe run our evaluations using vLLM (Kwon et al., 2023) as it is faster than the alternatives we tried. However, we find that\neven when using the same random seeds and greedy sampling, evaluation scores can change significantly across runs:\n Different batch sizes causing different results seehttps://github.com/vllm-project/vllm/issues/\n5898\n Continuing generations causing different results seehttps://github.com/vllm-project/vllm/issues/\n11783\n Changes in tensor parallelism causing different results\nAs our model generates long reasoning traces prior to its answer, small numeric changes can snowball into large differences.\nWe encounter many generations that are exactly the same for thousands of tokens and then suddenly differ in one token\neventually ending up with an entirely different answer. To partly counter this issue we generally run our final evaluations\nusing full precision unless otherwise indicated.\n16s1: Simple test-time scaling\nC. s1K details\nC.1. s1K summary\nTable 6.Summary of our dataset s1K. Token count measured by the Qwen-2.5 tokenizer. We prompt Claude to produce keywords given\nseveral questions from the domain.\nDomain #questions Total token count Keywords\nGeometry 109 560.2K Area, Triangle, Distance\nNumber theory 98 522.5K Sequences, Divisibility\nCombinatorics 75 384.7K Permutations, Counting\nReal functions 43 234.8K Trigonometry, Calculus\nBiology 41 120.9K Organic reactions\nComplex functions 32 170.2K Complex roots\nQuantum theory 32 127.9K Particles, Wave functions\nField theory 28 150.1K Polynomials, Roots\nCalculus of variations 28 155.5K Optimization, Control\nDifference equations 24 132.5K Recurrence, Recursion\nElectromagnetic theory 23 95.8K Optics, Waves, Diffraction\nGroup theory 22 100.0K Groups, Automorphisms\nLinear algebra 22 128.3K Matrices, Determinants\nProbability theory 20 114.6K Random walk, Expectation\nAlgebraic systems 19 109.9K Functional equations\nMechanics 19 103.6K Forces, Motion, Energy\nThermodynamics 19 74.2K Heat engines, Entropy\nDifferential equations 18 89.6K Substitution, Existence\nComputer science 18 34.2K Complexity theory, Algorithms\nNumerical analysis 18 76.5K Error analysis, Stability\nCalculus 17 96.3K Convergence, Summation\nAlgebraic structures 17 90.4K Inequalities, Sets\nAstronomy 16 37.7K Stellar populations, Orbits\nRemaining 27 domains 242 982.2K Domains with  16 questions\nAll domains (51) 1000 4.7M s1K\nC.2. Dataset composition for full 59K questions\n17s1: Simple test-time scaling\nTable 7.Composition of full 59K questions.Thinking and response lengths are measured in tokens using the Qwen2.5-32B-Instruct\ntokenizer (Qwen et al., 2024). In addition to excluding our eval', 'ition for full 59K questions\n17s1: Simple test-time scaling\nTable 7.Composition of full 59K questions.Thinking and response lengths are measured in tokens using the Qwen2.5-32B-Instruct\ntokenizer (Qwen et al., 2024). In addition to excluding our evaluation benchmark, AIME24, we also exclude AIME questions from\n2022-2023 as we use these 90 questions during our development stage ofs1-32B.\nSource Description #Samples Avg.\nthinking\nlength\nNuminaMATH (LI et al., 2024) Math problems from online websites 30660 4.1K\nMATH (Hendrycks et al., 2021) Math problems from competitions 11999 2.9K\nOlympicArena (Huang et al.,\n2024a)\nAstronomy, Biology, Chemistry, Com-\nputer Science, Geography, Math, and\nPhysics olympiad questions\n4250 3.2K\nOmniMath (Gao et al., 2024a) Math problems from competitions 4238 4.4K\nAGIEval (Zhong et al., 2023;\nLingetal.,2017;Hendrycksetal.,\n2021; Liu et al., 2020; Zhong\net al., 2019; Wang et al., 2021)\nEnglish, Law, Logic and Math prob-\nlems from the SAT, LSAT and other\nexams\n2385 1.2K\nxword Crossword puzzles 999 0.7K\nOlympiadBench (He et al., 2024) Math and Physics olympiad questions 896 3.9K\nAIME (1983-2021) AmericanInvitationalMathematicsEx-\namination\n890 4.7K\nTheoremQA (Chen et al., 2023) Computer Science, Finance, Math, and\nPhysics university-level questions relat-\ning to theorems\n747 2.1K\nUSACO (Shi et al., 2024) Code problems from the USA Comput-\ning Olympiad\n519 3.6K\nJEEBench (Arora et al., 2023) Chemistry,Math,andPhysicsproblems\nused in the university entrance exami-\nnation of the Indian Institute of Tech-\nnology\n515 2.9K\nGPQA (Rein et al., 2023) PhD-Level Science Questions 348 2.9K\nSciEval (Sun et al., 2024) Biology, Chemistry, and Physics prob-\nlems from various sources\n227 0.7K\ns1-prob Stanford statistics qualifying exams 182 4.0K\nLiveCodeBench (Jain et al., 2024)Code problems from coding websites\n(LeetCode, AtCoder, and CodeForces)\n151 3.5K\ns1-teasers Math brain-teasers crawled from the In-\nternet\n23 4.1K\nAll 59K questions Composite of the above datasets with\nreasoning traces and solutions\n59029 3.6K\n18s1: Simple test-time scaling\nC.3. s1K grading prompt\nTogradewhetheranexampleiscorrectforourdatasetselectionin2,weusethepromptinFigure8. WegradeusingClaude\n3.5 except for the correctness among the final 1,000 samples, which we graded with Claude 3.7.\nYou are an AI assistant for grading a science problem. The user will provide you with the question itself, an attempt made by\na student and the correct answer to the problem. Your job is to judge whether the attempt is correct by comparing it with the\ncorrect answer. If the expected solution concludes with a number or choice, there should be no ambiguity. If the expected\nsolution involves going through the entire reasoning process, you should judge the attempt based on whether the reasoning\nprocess is correct with correct answer if helpful.\nThe user will provide the attempt and the correct answer in the following format:\n# Problem\n{problem}\n## Attempt\n{attempt}\n## Correct answer\n{solution}\nExplain your reasoning, and end your response on a new line with only "Yes" or "No" (without quotes).\nFigure 8.Grading prompt.\nC.4. s1K diversity selection\nAlgorithm 1 provides our algorithm for selecting data in our diversity selection stage. As mentioned in 2, we also include\nsamples from some specific benchmarks we perceive as high-quality. None of the samples overlap with our final evaluation.\nC.5. Decontamination\nWe filter all samples by checking for an 8-gram overlap between the selected examples and the evaluation benchmarks:\nMATH500, GPTQA Diamond, and AIME24. We exclude questions with more than an 8-gram overlap.\n19s1: Simple test-time scaling\nAlgorithm 1Two-stage sampling fors1K\n1: Input: \ue23d := Set of 24,496 questions with features\n2: Output: \ue23f := Set of 1,000 selected questions\n3: \ue23f   Initialize the output set (only tracks unique elements)\n4: for\ue23d do\n5: if IsGeminiCorrect()and (IsAIME()or IsGPQA()) then\n6: \ue23f  \ue23f {}\n7: Select all correct AIME/GPQA solutions\n8: else ifIsGeminiCorrect()andIsMATH() andThinkingLength() > 5600then\n9: \ue23f  \ue23f {}\n10: Select correct MATH500 solutions with long chains\n11: end if\n12: end for\n13: \ue230  All available domains\n14: Initialize domain pool\n15: while|\ue23f| <1000do\n16:   RandomChoice(\ue230)\n17: Randomly select a domain\n18:   Questions in domain\n19: Get questions from this domain\n20: ranks  RankByThinkingLength()\n21: Rank by thinking length\n22: weights 2ranks\n23: Apply power-law weighting\n24:  WeightedSample(, weights)\n25: Sample favoring longer chains\n26: \ue23f  \ue23f {}\n27: Add selected question\n28:     {}\n29: if  = then\n30: \ue230  \ue230  {}\n31: Remove exhausted domains\n32: end if\n33: end while\n20s1: Simple test-time scaling\nD. Training details\nWe take a model that has already been pretrained and instruction tuned and further finetune it for reasoning. Specifically, we\nuse Qwen2.5-32B-Instruct (Qwen et al., 2024), which on math tasks generally matches or outperforms the larger Qwen2.5-\n72B-Instru'], dimensions=1536, num_retries=3, timeout=120)[0m
2025-12-08 14:32:37,033 - DEBUG - 

2025-12-08 14:32:37,033 - DEBUG - self.optional_params: {}
2025-12-08 14:32:37,033 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:32:37,033 - DEBUG - self.optional_params: {'dimensions': 1536}
2025-12-08 14:32:37,034 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://api.openai.com/v1 \
-d '{'model': 'text-embedding-3-small', 'input': ['s1: Simple test-time scaling\nNiklas Muennighoff*134 Zitong Yang*1 Weijia Shi*23 Xiang Lisa Li*1 Li Fei-Fei1 Hannaneh Hajishirzi23\nLuke Zettlemoyer2 Percy Liang1 Emmanuel Cands1 Tatsunori Hashimoto1\nAbstract\nTest-time scaling is a promising new approach to\nlanguage modeling that uses extra test-time com-\nputetoimproveperformance. Recently,OpenAIs\no1 model showed this capability but did not pub-\nlicly share its methodology, leading to many repli-\ncation efforts. We seek the simplest approach to\nachievetest-timescalingandstrongreasoningper-\nformance. First, we curate a small datasets1K\nof 1,000 questions paired with reasoning traces\nrelying on three criteria we validate through abla-\ntions: difficulty,diversity,andquality. Second,we\ndevelop budget forcing to control test-time com-\npute by forcefully terminating the models think-\ning process or lengthening it by appending Wait\nmultiple times to the models generation when it\ntries to end. This can lead the model to double-\ncheck its answer, often fixing incorrect reasoning\nsteps. After supervised finetuning the Qwen2.5-\n32B-Instruct language model ons1Kand equip-\nping it with budget forcing, our models1-32B ex-\nceeds o1-preview on competition math questions\nby up to 27% (MATH and AIME24). Further,\nscaling s1-32B with budget forcing allows extrap-\nolating beyond its performance without test-time\nintervention: from 50% to 57% on AIME24. Our\nmodel, data, and code are open-source athttps:\n//github.com/simplescaling/s1.\n1. Introduction\nPerformance improvements of language models (LMs) over\nthe past years have largely relied on scaling up train-time\ncompute using large-scale self-supervised pretraining (Ka-\nplan et al., 2020; Hoffmann et al., 2022). The creation of\nthese powerful models has set the stage for a new scaling\nparadigm built on top of them:test-time scaling. The aim\n*Equal contribution. ZY and NM started the project. WS, NM\nand ZY collected the prompts, XL, ZY and NM, built the data\npipeline, LZ and WS proposed using a 1K subset and ZY and\nNM built budget forcing.1 Stanford University.2 University of\nWashington, Seattle.3 Allen Institute for AI.4 Contextual AI.\n512 2048\n65\n75\n85\n95Accuracy (%)\nMathematical\nProblem Solving\n(MATH500)\n512 2048 8192\n0\n20\n40\n60\nCompetition\nMath\n(AIME24)\n1024 4096\n40\n50\n60\nPhD-Level\nScience Questions\n(GPQA Diamond)\nAverage thinking time (tokens)\nFigure 1.Test-time scaling with s1-32B.We benchmarks1-32B\non reasoning-intensive tasks and vary test-time compute.\nof this approach is to increase the compute at test time to\nget better results. There has been much work exploring this\nidea (Snell et al., 2024; Welleck et al., 2024), and the via-\nbility of this paradigm was recently validated by OpenAI\no1 (OpenAI, 2024). o1 has demonstrated strong reasoning\nperformance with consistent gains from scaling test-time\ncompute. OpenAI describes their approach as using large-\nscalereinforcementlearning(RL)implyingtheuseofsizable\namounts of data (OpenAI, 2024). This has led to various\nattempts to replicate their models relying on techniques like\nMonte Carlo Tree Search (Gao et al., 2024b; Zhang et al.,\n2024a), multi-agent approaches (Qin et al., 2024), and oth-\ners (Wang et al., 2024a; Huang et al., 2024b; 2025). Among\nthese approaches, DeepSeek R1 (DeepSeek-AI et al., 2025)\nhas successfully replicated o1-level performance, also em-\nploying reinforcement learning via millions of samples and\nmultiple training stages. However, despite the large num-\nber of o1 replication attempts, none have openly replicated\na clear test-time scaling behavior. Thus, we ask: what is\nthe simplest approach to achieve both test-time scaling and\nstrong reasoning performance?\nWeshowthattrainingononly1,000sampleswithnext-token\nprediction and controlling thinking duration via a simple\ntest-time technique we refer to asbudget forcingleads to\na strong reasoning model that scales in performance with\nmore test-time compute. Specifically, we constructs1K,\nwhich consists of 1,000 carefully curated questions paired\nwith reasoning traces and answers distilled from Gemini\nThinking Experimental (Google, 2024). We perform super-\nvised fine-tuning (SFT) of an off-the-shelf pretrained model\n1\narXiv:2501.19393v3  [cs.CL]  1 Mar 2025s1: Simple test-time scaling\non our small dataset requiring just 26 minutes of training\non 16 H100 GPUs. After training, we control the amount\nof test-time compute our model spends usingbudget forc-\ning: (I) If the model generates more thinking tokens than\na desired limit, we forcefully end the thinking process by\nappending an end-of-thinking token delimiter. Ending the\nthinking this way makes the model transition to generating\nitsanswer. (II)Ifwewantthemodeltospendmoretest-time\ncompute on a problem, we suppress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our mode', 'ress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our models1-\n32Bexhibits test-time scaling (Figure 1). Further,s1-32Bis\nthe most sample-efficient reasoning model and outperforms\nclosed-source models like OpenAIs o1-preview (Figure 2).\nWe conduct extensive ablation experiments targeting (a) our\nselection of 1,000 (1K) reasoning samples and (b) our test-\ntime scaling. For (a), we find that jointly incorporating\ndifficulty, diversity, and quality measures into our selec-\ntion algorithm is important. Random selection, selecting\nsamples with the longest reasoning traces, or only selecting\nmaximally diverse samples all lead to significantly worse\nperformance (around30% on AIME24 on average). Train-\ning on our full data pool of 59K examples, a superset of\ns1K, does not offer substantial gains over our 1K selection.\nThis highlights the importance of careful data selection and\nechoes prior findings for instruction tuning (Zhou et al.,\n2023). For(b), we define desiderata for test-time scaling\nmethods to compare different approaches. Budget forcing\nleads to the best scaling as it has perfect controllability with\na clear positive slope leading to strong performance.\nInsummary,ourcontributionsare: Wedevelopsimplemeth-\nods for creating a sample-efficient reasoning dataset (2)\nand test-time scaling (3); Based on these we builds1-32B\nwhich is competitive with o1-preview (4); We ablate sub-\ntleties of data (5.1) and test-time scaling (5.2). We end\nwith a discussion to motivate future work on simple rea-\nsoning (6). Our code, model, and data are open-source at\nhttps://github.com/simplescaling/s1.\n2. Reasoning data curation to create s1K\nIn this section, we describe our process for creating a large\ndataset first in 2.1 and then filtering it down tos1Kin 2.2.\n2.1. Initial collection of 59K samples\nWecollectaninitial59,029questionsfrom16sourcesfollow-\ning three guiding principles.Quality: Datasets should be\nhigh-quality; we always inspect samples and ignore datasets\nwith, e.g., poor formatting;Difficulty: Datasets should be\nchallenging and require significant reasoning effort;Diver-\nsity: Datasetsshouldstemfromvariousfieldstocoverdiffer-\nent reasoning tasks. We collect datasets of two categories:\nCuration of existing datasetsOur largest source is Nu-\nminaMATH (LI et al., 2024) with 30,660 mathematical\nproblems from online websites. We also include histori-\ncal AIME problems (1983-2021). To enhance diversity,\nwe add OlympicArena (Huang et al., 2024a) with 4,250\nquestions spanning Astronomy, Biology, Chemistry, Com-\nputer Science, Geography, Mathematics, and Physics from\nvarious Olympiads. OmniMath (Gao et al., 2024a) adds\n4,238 competition-level mathematics problems. We also\ninclude 2,385 problems from AGIEval (Zhong et al., 2023),\nwhich features questions from standardized tests like SAT\nand LSAT, covering English, Law, and Logic. We refer to\nTable 7 in C for our other sources.\nNew datasets in quantitative reasoning To comple-\nment these existing datasets, we create two original\ndatasets. s1-prob consists of 182 questions from the prob-\nability section of Stanford Universitys Statistics Depart-\nments PhD Qualifying Exams (https://statistics.\nstanford.edu), accompanied by handwritten solutions\nthat cover difficult proofs. The probability qualifying exam\nis held yearly and requires professional-level mathemati-\ncal problem-solving. s1-teasers comprises 23 challenging\nbrain-teasers commonly used in interview questions for\nquantitative trading positions. Each sample consists of a\nproblem and solution taken from PuzzledQuant (https:\n//www.puzzledquant.com/). Weonlytakeexamples\nwith the highest difficulty level ("Hard").\nFor each question, we generate a reasoning trace and solu-\ntion using the Google Gemini Flash Thinking API (Google,\n2024)extractingitsreasoningtraceandresponse. Thisyields\n59K triplets of a question, generated reasoning trace, and\ngenerated solution. Examples from our dataset are in D.2.\nWe decontaminate all samples against our evaluation ques-\ntions (MATH500, GPQA Diamond, AIME24; C.5) using\n8-grams and deduplicate the data.\n2.2. Final selection of 1K samples\nWe could directly train on our pool of 59K questions, how-\never, our goal is to find thesimplest approach with minimal\nresources. Thus, we go through three stages of filtering to\narriveataminimalsetof1,000samplesrelyingonourthree\nguiding data principles: Quality, Difficulty, and Diversity.\nQuality We first remove any questions where we ran into\nanyAPIerrorsreducingourdatasetto54,116samples. Next,\nwefilteroutlow-qualityexamplesbycheckingiftheycontain\nany string patterns with formatting issues, such as ASCII\nart diagrams, non-existent image references, or inconsistent\nquestionnumberingreducingourdatasetto51,581examples.\n2s1: Simple test-time scaling\nGeometry', '-qualityexamplesbycheckingiftheycontain\nany string patterns with formatting issues, such as ASCII\nart diagrams, non-existent image references, or inconsistent\nquestionnumberingreducingourdatasetto51,581examples.\n2s1: Simple test-time scaling\nGeometry Number\ntheory\nCombin\natorics\nReal\nfunctions\nBiology\nComplex\nfunctions\nQuantum\ntheory\nField\ntheory\nCalculus of\nvariations\nDifference\nequations\nElectro\ndynamics\nGroup\ntheory\nLinear\nalgebra\nProbability\ntheory\nAlgebraic\nsystems\nMechanics\nThermo\ndynamics\nDifferential\nequations\nComputer\nscience\nNumerical\nanalysis\nCalculus\nAlgebraic\nstructures\nAstronomy\nDynamical\nsystems\nStatistical\nmechanics\nOperations\nresearch\nMath\nematics\neducation\nMeasure\ntheory\nConvex\ngeometry\nFluid\nmechanics\nAlgebraic\ngeometry\nStatistics General\ntopology\nEconomics\nAssociative\nrings\nGeneral\nrelativity\nDifferential\ngeometry\nMath\nematical\nlogic\nPartial\ndifferential\nequations\nInformation\ntheory\nSolid\nmech\nanics\nFunctional\nanalysis\nSpecial\nfunctions\nComm\nutative\nalgebra\nIntegral\nequations\nIntegral\ntransform\nApproxi\nmation\ntheory\nPotential\ntheory\nHarmonic\nanalysis\nControl\ntheory\nGeo\nphysics\n1000 17000 800000 N/A\nNumber of Examples\n80\n85\n90\n95\n100MATH500 Accuracy (%)\ns1\nr1-distill\nSky-T1\nQwQ\nBespoke-Stratos\no1-preview\nMost sample-efficient\nFigure 2.s1Kands1-32B. (left)s1Kisadatasetof1,000high-quality,diverse,anddifficultquestionswithreasoningtraces. (right)s1-32B,\na 32B parameter model finetuned ons1K is on the sample-efficiency frontier. See Table 1 for details on other models.\nFrom this pool, we identify 384 samples for our final 1,000\nsamples from datasets that we perceive as high-quality and\nnot in need of further filtering (see C.4 for details).\nDifficulty Fordifficulty, weusetwoindicators: modelper-\nformance and reasoning trace length. We evaluate two mod-\nels on each question: Qwen2.5-7B-Instruct and Qwen2.5-\n32B-Instruct (Qwen et al., 2024), with correctness assessed\nby Claude 3.5 Sonnet comparing each attempt against the\nreference solution (see C.3 for the grading protocol). We\nmeasure the token length of each reasoning trace to indicate\nproblem difficulty using the Qwen2.5 tokenizer. This relies\non the assumption that more difficult problems require more\nthinkingtokens. Basedonthegrading,weremovequestions\nthat either Qwen2.5-7B-Instruct or Qwen2.5-32B-Instruct\ncan solve correctly and thus may be too easy. By using two\nmodels we reduce the likelihood of an easy sample slipping\nthrough our filtering due to a rare mistake on an easy ques-\ntion of one of the models. This brings our total samples\ndown to 24,496, setting the stage for the next round of sub-\nsampling based on diversity. While filtering with these two\nmodels may be optimized for our setup as we will also use\nQwen2.5-32B-Instruct as our model to finetune, the idea of\nmodel-based filtering generalizes to other setups.\nDiversity To quantify diversity, we classify questions into\ndomainsusingClaude3.5SonnetbasedontheMathematics\nSubject Classification (MSC) system (e.g., geometry, com-\nbinatorics, etc.) from the American Mathematical Society.1\nThe taxonomy focuses on topics in mathematics but also\nincludes other sciences such as biology, physics, and eco-\n1https://mathscinet.ams.org/mathscinet/\nmsc/msc2020.html\nnomics. Toselectourfinalexamplesfromthepoolof24,496\nquestions, we first choose one domain uniformly at random.\nThen, we sample one problem from this domain according\ntoadistributionthatfavorslongerreasoningtraces(seeC.4\nfordetails)asmotivatedin Difficulty. Werepeatthisprocess\nuntil we have 1,000 total samples spanning 50 domains.\nIn 5.1, we will show that using our three criteria in combi-\nnation is important, as only relying on quality, diversity, or\ndifficulty in isolation leads to worse datasets. Some distilled\ngenerations are incorrect, which we allow in our data as we\nfocusoncapturingthereasoningprocessratherthanentirely\ncorrect solutions. Our grader (C.3) deems 53.6% correct\nin s1Kand 63.0% in our follow-ups1K-1.1(see A).\n3. Test-time scaling\n3.1. Method\nWe classify test-time scaling methods into1) Sequential,\nwherelatercomputationsdependonearlierones(e.g.,along\nreasoningtrace),and 2)Parallel,wherecomputationsrunin-\ndependently(e.g.,majorityvoting)(Snelletal.,2024;Brown\netal.,2024). Wefocusonsequentialscalingasintuitivelywe\nbelieve it should scale better, since later computations can\nbuild on intermediate results, allowing for deeper reasoning\nanditerativerefinement. Weproposenewsequentialscaling\nmethods and ways to benchmark them.\nBudget forcing We propose a simple decoding-time in-\ntervention by forcing a maximum and/or minimum number\nof thinking tokens. Specifically, we enforce a maximum\ntoken count by simply appending the end-of-thinking token\ndelimiter and optionally Final Answer: to early exit\n3s1: Simple test-time scaling\nHo w many r in r aspberry ?\nLet \' s br eak do wn the pr ocess of counting the letter \' r \' in the \nwor d " r aspberry " ...\n* First letter: \' r \' - This is an \' r \', count = 1.\n* Secon', 'nally Final Answer: to early exit\n3s1: Simple test-time scaling\nHo w many r in r aspberry ?\nLet \' s br eak do wn the pr ocess of counting the letter \' r \' in the \nwor d " r aspberry " ...\n* First letter: \' r \' - This is an \' r \', count = 1.\n* Second letter: \' a \' - Not an \' r \', count r emains 1 ...\n* Sixth letter: \' e \' - Not an \' r \', count r emains 1.\n* Se venth letter: \' r \' - This is an \' r \', count = 2.\n* Eighth letter: \'y \' - Not an \' r \', count r emains 2 ...\nThe number of \' r \' s in " r aspberry " is 2.\n, let \' s r e-r ead the question car efully . It asks "Ho w many r \nin r aspberry ?" ... * r - a - s - p - b - e - r - r - y ... * First \' r \' ... * \nSecond \' r \' ... * Thir d \' r \' ... Count = 3 ...\nW ait\nMy initial answer of 2 was incorr ect due to a quick r eading of \nthe wor d. Final Answer: The final answer is\nQuestion\nReasoning tr ace\nResponse\nFigure 3.Budget forcing with s1-32B.The model tries to stop\nafter ...is 2., but we suppress the end-of-thinking token delimiter\ninstead appending Wait leadings1-32Bto self-correct its answer.\nthe thinking stage and make the model provide its current\nbest answer. To enforce a minimum, we suppress the gener-\nation of the end-of-thinking token delimiter and optionally\nappend the string Wait to the models current reasoning\ntrace to encourage the model to reflect on its current gen-\neration. Figure 3 contains an example of how this simple\napproach can lead the model to arrive at a better answer.\nBaselines We benchmark budget forcing with:(I) Con-\nditional length-control methods, which rely on telling the\nmodel in the prompt how long it should generate for. We\ngroup them by granularity into (a) Token-conditional con-\ntrol: We specify an upper bound of thinking tokens in the\nprompt; (b) Step-conditional control: We specify an upper\nbound of thinking steps, where each step is around 100 to-\nkens; (c) Class-conditional control: We write two generic\npromptsthattellthemodeltoeitherthinkforashortorlong\namount of time (see E.1 for details).(II) Rejection sam-\npling, which samples until a generation fits a predetermined\ncompute budget. This oracle captures the posterior over\nresponses conditioned on its length.\n3.2. Metrics\nWe establish a set of desiderata as evaluation metrics to\nmeasure test-time scaling across methods. Importantly, we\ndo not only care about the accuracy a method can achieve\nbut also its controllability and test-time scaling slope. For\neach method we consider, we run a set of evaluations\n\ue22d varying test-time compute on a fixed benchmark, e.g.\nAIME24. This produces a piece-wise linear function\nwith compute as the x-axis measured in thinking tokens and\naccuracyasthey-axis(seeFigure1,wheretherightmostdot\nfor AIME24 corresponds to(7320)=57% ). We measure\nthree metrics:\nControl = 1\n|\ue22d|\n\n\ue22d\n(min   max) (1)\nwhere min,max refer to a pre-specified minimum and max-\nimum amount of test-time compute; in our case thinking\ntokens. We usually only constrainmax. As tokens gener-\nated correspond to the amount of test-time compute spent,\nthis metric measures the extent to which a method allows\ncontrollability over the use of that test-time compute. We\nreport it as a percentage with 100% being perfect control.\nScaling = 1(|\ue22d|\n2\n)\n\n,\ue22d\n>\n() ()\n (2)\nScalingistheaverageslopeofthepiece-wiselinearfunction.\nIt must be positive for useful methods and larger is better.\nPerformance =max\n\ue22d\n() (3)\nPerformance is simply the maximum performance the\nmethod achieves on the benchmark. A method with mono-\ntonically increasing scaling achieves 100% performance on\nany benchmark in the limit. However, the methods we in-\nvestigate eventually flatten out or further scaling fails due to\ncontrol or context window limitations.\n4. Results\n4.1. Setup\nTraining We perform supervised finetuning on Qwen2.5-\n32B-Instruct usings1Kto obtain our models1-32Busing\nbasic hyperparameters outlined in D. Finetuning took 26\nminutes on 16 NVIDIA H100 GPUs with PyTorch FSDP.\nEvaluation We select three representative reasoning\nbenchmarks widely used in the field:AIME24(of America,\n2024)has30problemsthatwereusedinthe2024American\nInvitational Mathematics Examination (AIME) held from\nJanuary 31  February 1, 2024. AIME tests mathematical\nproblem-solving with arithmetic, algebra, counting, geome-\ntry, number theory, probability, and other secondary school\nmathtopics. High-scoringhighschoolstudentsinthetestare\ninvitedtoparticipateintheUnitedStatesofAmericaMathe-\nmaticsOlympiad(USAMO).AllAIMEanswersareintegers\nranging from000to999, inclusive. Some AIME problems\nrely on figures that we provide to our model using the vec-\ntor graphics language Asymptote as it cannot take image\ninputs. MATH500(Hendrycks et al., 2021) is a benchmark\nof competition math problems of varying difficulty. We\nevaluate on the same 500 samples selected by OpenAI in\nprior work (Lightman et al., 2023).GPQA Diamond(Rein\net al., 2023) consists of 198 PhD-level science questions\n4s1: Simple test-time scaling\nF', ' competition math problems of varying difficulty. We\nevaluate on the same 500 samples selected by OpenAI in\nprior work (Lightman et al., 2023).GPQA Diamond(Rein\net al., 2023) consists of 198 PhD-level science questions\n4s1: Simple test-time scaling\nFor cing 2048/4096 max thinking \ntokens\nIgnoring end-of-thinking 2x/4x/6x and \nappending  W ait  thus for cing it to continue \nr easoning when it tries to stop\n(a) Sequential scaling via budget forcing\nPar allel scaling via Majority \nvoting with base model \n(Qwen2.5-32B-Instruct)\nSequential scaling \nvia For cing with s1\n60% accur acy (b) Parallel scaling via majority voting\nFigure 4.Sequential and parallel test-time scaling.(a): Budget forcing shows clear scaling trends and extrapolates to some extent. For\nthe three rightmost dots, we prevent the model from stopping its thinking 2/4/6 times, each time appending Wait to its current reasoning\ntrace. (b): For Qwen2.5-32B-Instruct we perform 64 evaluations for each sample with a temperature of 1 and visualize the performance\nwhen majority voting across 2, 4, 8, 16, 32, and 64 of these.\nfromBiology,ChemistryandPhysics. ExpertswithPhDsin\nthe corresponding domains only achieved 69.7% on GPQA\nDiamond (OpenAI, 2024). When we write GPQA in the\ncontext of evaluation in this work, we always refer to the\nDiamond subset. We build on the lm-evaluation-harness\nframework (Gao et al., 2021; Biderman et al., 2024). Unless\notherwise specified, we evaluate with a temperature of 0\n(greedy) and measure accuracy (equivalent to pass@1).\nOther models We benchmarks1-32Bagainst: OpenAI\no1 series(OpenAI, 2024), closed-source models that pop-\nularized test-time scaling;DeepSeek r1 series(DeepSeek-\nAI et al., 2025), open-weight reasoning models with up to\no1-level performance; QwensQwQ-32B-preview(Team,\n2024), a 32B open-weight reasoning model without dis-\nclosed methodology;Sky-T1-32B-Preview(Team, 2025)\nandBespoke-32B(Labs, 2025), open models with open rea-\nsoningdatadistilledfromQwQ-32B-previewandr1; Google\nGemini 2.0 Flash Thinking Experimental(Google, 2024),\nthe API that we distill from. As it has no official evaluation\nscores, we use the Gemini API to benchmark it ourselves.\nHowever, the recitation error of the Gemini API makes\nevaluation challenging.2 We circumvent this, by manually\ninsertingall30AIME24questionsinitswebinterfacewhere\ntheerrordoesnotappear. However,weleaveoutMATH500\n(500 questions) and GPQA Diamond (198 questions), thus\nthey are N.A. in Table 1. Our model,s1-32B, is fully open\nincluding weights, reasoning data, and code.\n2https://github.com/google/\ngenerative-ai-docs/issues/257\nTable 1.s1-32B is a strong open reasoning model.We evaluate\ns1-32B, Qwen, and Gemini (some entries are unknown (N.A.), see\n4). Other results are from the respective reports (Qwen et al.,\n2024; Team, 2024; OpenAI, 2024; DeepSeek-AI et al., 2025; Labs,\n2025; Team, 2025). # ex. = number examples used for reasoning\nfinetuning; BF = budget forcing. See A for our betters1.1 model.\nModel # ex. AIME\n2024\nMATH\n500\nGPQA\nDiamond\nAPI only\no1-preview N.A. 44.6 85.5 73.3\no1-mini N.A. 70.0 90.0 60.0\no1 N.A. 74.4 94.8 77.3\nGemini 2.0 N.A. 60.0 N.A. N.A.Flash Think.\nOpen Weights\nQwen2.5- N.A. 26.7 84.0 49.032B-Instruct\nQwQ-32B N.A. 50.0 90.6 54.5\nr1 800K 79.8 97.3 71.5\nr1-distill 800K 72.6 94.3 62.1\nOpen Weights and Open Data\nSky-T1 17K 43.3 82.4 56.8\nBespoke-32B 17K 63.3 93.0 58.1\ns1 w/o BF 1K 50.0 92.6 56.6\ns1-32B 1K 56.7 93.0 59.6\n5s1: Simple test-time scaling\n4.2. Performance\nTest-time scaling Figure 1 shows the performance ofs1-\n32Bwithbudgetforcingscaleswithmoretest-timecompute.\nInFigure4(left),weexpandtheplotfromFigure1(middle)\nshowing that while we can improve AIME24 performance\nusing our budget forcing technique (3) and more test-time\ncomputeitdoeseventuallyflattenoutatsixtimes. Suppress-\ningtheend-of-thinkingtokendelimitertoooftencanleadthe\nmodel into repetitive loops instead of continued reasoning.\nIn Figure 4 (right), we show that after training Qwen2.5-\n32B-Instruct on our 1,000 samples to produces1-32Band\nequipping it with the simple budget forcing technique, it\noperates in a different scaling paradigm. Scaling test-time\ncomputeonthebasemodelviamajorityvotingcannotcatch\nup with the performance ofs1-32B which validates our\nintuition from 3 that sequential scaling is more effective\nthan parallel. We provide example generations ofs1-32B in\nFigure 5.\nSample-efficiency In Figure 2 (right) and Table 1 we\ncompare s1-32B with other models. We find that s1-\n32B is the most sample-efficient open data reasoning\nmodel. It performs significantly better than our base model\n(Qwen2.5-32B-Instruct) despite just training it on an ad-\nditional 1,000 samples. The concurrently released r1-32B\nshows stronger performance thans1-32B while also only\nusingSFT(DeepSeek-AIetal.,2025). However,itistrained\non 800more reasoning samples. It is an open question\nwhether one can achieve their performance with just 1,000\nsamples. Finally, our model nearly matches Gemin', 'rformance thans1-32B while also only\nusingSFT(DeepSeek-AIetal.,2025). However,itistrained\non 800more reasoning samples. It is an open question\nwhether one can achieve their performance with just 1,000\nsamples. Finally, our model nearly matches Gemini 2.0\nThinking on AIME24. As the data fors1-32B is distilled\nfrom Gemini 2.0, this shows our distillation procedure was\nlikely effective.\n5. Ablations\n5.1. Data Quantity, Diversity, and Difficulty\nIn 2 we outlined our three guiding principles in curating\ns1K: Quality, Difficulty, and Diversity. Here we test the\nimportanceofcombiningthemandtheoverallefficacyofour\nselection. Only Quality (1K-random): After obtaining our\nhigh-quality reasoning chains from Gemini, we select 1,000\nsamplesatrandom;notrelyingonourdifficultyanddiversity\nfiltering at all. Table 2 shows this approach performs much\nworse thans1K across all benchmarks. Only Diversity\n(1K-diverse): For this dataset, we sample uniformly across\ndomains to maximize diversity disregarding any notion of\ndifficulty. This approach also leads to poor performance\nsimilar to 1K-random.Only Difficulty (1K-longest): Here\nwerelyononeofourdifficultyindicatorsintroducedin2by\nselectingthe1,000sampleswiththelongestreasoningtraces.\nThis approach significantly boosts GPQA performance but\noverall still falls short of usings1K.Maximize Quantity:\nTable 2.s1K data ablations.We budget force (BF) a maximum\nof around 30,000 thinking tokens for all scores in this table. This\nperforms slightly better than the scores without BF (Table 1) as\nit allows the model to finish with a best guess when stuck in an\ninfinite loop. We report 95% paired bootstrap confidence intervals\nfor differences relative to thes1K model using 10,000 bootstrap\nsamples. E.g., the interval [-13%, 20%] means that, with 95% con-\nfidence, the true difference between 59K-full ands1Kis between\n-13% and +20%. If the entire interval is negative, e.g. [-27%, -3%],\nwe can confidently say that the performance is worse thans1K.\nModel AIME\n2024\nMATH\n500\nGPQA\nDiamond\n1K-random 36.7 90.6 52.0\n[-26.7%, -3.3%] [-4.8%, 0.0%] [-12.6%, 2.5%]\n1K-diverse 26.7 91.2 54.6\n[-40.0%, -10.0%] [-4.0%, 0.2%] [-10.1%, 5.1%]\n1K-longest 33.3 90.4 59.6\n[-36.7%, 0.0%] [-5.0%, -0.2%] [-5.1%, 10.1%]\n59K-full 53.3 92.8 58.1\n[-13.3%, 20.0%] [-2.6%, 2.2%] [-6.6%, 8.6%]\ns1K 50.0 93.0 57.6\nFinally, we compare with just training on all of our 59K\nsamples, a superset of all the 1K-sample versions. This\nleads to a strong model but uses much more resources. To\nfinetune on 59K samples, we use 394 H100 GPU hours\nwhiles1-32Bonly required 7 H100 GPU hours. Moreover,\nrelyingonlyon s1Kisextremelycompetitiveasshownin2.\nOverall, combining all three criteria Quality, Difficulty,\nDiversity  via our methodology in 2 is key for sample-\nefficient reasoning training.\n5.2. Test-time scaling methods\nTable 3.Ablations on methods to scale test-time compute on\nAIME24. |\ue22d| refers to the number of evaluation runs used to\nestimate the properties; thus a higher value indicates more robust-\nness. Bold indicates our chosen method and the best values. BF\n= budget forcing, TCC/SCC/CCC = token/step/class-conditional\ncontrol, RS = rejection sampling.\nMethod Control Scaling Performance |\ue22d|\nBF 100% 15 56.7 5\nTCC 40% -24 40.0 5\nTCC + BF 100% 13 40.0 5\nSCC 60% 3 36.7 5\nSCC + BF 100% 6 36.7 5\nCCC 50% 25 36.7 2\nRS 100% -35 40.0 5\n6s1: Simple test-time scaling\n[AIME24]\nAlice and Bob play the following game. A stack of\ntokens lies before them. The players take turns\nwith Alice going first. On each turn, the player\nremoves either1token or4tokens from the stack.\nWhoever removes the last token wins. Find the\nnumber of positive integersless than or equal to\n2024for which there exists a strategy for Bob that\nguarantees that Bob will win the game regardless\nof Alices play.\nThe problem asks for the number of positive integers\n 2024such that Bob has a winning strategy. This\nis a typical impartial game that can be analyzed using\nthe Sprague-Grundy theorem or by directly finding\nthe winning and losing positions. Let be the set of\nwinning positions (number of tokenssuch that the\nplayer whose turn it is can force a win) andbe the\nset of losing positions. [...]\nLet()be the statement "the player whose turn it is\nwhen there aretokens has a winning strategy.(0)\nis false (the player whose turn it is and faces0tokens\nhas already lost). Consider the small values of:\n = 1: The player whose turn it is can take1token\nand win. So(1)is true.1 .\n = 2: The player whose turn it is can take1token,\nleaving1token for the opponent. Since1 , the\nopponent wins. So(2)is false.2 .\n = 3: The player whose turn it is can take1token,\nleaving2tokens for the opponent. Since2 , the\nplayer whose turn it is wins. So(3)is true. 3 \n.[...]\nWe are looking for the number of positive integers\n 2024for which Bob has a winning strategy. This\nmeans that if the game starts withtokens, and Alice\ngoes first, Bob can win. This happens if and only if\nis a losing pos', 'So(3)is true. 3 \n.[...]\nWe are looking for the number of positive integers\n 2024for which Bob has a winning strategy. This\nmeans that if the game starts withtokens, and Alice\ngoes first, Bob can win. This happens if and only if\nis a losing position (because if, then Alices\nfirst move must lead to a winning position for Bob).\n[...]\nLets try to find a pattern in the losing positions\n={0,2,5,7,10,12,15,17,20,22,...}. Differences\nbetween consecutive terms: 2,3,2,3,2,3,2,3,2,...\nThe pattern of differences is alternating2,3. [...]\nThe losing positions are of the form5or 5+2,\nwhere  0. We need to find the number of positive\nintegers 2024thatareinthesetoflosingpositions\n. [...]\nCase 1:  = 5, where  1. 1  5  2024\n15    20245 = 404.8Since is an integer,\n1  404. The number of values ofis404.\nCase 2: = 5+2, where  0. 5+2  2024\n5  2022  20225 = 404.4Since is an in-\nteger, 0    404. The number of values ofis\n4040+1=405 .\nThe total number of positive integers  2024that\nare losing positions is404+405= 809 .\n[MATH500]\nFind the projection ofonto  =(2\n6\n3\n)\nif=8.\nTheprojectionofvector  ontovector\n is given by the formula:\nproj =  \n2\nWearegiven  =\n(2\n6\n3\n)\nand  =8.\nFirst, we need to find the magnitude\nsquared of, which is2.\n2 = =22+62+32 =4+36+9=49 .\nNow we can plug the values into the\nprojection formula:\nproj = 8\n49\nSubstitute the vector:\nproj = 8\n49\n(2\n6\n3\n)\nMultiply the scalar by each compo-\nnent of the vector:\nproj =\n\n\n\n\n8\n49 2\n8\n49 6\n8\n49 3\n\n\n\n\n=\n\n\n\n\n16\n4948\n4924\n49\n\n\n\n\nFinal Answer: The final answer is\n\n\n\n\n16\n4948\n4924\n49\n\n\n\n\n[GPQA]\nA quantum mechanical particle of\nmassmoves in two dimensions in\nthe following potential, as a function\nof the polar coordinates(,):\n(,)= 1\n22 + 3\n22cos2()\nFind the energy spectrum. Hint:\nWrite the potential in Cartesian co-\nordinates.\nThe problem asks for the energy spec-\ntrum of a particle in a given 2D po-\ntential. The potential is given in polar\ncoordinates(,):\n(,)= 1\n22 + 3\n22cos2().\nWe can rewrite the potential as:\n(,)= 1\n22(1+3cos 2()).\nThe Hamiltonian for a particle in 2D\nis given by:\n = 2\n2\n(\n2\n2 + 2\n2\n)\n+(,).\nIn polar coordinates(,), the Lapla-\ncian operator is:\n2 = 1\n\n\n\n(\n \n\n)\n+ 1\n2\n2\n2.\nSo the Hamiltonian in polar coordi-\nnates is:\n = 2\n2\n[\n1\n\n\n\n(\n \n\n)\n+ 1\n2\n2\n2\n]\n+(,).\nThe time-independent Schrdinger\nequation is = . We can try\nto separate variables by assuming a so-\nlution of the form(,)= ()().\n[...]\nThis confirms the energy spectrum is:\n =\n\n\n\n(\n2 + + 3\n2\n)\n.\nFinal Answer:  =\n\n\n\n\n(\n2 + + 3\n2\n)\n.\nFigure 5.Example model outputs.We pick a question from AIME24 (left), MATH500 (middle), and GPQA (right), where our model\ngenerates the correct answer. The black text is the prompt, the light blue text is the reasoning trace, and the blue text is the answer of\ns1-32B. The gray ellipsis [...] indicates that the text was trimmed to fit this page, but the generated text is actually longer.\n7s1: Simple test-time scaling\nBudget forcing In Table 3 we compare the test-time scal-\ning methods we have introduced in 3. Overall, we find that\nbudget forcingprovides perfect control, good scaling, and\nleads to our best AIME24 score. Thus, this is the method\nwe use fors1-32B in Figure 1 and in 4. In Table 4, we\ncomparedifferentstringsforextrapolatingperformance. We\nfind that Wait generally gives the best performance.\nClass-conditional controlWe provide benchmark scores\nfor this method in E.1 and summarize three findings here:\n(1)Token-conditionalcontrolfailswithoutbudgetforcing,as\nour model cannot reliably count tokens - even when trained\nto do so.(2) Under step-conditional control, the model gen-\nerates a similar total number of tokens when given different\nstep targets, as the model goes from few steps with many\ntokens per step, to many steps with few tokens in each step.\nThus, the model learns to hack its way around the com-\npute constraint making the controllability of this method\nmediocre. (3) Class-conditional control can work - telling a\nmodel to simply think longer can increase its test-time com-\npute and performance, which leads good scaling in Table 3.\nTable 4.Budget forcing extrapolation ablations.We compare\nignoring the end-of-thinking delimiter twice and appending none\nor various strings.\nModel AIME\n2024\nMATH\n500\nGPQA\nDiamond\nNo extrapolation 50.0 93.0 57.6\n2x without string 50.0 90.2 55.1\n2x Alternatively 50.0 92.2 59.6\n2x Hmm 50.0 93.0 59.6\n2x Wait 53.3 93.0 59.6\nRejectionsampling Surprisingly,wefindthatsimplysam-\npling until the generation fits a specific length leads to an\ninverse scaling trend as depicted in Figure 6. In E.2 we in-\n3072 4096 5120\nAverage thinking time (tokens)\n22\n27\n32\n37\n42Accuracy (%)\nFigure 6.Rejection sampling on AIME24 with s1-32B.We sam-\nplewithatemperatureof1untilallgenerationshavelessthan(from\nleft to right) 3500, 4000, 5000,', 'in Figure 6. In E.2 we in-\n3072 4096 5120\nAverage thinking time (tokens)\n22\n27\n32\n37\n42Accuracy (%)\nFigure 6.Rejection sampling on AIME24 with s1-32B.We sam-\nplewithatemperatureof1untilallgenerationshavelessthan(from\nleft to right) 3500, 4000, 5000, 8000, and 16000 thinking tokens\nrequiring an average of 655, 97, 8, 3, 2, and 1 tries per sample.\nspectaquestion,whichwasansweredcorrectlybythemodel\nwhen rejection sampling for 4000, but not for the 8000\ntokensetting. Inthe  4000settingthemodeldirectlyjumps\nto the correct approach, while for the 8000setting it back-\ntracks a lot. We hypothesize that there is a correlation such\nthat shorter generations tend to be the ones where the model\nwas on the right track from the start, whereas longer ones\ntend to be ones where the model made mistakes and thus\nbacktracks or questions itself. This leads to longer samples\noften being wrong when rejection sampling and thus the\ninverse scaling trend.\n6. Discussion and related work\n6.1. Sample-efficient reasoning\nModels There are a number of concurrent efforts to build\nmodelsthatreplicatetheperformanceofo1(OpenAI,2024).\nFor example, DeepSeek-r1 and k1.5 (DeepSeek-AI et al.,\n2025; Team et al., 2025) are built with reinforcement learn-\ning methods, while others rely on SFT using tens of thou-\nsands of distilled examples (Team, 2025; Xu et al., 2025;\nLabs, 2025). We show that SFT on only 1,000 examples\nsuffices to build a competitive reasoning model matching\no1-preview and produces a model that lies on the pareto\nfrontier (Figure 2). Further, we introduce budget forcing\nwhich combined with our reasoning model leads to the first\nreproduction of OpenAIs test-time scaling curves (OpenAI,\n2024). Why does supervised finetuning on just 1,000 sam-\nples lead to such performance gains? We hypothesize that\nthe model is already exposed to large amounts of reason-\ning data during pretraining which spans trillions of tokens.\nThus, the ability to perform reasoning is already present in\nour model. Our sample-efficient finetuning stage just acti-\nvatesitandwescaleitfurtherattesttimewithbudgetforcing.\nThis is similar to the "Superficial Alignment Hypothesis"\npresented in LIMA (Zhou et al., 2023), where the authors\nfind that 1,000 examples can be sufficient to align a model\nto adhere to user preferences.\nBenchmarks and methods To evaluate and push the lim-\nits of these models, increasingly challenging benchmarks\nhave been introduced, such as Olympiad-level science com-\npetitions (He et al., 2024; Jain et al., 2024; Zhong et al.,\n2023)andothers(Srivastavaetal.,2023;Glazeretal.,2024;\nSu et al., 2024; Kim et al., 2024; Phan et al., 2025). To\nenhancemodelsperformanceonreasoning-relatedtasks,re-\nsearchers have pursued several strategies: Prior works have\nexplored continuing training language models on special-\nized corpora related to mathematics and science (Azerbayev\netal.,2023;Yangetal.,2024),sometimesevensynthetically\ngenerated data (Yu et al., 2024). Others have developed\ntraining methodologies specifically aimed at reasoning per-\nformance (Zelikman et al., 2022; 2024; Luo et al., 2025;\n8s1: Simple test-time scaling\nYuanetal.,2025;Wuetal.,2024a). Anothersignificantline\nof work focuses on prompting-based methods to elicit and\nimprove reasoning abilities, including methods like Chain-\nof-Thought prompting (Wei et al., 2023; Yao et al., 2023a;b;\nBi et al., 2023; Fu et al., 2023; Zhang et al., 2024b; Xi-\nang et al., 2025; Hu et al., 2024; Diao et al., 2024). These\ncombined efforts aim to advance the reasoning ability of\nlanguage models, enabling them to handle more complex\nand abstract tasks effectively.\n6.2. Test-time scaling\nMethods Asweintroducein3,wedifferentiatetwometh-\nodstoscaletest-timecompute: parallelandsequential. The\nformer relies on multiple solution attempts generated in par-\nallel and selecting the best outcome via specific criteria.\nThese criteria include choosing the most frequent response\nfor majority voting or the best response based on an exter-\nnal reward for Best-of-N (Brown et al., 2024; Irvine et al.,\n2023; Levi, 2024). Unlike repeated sampling, previous se-\nquential scaling methods let the model generate solution\nattempts sequentially based on previous attempts, allowing\nit to refine each attempt based on previous outcomes (Snell\net al., 2024; Hou et al., 2025; Lee et al., 2025). Tree-based\nsearch methods (Gandhi et al., 2024; Wu et al., 2024b) offer\na hybrid approach between sequential and parallel scaling,\nsuch as Monte-Carlo Tree Search (MCTS) (Liu et al., 2024;\nZhang et al., 2023; Zhou et al., 2024; Choi et al., 2023) and\nguided beam search (Xie et al., 2023).REBASE (Wu et al.,\n2024b) employs a process reward model to balance exploita-\ntion and pruning during tree search. Empirically,REBASE\nhas been shown to outperform sampling-based methods and\nMCTS (Wu et al., 2024b). Reward models (Lightman et al.,\n2023; Wang et al., 2024b;c) play a key role in these meth-\nods. They come in two variants: outcome reward models\nand p', 'Empirically,REBASE\nhas been shown to outperform sampling-based methods and\nMCTS (Wu et al., 2024b). Reward models (Lightman et al.,\n2023; Wang et al., 2024b;c) play a key role in these meth-\nods. They come in two variants: outcome reward models\nand process reward models. Outcome reward models (Xin\net al., 2024; Ankner et al., 2024) assign a score to complete\nsolutions and are particularly useful in Best-of-N selection,\nwhile process reward models (Lightman et al., 2023; Wang\net al., 2024b; Wu et al., 2024b) assess individual reasoning\nsteps and are effective in guiding tree-based search methods.\nLimits to further test-time scalingWe have shown that\nbudget forcing allows extrapolating test-time compute in\n4, e.g., improving AIME24 performance from 50% to 57%.\nHowever, it has two key limitations when scaling further: it\neventuallyflattens out(Figure 4), and thecontext window\nof the underlying language model constrains it. Despite\nthese constraints, our work shows test-time scaling across a\nwide range of accuracies (Figure 1), partly because scaling\ndown test-time compute behaves predictably and does not\nsuffer from these constraints.\nContinuing test-time scaling will require approaches that\ncan further extrapolate test-time compute. How can we get\n2048 8192 32768 131072\nAverage thinking time (tokens)\n30\n40\n50\n60Accuracy (%)\nREBASE\nMajority\nSequential\nscaling\nFigure 7.Scalingfurtherwithparallelscalingmethods. Allmet-\nrics averaged over the 30 questions in AIME24. Average thinking\ntokensfor REBASE donotaccountfortheadditionalcomputefrom\nthe reward model. For sequential scaling, we prompt the model\nto use up to (from left to right) 32, 64, 256, and 512 steps. For\nREBASE and majority voting we generate 16 parallel trajectories\nto aggregate across.\nsuch extrapolation? There may be improvements to budget\nforcing such as rotating through different strings, not only\nWait, or combining it with frequency penalties or higher\ntemperature to avoid repetitive loops. An exciting direc-\ntion for future work is also researching whether applying\nbudget forcing to a reasoning model trained with reinforce-\nmentlearningyieldsbetterextrapolation; orifRLallowsfor\nnew ways of test-time scaling beyond budget forcing. Our\nwork definestheright metrics (3.2) Control, Scaling, and\nPerformance  to enable future research and progress on\nextrapolating test-time compute.\nParallel scaling as a solutionParallel scaling offers one\nsolution to the limits of sequential scaling, thus we aug-\nment our sequentially scaled model with two methods:(I)\nMajority voting: After generating solutions, the final\nsolution is the most frequent one across generations;(II)\nTree search viaREBASE:We use theREBASE process\nreward model, which is initialized from LLaMA-34B and\nfurther finetuned on a synthetic process reward modeling\ndataset (Wu et al., 2024b). We then aggregate the solutions\ngenerated byREBASE via majority voting. As shown in\nFigure7,augmentingourmodelwith REBASE scalesbetter\nthan majority voting, and even sequential scaling in this sce-\nnario. However,REBASE requires an additional forward\npassateachstepfortherewardmodeladdingsomecomputa-\ntionoverhead. Forsequentialscaling,whenpromptedtouse\nup to 512 steps, for 12 out of the 30 evaluation questions the\nmodelgeneratesaresponsethatexceedsthecontextwindow\nleading to a large performance drop. Overall, we find that\nthese parallel scaling methods complement sequential scal-\ning thus they offer an avenue for scaling test-time compute\neven further; beyond fixed context windows.\n9s1: Simple test-time scaling\nImpact Statement\nLanguage models with strong reasoning capabilities have\nthe potential to greatly enhance human productivity, from\nassisting in complex decision-making to driving scientific\nbreakthroughs. However,recentadvancesinreasoning,such\nas OpenAIs o1 and DeepSeeks r1, lack transparency, limit-\ning broader research progress. Our work aims to push the\nfrontier of reasoning in a fully open manner, fostering in-\nnovation and collaboration to accelerate advancements that\nultimately benefit society.\nAcknowledgements\nWethankRyanMartenforgeneratingtracesfromDeepSeek\nr1 for s1.1 using Bespoke Curator (Marten et al., 2025).\nThisworkwaspartlyconductedusingtheStanfordMarlowe\nGPUcluster(Kapferetal.,2025),madepossiblebyfinancial\nsupport from Stanford University. We thank Alexander M.\nRush, Andrew Ilyas, Banghua Zhu, Chenglei Si, Chunting\nZhou, John Yang, Ludwig Schmidt, Samy Jelassi, Suhas\nKotha,TengyuMa,XuechenLi,YuSun,andYueZhangfor\nvery constructive discussions.\nReferences\nAnkner, Z., Paul, M., Cui, B., Chang, J. D., and Am-\nmanabrolu, P. Critique-out-loud reward models, 2024.\nURL https://arxiv.org/abs/2408.11791.\nArora, D., Singh, H. G., and Mausam. Have llms advanced\nenough? a challenging problem solving benchmark for\nlarge language models, 2023. URLhttps://arxiv.\norg/abs/2305.15074.\nAzerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D.,\nMcAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and\nWell', 'llms advanced\nenough? a challenging problem solving benchmark for\nlarge language models, 2023. URLhttps://arxiv.\norg/abs/2305.15074.\nAzerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D.,\nMcAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and\nWelleck, S. Llemma: An open language model for math-\nematics, 2023.\nBi, Z., Zhang, N., Jiang, Y., Deng, S., Zheng, G., and\nChen, H. When do program-of-thoughts work for reason-\ning?,2023. URLhttps://arxiv.org/abs/2308.\n15452.\nBiderman, S., Schoelkopf, H., Sutawika, L., Gao, L., Tow,\nJ., Abbasi, B., Aji, A. F., Ammanamanchi, P. S., Black,\nS., Clive, J., DiPofi, A., Etxaniz, J., Fattori, B., Forde,\nJ. Z., Foster, C., Hsu, J., Jaiswal, M., Lee, W. Y., Li, H.,\nLovering, C., Muennighoff, N., Pavlick, E., Phang, J.,\nSkowron, A., Tan, S., Tang, X., Wang, K. A., Winata,\nG. I., Yvon, F., and Zou, A. Lessons from the trenches on\nreproducible evaluation of language models, 2024.\nBrown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., R,\nC., and Mirhoseini, A. Large language monkeys: Scaling\ninference compute with repeated sampling, 2024. URL\nhttps://arxiv.org/abs/2407.21787.\nCesista, F.L. Multimodalstructuredgeneration: Cvprs2nd\nmmfm challenge technical report, 2024. URLhttps:\n//arxiv.org/abs/2406.11403.\nChen, W., Yin, M., Ku, M., Lu, P., Wan, Y., Ma, X.,\nXu, J., Wang, X., and Xia, T. Theoremqa: A theorem-\ndriven question answering dataset, 2023. URLhttps:\n//arxiv.org/abs/2305.12524.\nChoi,S.,Fang,T.,Wang,Z.,andSong,Y. Kcts: Knowledge-\nconstrained tree search decoding with token-level halluci-\nnation detection, 2023. URLhttps://arxiv.org/\nabs/2310.09044.\nDeepSeek-AI,Guo,D.,Yang,D.,Zhang,H.,Song,J.,Zhang,\nR., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X.,\nYu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao,\nZ., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu,\nC., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D.,\nChen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao,\nG., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang,\nH., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J.,\nLi, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai,\nJ. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao,\nK., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L.,\nZhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang,\nM., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian,\nN., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q.,\nGe, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin,\nR. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang,\nS., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu,\nS., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W.,\nZhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang,\nW., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X.,\nNie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X.,\nLi, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen,\nX., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X.,\nShan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y.,\nXu,Y., Li,Y., Zhao,Y., Sun,Y., Wang, Y.,Yu, Y.,Zhang,\nY., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan,\nY., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong,\nY., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y.,\nZhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng,\nY., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z.,\nRen, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao,\nZ., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li,\nZ., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang,\nZ., and Zhang, Z. Deepseek-r1: Incentivizing reasoning\ncapability in llms via reinforcement learning, 2025. URL\nhttps://arxiv.org/abs/2501.12948.\nDiao, S., Wang, P., Lin, Y., Pan, R., Liu, X., and Zhang,\nT. Active prompting with chain-of-thought for large lan-\n10s1: Simple test-time scaling\nguage models, 2024. URL https://arxiv.org/\nabs/2302.12246.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A.,\nLetman, A., Mathur, A., Schelten, A., Yang, A., Fan, A.,\nGoyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravanku-\nmar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A.,\nRodriguez, A., Gregerson, A., et al. The llama 3 herd\nof models, 2024. URLhttps://arxiv.org/abs/\n2407.21783.\nFu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot,\nT. Complexity-based prompting for multi-step reason-\ning, 2023. URLhttps://arxiv.org/abs/2210.\n00720.\nGandhi,K.,Lee,D.,Grand,G.,Liu,M.,Cheng,W.,Sharma,\nA.,andGoodman,N.D. Streamofsearch(sos): Learning\nto search in language, 2024. URLhttps://arxiv.\norg/abs/2404.03683.\nGao, B., Song, F., Yang, Z., Cai, Z., Miao, Y., Dong, Q., Li,\nL., Ma, C., Chen, L., Xu, R., Tang, Z., Wang, B., Zan, D.,\nQuan, S., Zhang, G., Sha, L., Zhang, Y., Ren, X., Liu, T.,\nand Chang, B. Omni-math: A universal olympiad level\nmathematic benchmark for large language models, 2024a.\nURL https://arxiv.org/abs/2410.07985.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,\nC., Golding, L., Hsu, J., McDonell, K., Muennighoff, N.,\nPhang, J., Reynolds, L., Tang, E., Thite, A., Wang, B.,\n', 'k for large language models, 2024a.\nURL https://arxiv.org/abs/2410.07985.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,\nC., Golding, L., Hsu, J., McDonell, K., Muennighoff, N.,\nPhang, J., Reynolds, L., Tang, E., Thite, A., Wang, B.,\nWang,K.,andZou,A.Aframeworkforfew-shotlanguage\nmodel evaluation, September 2021. URLhttps://\ndoi.org/10.5281/zenodo.5371628.\nGao,Z.,Niu,B.,He,X.,Xu,H.,Liu,H.,Liu,A.,Hu,X.,and\nWen, L. Interpretable contrastive monte carlo tree search\nreasoning,2024b. URLhttps://arxiv.org/abs/\n2410.01707.\nGlazer, E., Erdil, E., Besiroglu, T., Chicharro, D., Chen,\nE., Gunning, A., Olsson, C. F., Denain, J.-S., Ho, A.,\nde Oliveira Santos, E., Jrviniemi, O., Barnett, M., San-\ndler, R., Vrzala, M., Sevilla, J., Ren, Q., Pratt, E., Levine,\nL., Barkley, G., Stewart, N., Grechuk, B., Grechuk, T.,\nEnugandla,S.V.,andWildon,M. Frontiermath: Abench-\nmark for evaluating advanced mathematical reasoning in\nai, 2024. URL https://arxiv.org/abs/2411.\n04872.\nGoogle. Gemini 2.0 flash thinking mode (gemini-2.0-\nflash-thinking-exp-1219), December 2024. URL\nhttps://cloud.google.com/vertex-ai/\ngenerative-ai/docs/thinking-mode.\nGroeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney,\nR., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I.,\nWang, Y., Arora, S., Atkinson, D., Authur, R., Chandu,\nK.R., Cohan,A., Dumas,J., Elazar, Y., Gu,Y., Hessel,J.,\nKhot,T.,Merrill,W.,Morrison,J.,Muennighoff,N.,Naik,\nA., Nam, C., Peters, M. E., Pyatkin, V., Ravichander, A.,\nSchwenk,D.,Shah,S.,Smith,W.,Strubell,E.,Subramani,\nN., Wortsman, M., Dasigi, P., Lambert, N., Richardson,\nK.,Zettlemoyer,L.,Dodge,J.,Lo,K.,Soldaini,L.,Smith,\nN. A., and Hajishirzi, H. Olmo: Accelerating the science\nof language models, 2024.\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J.,\nHu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L.,\nLiu, Z., and Sun, M. Olympiadbench: A challeng-\ning benchmark for promoting agi with olympiad-level\nbilingual multimodal scientific problems, 2024. URL\nhttps://arxiv.org/abs/2402.14008.\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS.,Tang,E.,Song,D.,andSteinhardt,J. Measuringmath-\nematical problem solving with the math dataset, 2021.\nURL https://arxiv.org/abs/2103.03874.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai,T.,Rutherford,E.,deLasCasas,D.,Hendricks,L.A.,\nWelbl, J., Clark, A., Hennigan, T., Noland, E., Millican,\nK., van den Driessche, G., Damoc, B., Guy, A., Osindero,\nS., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and\nSifre, L. Training compute-optimal large language mod-\nels, 2022. URLhttps://arxiv.org/abs/2203.\n15556.\nHou, Z., Lv, X., Lu, R., Zhang, J., Li, Y., Yao, Z., Li, J.,\nTang, J., and Dong, Y. Advancing language model rea-\nsoning through reinforcement learning and inference scal-\ning, 2025. URLhttps://arxiv.org/abs/2501.\n11651.\nHu, Y., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettle-\nmoyer, L., Smith, N. A., and Krishna, R. Visual sketch-\npad: Sketchingasavisualchainofthoughtformultimodal\nlanguage models, 2024. URLhttps://arxiv.org/\nabs/2406.09403.\nHuang, Z., Wang, Z., Xia, S., Li, X., Zou, H., Xu, R., Fan,\nR.-Z., Ye, L., Chern, E., Ye, Y., Zhang, Y., Yang, Y., Wu,\nT.,Wang,B.,Sun,S.,Xiao,Y.,Li,Y.,Zhou,F.,Chern,S.,\nQin, Y., Ma, Y., Su, J., Liu, Y., Zheng, Y., Zhang, S., Lin,\nD., Qiao, Y., and Liu, P. Olympicarena: Benchmarking\nmulti-discipline cognitive reasoning for superintelligent\nai, 2024a. URLhttps://arxiv.org/abs/2406.\n12753.\nHuang, Z., Zou, H., Li, X., Liu, Y., Zheng, Y., Chern, E.,\nXia, S., Qin, Y., Yuan, W., and Liu, P. O1 replication\njourney  part 2: Surpassing o1-preview through simple\ndistillation, big progress or bitter lesson?, 2024b. URL\nhttps://arxiv.org/abs/2411.16489.\n11s1: Simple test-time scaling\nHuang, Z., Geng, G., Hua, S., Huang, Z., Zou, H., Zhang,\nS., Liu, P., and Zhang, X. O1 replication journey  part 3:\nInference-time scaling for medical reasoning, 2025. URL\nhttps://arxiv.org/abs/2501.06458.\nIrvine, R., Boubert, D., Raina, V., Liusie, A., Zhu, Z.,\nMudupalli, V., Korshuk, A., Liu, Z., Cremer, F., As-\nsassi, V., Beauchamp, C.-C., Lu, X., Rialan, T., and\nBeauchamp, W. Rewarding chatbots for real-world en-\ngagement with millions of users, 2023. URLhttps:\n//arxiv.org/abs/2303.06135.\nJain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang,\nT., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I.\nLivecodebench: Holistic and contamination free evalu-\nation of large language models for code, 2024. URL\nhttps://arxiv.org/abs/2403.07974.\nKapfer, C., Stine, K., Narasimhan, B., Mentzel, C., and\nCandes,E. Marlowe: Stanfordsgpu-basedcomputational\ninstrument, January 2025. URLhttps://doi.org/\n10.5281/zenodo.14751899.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J.,\nand Amodei, D. Scaling laws for neural language mod-\nels, 2020. URLhttps://arxiv.org/abs/2001.\n08361.\nKim,E.,Suk,J.,Kim,S.,Muennighoff,N.,Kim,D.,andOh,\nA. Llm-as-an-interviewer: Beyond static testing through\ndynamic llm ev', 'y, S., Radford, A., Wu, J.,\nand Amodei, D. Scaling laws for neural language mod-\nels, 2020. URLhttps://arxiv.org/abs/2001.\n08361.\nKim,E.,Suk,J.,Kim,S.,Muennighoff,N.,Kim,D.,andOh,\nA. Llm-as-an-interviewer: Beyond static testing through\ndynamic llm evaluation, 2024. URLhttps://arxiv.\norg/abs/2412.10424.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Effi-\ncientmemorymanagementforlargelanguagemodelserv-\ning with pagedattention, 2023. URLhttps://arxiv.\norg/abs/2309.06180.\nLabs, B. Bespoke-stratos: The unreasonable effectiveness\nof reasoning distillation, 2025. URL https://hf.\nco/bespokelabs/Bespoke-Stratos-32B. Ac-\ncessed: 2025-01-22.\nLee, K.-H., Fischer, I., Wu, Y.-H., Marwood, D., Baluja,\nS., Schuurmans, D., and Chen, X. Evolving deeper llm\nthinking, 2025. URL https://arxiv.org/abs/\n2501.09891.\nLevi, N. A simple model of inference scaling laws, 2024.\nURL https://arxiv.org/abs/2410.16377.\nLI, J., Beeching, E., Tunstall, L., Lipkin, B., Solet-\nskyi, R., Huang, S. C., Rasul, K., Yu, L., Jiang, A.,\nShen, Z., Qin, Z., Dong, B., Zhou, L., Fleureau, Y.,\nLample, G., and Polu, S. Numinamath, 2024. URL\nhttps://github.com/project-numina/\naimo-progress-prize/blob/main/report/\nnumina_dataset.pdf.\nLightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker,\nB., Lee, T., Leike, J., Schulman, J., Sutskever, I., and\nCobbe, K. Lets verify step by step, 2023. URLhttps:\n//arxiv.org/abs/2305.20050.\nLing,W.,Yogatama,D.,Dyer,C.,andBlunsom,P. Program\ninduction by rationale generation : Learning to solve and\nexplain algebraic word problems, 2017. URLhttps:\n//arxiv.org/abs/1705.04146.\nLiu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang,\nY. Logiqa: A challenge dataset for machine reading com-\nprehension with logical reasoning, 2020. URLhttps:\n//arxiv.org/abs/2007.08124.\nLiu, J., Cohen, A., Pasunuru, R., Choi, Y., Hajishirzi,\nH., and Celikyilmaz, A. Dont throw away your value\nmodel! generatingmorepreferabletextwithvalue-guided\nmonte-carlo tree search decoding, 2024. URLhttps:\n//arxiv.org/abs/2309.15028.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization, 2019.\nLuo,H.,Sun,Q.,Xu,C.,Zhao,P.,Lou,J.,Tao,C.,Geng,X.,\nLin, Q., Chen, S., Tang, Y., and Zhang, D. Wizardmath:\nEmpowering mathematical reasoning for large language\nmodels via reinforced evol-instruct, 2025. URLhttps:\n//arxiv.org/abs/2308.09583.\nMarten, R., Vu, T., Ji, C. C.-J., Sharma, K., Pimpalgaonkar,\nS., Dimakis, A., and Sathiamoorthy, M. Curator: A tool\nfor synthetic data creation.https://github.com/\nbespokelabsai/curator, January 2025.\nMuennighoff, N., Soldaini, L., Groeneveld, D., Lo, K., Mor-\nrison,J.,Min,S.,Shi,W.,Walsh,P.,Tafjord,O.,Lambert,\nN., Gu, Y., Arora, S., Bhagia, A., Schwenk, D., Wadden,\nD., Wettig, A., Hui, B., Dettmers, T., Kiela, D., Farhadi,\nA., Smith, N. A., Koh, P. W., Singh, A., and Hajishirzi, H.\nOlmoe: Open mixture-of-experts language models, 2024.\nURL https://arxiv.org/abs/2409.02060.\nof America, M. A. Aime, February 2024. URLhttps:\n//artofproblemsolving.com/wiki/index.\nphp/AIME_Problems_and_Solutions/.\nOpenAI. Learning to reason with llms, Septem-\nber 2024. URL https://openai.com/index/\nlearning-to-reason-with-llms/ .\nOpenAI. Openaio3-mini,2025. URLhttps://openai.\ncom/index/openai-o3-mini/. Accessed: 2025-\n02-24.\n12s1: Simple test-time scaling\nPhan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Shi,\nS., Choi, M., Agrawal, A., Chopra, A., et al. Humanitys\nlast exam, 2025. URLhttps://arxiv.org/abs/\n2501.14249.\nQin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye,\nY., Yuan, W., Liu, H., Li, Y., and Liu, P. O1 replication\njourney: A strategic progress report  part 1, 2024. URL\nhttps://arxiv.org/abs/2410.18982.\nQwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng,\nB., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H.,\nYang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J.,\nLin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li,\nM., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li,\nT., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y.,\nWan,Y.,Liu,Y.,Cui,Z.,Zhang,Z.,andQiu,Z. Qwen2.5\ntechnical report, 2024. URLhttps://arxiv.org/\nabs/2412.15115.\nRein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y.,\nDirani, J., Michael, J., and Bowman, S. R. Gpqa: A\ngraduate-level google-proof q&a benchmark, 2023. URL\nhttps://arxiv.org/abs/2311.12022.\nShi, Q., Tang, M., Narasimhan, K., and Yao, S. Can lan-\nguagemodels solveolympiad programming?, 2024. URL\nhttps://arxiv.org/abs/2404.10952.\nSnell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-\ntime compute optimally can be more effective than scal-\ning model parameters, 2024. URLhttps://arxiv.\norg/abs/2408.03314.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., et al. Beyond the imitation game:\nQuantifyingandextrapolatingthecapabilitiesoflanguage\nmodels, 2023.\nSu,H.,Yen,H.,Xia,M.,Shi,W.,Muennighoff,N.,yuWang,\nH.,Liu,H.,Shi,Q', ' M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., et al. Beyond the imitation game:\nQuantifyingandextrapolatingthecapabilitiesoflanguage\nmodels, 2023.\nSu,H.,Yen,H.,Xia,M.,Shi,W.,Muennighoff,N.,yuWang,\nH.,Liu,H.,Shi,Q.,Siegel,Z.S.,Tang,M.,Sun,R.,Yoon,\nJ., Arik, S. O., Chen, D., and Yu, T. Bright: A realis-\ntic and challenging benchmark for reasoning-intensive\nretrieval, 2024. URL https://arxiv.org/abs/\n2407.12883.\nSun,L.,Han,Y.,Zhao,Z.,Ma,D.,Shen,Z.,Chen,B.,Chen,\nL., and Yu, K. Scieval: A multi-level large language\nmodel evaluation benchmark for scientific research, 2024.\nURL https://arxiv.org/abs/2308.13149.\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li,\nC., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang,\nD., Yuan, E., Lu, E., Tang, F., Sung, F., Wei, G., Lai, G.,\nGuo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H.,\nYao, H., Zhao, H., Lu, H., Li, H., Yu, H., Gao, H., Zheng,\nH., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J.,\nZhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S.,\nMa, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W.,\nXiong, W., He, W., Huang, W., Wu, W., He, W., Wei, X.,\nJia,X.,Wu,X.,Xu,X.,Zu,X.,Zhou,X.,Pan,X.,Charles,\nY.,Li,Y.,Hu,Y.,Liu,Y.,Chen,Y.,Wang,Y.,Liu,Y.,Qin,\nY., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y.,\nZhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang, Z., Wang, Z.,\nYang,Z.,Huang,Z.,Huang,Z.,Xu,Z.,andYang,Z. Kimi\nk1.5: Scaling reinforcement learning with llms, 2025.\nURL https://arxiv.org/abs/2501.12599.\nTeam, N. Sky-t1: Fully open-source reasoning\nmodel with o1-preview performance in $450 budget,\n2025. URL https://novasky-ai.github.io/\nposts/sky-t1. Accessed: 2025-01-09.\nTeam, Q. Qwq: Reflect deeply on the boundaries of the\nunknown, November 2024. URLhttps://qwenlm.\ngithub.io/blog/qwq-32b-preview/.\nWang, J., Meng, F., Liang, Y., and Zhou, J. Drt-o1: Op-\ntimized deep reasoning translation via long chain-of-\nthought, 2024a. URLhttps://arxiv.org/abs/\n2412.17498.\nWang, P., Li, L., Shao, Z., Xu, R. X., Dai, D., Li, Y., Chen,\nD., Wu, Y., and Sui, Z. Math-shepherd: Verify and rein-\nforcellmsstep-by-stepwithouthumanannotations,2024b.\nURL https://arxiv.org/abs/2312.08935.\nWang, S., Liu, Z., Zhong, W., Zhou, M., Wei, Z., Chen, Z.,\nand Duan, N. From lsat: The progress and challenges\nof complex reasoning, 2021. URLhttps://arxiv.\norg/abs/2108.00648.\nWang, Z., Dong, Y., Delalleau, O., Zeng, J., Shen, G., Egert,\nD., Zhang, J. J., Sreedhar, M. N., and Kuchaiev, O. Help-\nsteer2: Open-source dataset for training top-performing\nreward models, 2024c. URLhttps://arxiv.org/\nabs/2406.08673.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter,\nB., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-\nthought prompting elicits reasoning in large language\nmodels, 2023. URL https://arxiv.org/abs/\n2201.11903.\nWelleck,S.,Bertsch,A.,Finlayson,M.,Schoelkopf,H.,Xie,\nA., Neubig, G., Kulikov, I., and Harchaoui, Z. From de-\ncoding to meta-generation: Inference-time algorithms for\nlarge language models, 2024. URLhttps://arxiv.\norg/abs/2406.16838.\n13s1: Simple test-time scaling\nWu, T., Lan, J., Yuan, W., Jiao, J., Weston, J., and\nSukhbaatar, S. Thinking llms: General instruction fol-\nlowing with thought generation, 2024a. URLhttps:\n//arxiv.org/abs/2410.10630.\nWu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y. Infer-\nence scaling laws: An empirical analysis of compute-\noptimalinferenceforproblem-solvingwithlanguagemod-\nels,2024b. URLhttps://arxiv.org/abs/2408.\n00724.\nXiang, V., Snell, C., Gandhi, K., Albalak, A., Singh, A.,\nBlagden, C., Phung, D., Rafailov, R., Lile, N., Mahan,\nD., Castricato, L., Franken, J.-P., Haber, N., and Finn, C.\nTowards system 2 reasoning in llms: Learning how to\nthink with meta chain-of-thought, 2025. URLhttps:\n//arxiv.org/abs/2501.04682.\nXie, Y., Kawaguchi, K., Zhao, Y., Zhao, X., Kan, M.-Y., He,\nJ., and Xie, Q. Self-evaluation guided beam search for\nreasoning, 2023. URLhttps://arxiv.org/abs/\n2305.00633.\nXin, H., Guo, D., Shao, Z., Ren, Z., Zhu, Q., Liu, B., Ruan,\nC., Li, W., and Liang, X. Deepseek-prover: Advancing\ntheorem proving in llms through large-scale synthetic\ndata,2024. URLhttps://arxiv.org/abs/2405.\n14333.\nXu, H., Wu, X., Wang, W., Li, Z., Zheng, D., Chen, B.,\nHu, Y., Kang, S., Ji, J., Zhang, Y., Guo, Z., Yang, Y.,\nZhang,M.,andZhang,D. Redstar: Doesscalinglong-cot\ndata unlock better slow-reasoning systems?, 2025. URL\nhttps://arxiv.org/abs/2501.11284.\nYang, Z., Band, N., Li, S., Cands, E., and Hashimoto, T.\nSynthetic continued pretraining, 2024. URLhttps:\n//arxiv.org/abs/2409.07431.\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao,\nY., and Narasimhan, K. Tree of thoughts: Deliberate\nproblemsolvingwithlargelanguagemodels,2023a. URL\nhttps://arxiv.org/abs/2305.10601.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK., and Cao, Y. React: Synergizing reasoning and acting\nin language models, 2023b. URLhttps://arxiv.\norg/abs/2210', 'msolvingwithlargelanguagemodels,2023a. URL\nhttps://arxiv.org/abs/2305.10601.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK., and Cao, Y. React: Synergizing reasoning and acting\nin language models, 2023b. URLhttps://arxiv.\norg/abs/2210.03629.\nYe, Y., Huang, Z., Xiao, Y., Chern, E., Xia, S., and Liu, P.\nLimo: Less is more for reasoning, 2025a. URLhttps:\n//arxiv.org/abs/2502.03387.\nYe, Y., Xiao, Y., Mi, T., and Liu, P. Aime-preview: A\nrigorous and immediate evaluation framework for ad-\nvanced mathematical reasoning. https://github.\ncom/GAIR-NLP/AIME-Preview, 2025b. GitHub\nrepository.\nYu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok,\nJ.T.,Li,Z.,Weller,A.,andLiu,W. Metamath: Bootstrap\nyour own mathematical questions for large language mod-\nels, 2024. URLhttps://arxiv.org/abs/2309.\n12284.\nYuan, S., Chen, Z., Xi, Z., Ye, J., Du, Z., and Chen, J.\nAgent-r: Training language model agents to reflect via\niterative self-training, 2025. URLhttps://arxiv.\norg/abs/2501.11425.\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. D. Star:\nBootstrapping reasoning with reasoning, 2022. URL\nhttps://arxiv.org/abs/2203.14465.\nZelikman, E., Harik, G., Shao, Y., Jayasiri, V., Haber, N.,\nand Goodman, N. D. Quiet-star: Language models can\nteach themselves to think before speaking, 2024. URL\nhttps://arxiv.org/abs/2403.09629.\nZhang, H. and Chen, C. Test-time compute scaling laws,\n2024. URLhttps://github.com/hughbzhang/\no1_inference_scaling_laws.\nZhang, S., Chen, Z., Shen, Y., Ding, M., Tenenbaum, J. B.,\nand Gan, C. Planning with large language models for\ncode generation, 2023. URLhttps://arxiv.org/\nabs/2303.05510.\nZhang, Y., Wu, S., Yang, Y., Shu, J., Xiao, J., Kong, C., and\nSang, J. o1-coder: an o1 replication for coding, 2024a.\nURL https://arxiv.org/abs/2412.00154.\nZhang, Y., Yang, J., Yuan, Y., and Yao, A. C.-C. Cumula-\ntive reasoning with large language models, 2024b. URL\nhttps://arxiv.org/abs/2308.04371.\nZhong, H., Xiao, C., Tu, C., Zhang, T., Liu, Z., and Sun, M.\nJec-qa: A legal-domain question answering dataset, 2019.\nURL https://arxiv.org/abs/1911.12011.\nZhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang,\nY., Saied, A., Chen, W., and Duan, N. Agieval: A\nhuman-centric benchmark for evaluating foundation mod-\nels, 2023. URLhttps://arxiv.org/abs/2304.\n06364.\nZhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H.,\nand Wang, Y.-X. Language agent tree search unifies rea-\nsoning acting and planning in language models, 2024.\nURL https://arxiv.org/abs/2310.04406.\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M.,\nZettlemoyer, L., and Levy, O. Lima: Less is more for\nalignment, 2023. URLhttps://arxiv.org/abs/\n2305.11206.\n14s1: Simple test-time scaling\nContents\n1 Introduction 1\n2 Reasoning data curation to create s1K 2\n2.1 Initial collection of 59K samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n2.2 Final selection of 1K samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n3 Test-time scaling 3\n3.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n3.2 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n4 Results 4\n4.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n4.2 Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n5 Ablations 6\n5.1 Data Quantity, Diversity, and Difficulty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n5.2 Test-time scaling methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n6 Discussion and related work 8\n6.1 Sample-efficient reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n6.2 Test-time scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\nA s1.1 16\nB Evaluation determinism 16\nC s1K details 17\nC.1 s1K summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nC.2 Dataset composition for full 59K questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nC.3 s1K grading prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC.4 s1K diversity selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC.5 Decontamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nD Training details 21\nD.1 Training Ablations: Sequence length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nD.2 Training Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nE Test-time sca', 'D.1 Training Ablations: Sequence length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nD.2 Training Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nE Test-time scaling details 35\nE.1 Sequential scaling ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\nE.2 Examples for rejection sampling ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nF Version Control 46\n15s1: Simple test-time scaling\nA. s1.1\nSeven days after our release of s1, we released s1.1. We regenerated traces for our 1,000 samples ins1Kusing DeepSeek\nr1 (DeepSeek-AI et al., 2025) to creates1K-1.1. We use the same training procedure to train our models1.1. Other updates\nsince our launch include the release of o3 (OpenAI, 2025), LIMO (Ye et al., 2025a), and AIME 2025. We consider all these\nnew developments in Table 5. We find that s1.1 performs significantly better than s1. We also tried distilling from Claude\n3.7, which led to worse performance than from r1 (not reported).\nTable 5.s1-32B is an open and sample-efficient reasoning model.We evaluates1-32B, Qwen, and Gemini (some entries are unknown\n(N.A.), see 4). Other results are from the respective reports (Qwen et al., 2024; Team, 2024; OpenAI, 2024; 2025; DeepSeek-AI et al.,\n2025; Labs, 2025; Team, 2025) except for AIME 2025 (Ye et al., 2025b). # ex. = number examples used for reasoning finetuning; BF =\nbudget forcing.\nModel # Examples MATH500 GPQA AIME 2024 AIME 2025\nAPI only\no3-mini-low N/A 95.8 70.6 56.3 42.1\no3-mini-medium N/A 97.3 76.8 75.8 70.4\no3-mini-high N/A 97.9 79.7 83.8 80.9\nOpen Weights\nQwQ-32B N.A. 90.6 54.5 46.7 32.7\nr1 800K 07.3 71.5 79.8 70.0\nr1-distill-Llama-70B 800K 94.5 65.2 57.1 56.3\nr1-distill-Qwen-14B 800K 93.9 59.1 61.7 48.0\nr1-distill-Qwen-32B 800K 94.3 62.1 58.3 49.6\nOpen Weights and Open Data\nLIMO 817 94.8 66.7 56.3 44.6\ns1 w/o BF 1K 92.6 56.6 50.0 26.7\ns1 with Budget Forcing Wait 1x 1K 92.8 59.6 53.3 30.0\ns1 with Budget Forcing Wait 2x 1K 93.0 59.6 53.3 33.3\ns1 with Budget Forcing Wait 4x 1K 92.2 58.6 56.7 36.7\ns1.1 w/o BF 1K 94.4 60.6 56.7 50.0\ns1.1 with Budget Forcing Wait 1x 1K 95.4 62.6 56.7 50.0\ns1.1 with Budget Forcing Wait 2x 1K 95.4 63.6 56.7 50.0\nB. Evaluation determinism\nWe run our evaluations using vLLM (Kwon et al., 2023) as it is faster than the alternatives we tried. However, we find that\neven when using the same random seeds and greedy sampling, evaluation scores can change significantly across runs:\n Different batch sizes causing different results seehttps://github.com/vllm-project/vllm/issues/\n5898\n Continuing generations causing different results seehttps://github.com/vllm-project/vllm/issues/\n11783\n Changes in tensor parallelism causing different results\nAs our model generates long reasoning traces prior to its answer, small numeric changes can snowball into large differences.\nWe encounter many generations that are exactly the same for thousands of tokens and then suddenly differ in one token\neventually ending up with an entirely different answer. To partly counter this issue we generally run our final evaluations\nusing full precision unless otherwise indicated.\n16s1: Simple test-time scaling\nC. s1K details\nC.1. s1K summary\nTable 6.Summary of our dataset s1K. Token count measured by the Qwen-2.5 tokenizer. We prompt Claude to produce keywords given\nseveral questions from the domain.\nDomain #questions Total token count Keywords\nGeometry 109 560.2K Area, Triangle, Distance\nNumber theory 98 522.5K Sequences, Divisibility\nCombinatorics 75 384.7K Permutations, Counting\nReal functions 43 234.8K Trigonometry, Calculus\nBiology 41 120.9K Organic reactions\nComplex functions 32 170.2K Complex roots\nQuantum theory 32 127.9K Particles, Wave functions\nField theory 28 150.1K Polynomials, Roots\nCalculus of variations 28 155.5K Optimization, Control\nDifference equations 24 132.5K Recurrence, Recursion\nElectromagnetic theory 23 95.8K Optics, Waves, Diffraction\nGroup theory 22 100.0K Groups, Automorphisms\nLinear algebra 22 128.3K Matrices, Determinants\nProbability theory 20 114.6K Random walk, Expectation\nAlgebraic systems 19 109.9K Functional equations\nMechanics 19 103.6K Forces, Motion, Energy\nThermodynamics 19 74.2K Heat engines, Entropy\nDifferential equations 18 89.6K Substitution, Existence\nComputer science 18 34.2K Complexity theory, Algorithms\nNumerical analysis 18 76.5K Error analysis, Stability\nCalculus 17 96.3K Convergence, Summation\nAlgebraic structures 17 90.4K Inequalities, Sets\nAstronomy 16 37.7K Stellar populations, Orbits\nRemaining 27 domains 242 982.2K Domains with  16 questions\nAll domains (51) 1000 4.7M s1K\nC.2. Dataset composition for full 59K questions\n17s1: Simple test-time scaling\nTable 7.Composition of full 59K questions.Thinking and response lengths are measured in tokens using the Qwen2.5-32B-Instruct\ntokenizer (Qwen et al., 2024). In addition to excluding our eval', 'ition for full 59K questions\n17s1: Simple test-time scaling\nTable 7.Composition of full 59K questions.Thinking and response lengths are measured in tokens using the Qwen2.5-32B-Instruct\ntokenizer (Qwen et al., 2024). In addition to excluding our evaluation benchmark, AIME24, we also exclude AIME questions from\n2022-2023 as we use these 90 questions during our development stage ofs1-32B.\nSource Description #Samples Avg.\nthinking\nlength\nNuminaMATH (LI et al., 2024) Math problems from online websites 30660 4.1K\nMATH (Hendrycks et al., 2021) Math problems from competitions 11999 2.9K\nOlympicArena (Huang et al.,\n2024a)\nAstronomy, Biology, Chemistry, Com-\nputer Science, Geography, Math, and\nPhysics olympiad questions\n4250 3.2K\nOmniMath (Gao et al., 2024a) Math problems from competitions 4238 4.4K\nAGIEval (Zhong et al., 2023;\nLingetal.,2017;Hendrycksetal.,\n2021; Liu et al., 2020; Zhong\net al., 2019; Wang et al., 2021)\nEnglish, Law, Logic and Math prob-\nlems from the SAT, LSAT and other\nexams\n2385 1.2K\nxword Crossword puzzles 999 0.7K\nOlympiadBench (He et al., 2024) Math and Physics olympiad questions 896 3.9K\nAIME (1983-2021) AmericanInvitationalMathematicsEx-\namination\n890 4.7K\nTheoremQA (Chen et al., 2023) Computer Science, Finance, Math, and\nPhysics university-level questions relat-\ning to theorems\n747 2.1K\nUSACO (Shi et al., 2024) Code problems from the USA Comput-\ning Olympiad\n519 3.6K\nJEEBench (Arora et al., 2023) Chemistry,Math,andPhysicsproblems\nused in the university entrance exami-\nnation of the Indian Institute of Tech-\nnology\n515 2.9K\nGPQA (Rein et al., 2023) PhD-Level Science Questions 348 2.9K\nSciEval (Sun et al., 2024) Biology, Chemistry, and Physics prob-\nlems from various sources\n227 0.7K\ns1-prob Stanford statistics qualifying exams 182 4.0K\nLiveCodeBench (Jain et al., 2024)Code problems from coding websites\n(LeetCode, AtCoder, and CodeForces)\n151 3.5K\ns1-teasers Math brain-teasers crawled from the In-\nternet\n23 4.1K\nAll 59K questions Composite of the above datasets with\nreasoning traces and solutions\n59029 3.6K\n18s1: Simple test-time scaling\nC.3. s1K grading prompt\nTogradewhetheranexampleiscorrectforourdatasetselectionin2,weusethepromptinFigure8. WegradeusingClaude\n3.5 except for the correctness among the final 1,000 samples, which we graded with Claude 3.7.\nYou are an AI assistant for grading a science problem. The user will provide you with the question itself, an attempt made by\na student and the correct answer to the problem. Your job is to judge whether the attempt is correct by comparing it with the\ncorrect answer. If the expected solution concludes with a number or choice, there should be no ambiguity. If the expected\nsolution involves going through the entire reasoning process, you should judge the attempt based on whether the reasoning\nprocess is correct with correct answer if helpful.\nThe user will provide the attempt and the correct answer in the following format:\n# Problem\n{problem}\n## Attempt\n{attempt}\n## Correct answer\n{solution}\nExplain your reasoning, and end your response on a new line with only "Yes" or "No" (without quotes).\nFigure 8.Grading prompt.\nC.4. s1K diversity selection\nAlgorithm 1 provides our algorithm for selecting data in our diversity selection stage. As mentioned in 2, we also include\nsamples from some specific benchmarks we perceive as high-quality. None of the samples overlap with our final evaluation.\nC.5. Decontamination\nWe filter all samples by checking for an 8-gram overlap between the selected examples and the evaluation benchmarks:\nMATH500, GPTQA Diamond, and AIME24. We exclude questions with more than an 8-gram overlap.\n19s1: Simple test-time scaling\nAlgorithm 1Two-stage sampling fors1K\n1: Input: \ue23d := Set of 24,496 questions with features\n2: Output: \ue23f := Set of 1,000 selected questions\n3: \ue23f   Initialize the output set (only tracks unique elements)\n4: for\ue23d do\n5: if IsGeminiCorrect()and (IsAIME()or IsGPQA()) then\n6: \ue23f  \ue23f {}\n7: Select all correct AIME/GPQA solutions\n8: else ifIsGeminiCorrect()andIsMATH() andThinkingLength() > 5600then\n9: \ue23f  \ue23f {}\n10: Select correct MATH500 solutions with long chains\n11: end if\n12: end for\n13: \ue230  All available domains\n14: Initialize domain pool\n15: while|\ue23f| <1000do\n16:   RandomChoice(\ue230)\n17: Randomly select a domain\n18:   Questions in domain\n19: Get questions from this domain\n20: ranks  RankByThinkingLength()\n21: Rank by thinking length\n22: weights 2ranks\n23: Apply power-law weighting\n24:  WeightedSample(, weights)\n25: Sample favoring longer chains\n26: \ue23f  \ue23f {}\n27: Add selected question\n28:     {}\n29: if  = then\n30: \ue230  \ue230  {}\n31: Remove exhausted domains\n32: end if\n33: end while\n20s1: Simple test-time scaling\nD. Training details\nWe take a model that has already been pretrained and instruction tuned and further finetune it for reasoning. Specifically, we\nuse Qwen2.5-32B-Instruct (Qwen et al., 2024), which on math tasks generally matches or outperforms the larger Qwen2.5-\n72B-Instru'], 'dimensions': 1536}'
[0m

2025-12-08 14:32:37,036 - DEBUG - Using AiohttpTransport...
2025-12-08 14:32:37,036 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:32:37,036 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:32:37,036 - DEBUG - RAW RESPONSE:
The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


2025-12-08 14:32:37,036 - DEBUG - Logging Details: logger_fn - None | callable(logger_fn) - False
2025-12-08 14:32:37,037 - DEBUG - Logging Details LiteLLM-Failure Call: [<bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x120112b10>>, <bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x120ca3290>>]
2025-12-08 14:32:37,038 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:32:37,039 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:32:37,039 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:32:37,039 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:32:37,041 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:32:37,042 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:32:37,042 - ERROR - Error adding /Users/gabriele/Documents/GitHub/research-agent-cli/library/4603a874ea67c1b582ccff73ff5f82f2/2501.19393v3.pdf: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable LiteLLM Retried: 3 times
2025-12-08 14:32:37,043 - DEBUG - Adding PDF: /Users/gabriele/Documents/GitHub/research-agent-cli/library/04351a7a49b36885fcc89a38088d75c4/327-440-1-sm.pdf
2025-12-08 14:32:37,043 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:37,170 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'custom_llm_provider': None}
2025-12-08 14:32:37,170 - DEBUG - model_info: {'key': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': None, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': None, 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:37,170 - DEBUG - added/updated model=8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be in litellm.model_cost: 8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be
2025-12-08 14:32:37,170 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:37,170 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:37,170 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:37,170 - DEBUG - added/updated model=gemini/gemini-2.0-flash-exp in litellm.model_cost: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:37,170 - DEBUG - 
Initialized Model List ['gemini/gemini-2.0-flash-exp']
2025-12-08 14:32:37,170 - INFO - Routing strategy: simple-shuffle
2025-12-08 14:32:37,170 - DEBUG - Inside async function with retries.
2025-12-08 14:32:37,170 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x120c5f890>>, num_retries - 3
2025-12-08 14:32:37,171 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:37,171 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:37,171 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:37,171 - DEBUG - retrieve cooldown models: []
2025-12-08 14:32:37,171 - DEBUG - async cooldown deployments: []
2025-12-08 14:32:37,171 - DEBUG - cooldown_deployments: []
2025-12-08 14:32:37,171 - DEBUG - cooldown deployments: []
2025-12-08 14:32:37,171 - DEBUG - 

2025-12-08 14:32:37,171 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:32:37,171 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\n\xa02 (26) 2024\n87\nURL: https://cyberspace.pgu.ru\n 004.8 + 004.93\nDOI 10.17726/philIT.2024.2.6\n   ,  ?\n . ,\n   \n- , \nalexander.xom@gmail.com\n. \xa0\xa0\xa0\xa0\xa0-\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0-\xa0Google\xa0Gemini\xa0\n\xa0\xa0\xa0,\xa0\xa0\xa0-,\xa0\xa0\xa0\xa0\xa0\n.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0LLM,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0.,\xa0\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0-\n\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\n\xa0\xa0\xa0(inference).\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\n\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0,\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa01950-\xa0\xa0\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0.\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0(\xa0)\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0.\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0.\n :\xa0LLM;\xa0;\xa0;\xa0;\xa0-\n\xa0;\xa0\xa0;\xa0.\nWhy dont transformers think like humans?\nAlexander B. Khomyakov,\nMaster of Physical Sciences \nSaint- Petersburg, Russia\nalexander.xom@gmail.com88\n     \n \nAbstract. Large\xa0language\xa0models\xa0in\xa0the\xa0form\xa0of\xa0chatbots\xa0very\xa0realistical-\nly\xa0imitate\xa0a\xa0dialogue\xa0as\xa0an\xa0omniscient\xa0interlocutor\xa0and\xa0therefore\xa0have\xa0become\xa0\nwidespread.\xa0But\xa0even\xa0Google\xa0in\xa0its\xa0Gemini\xa0chatbot\xa0does\xa0not\xa0recommend\xa0trust-\ning\xa0what\xa0the\xa0chatbot\xa0will\xa0write\xa0and\xa0asks\xa0to\xa0check\xa0its\xa0answers.\xa0In\xa0this\xa0review,\xa0\nvarious\xa0types\xa0of\xa0LLM\xa0errors\xa0such\xa0as\xa0the\xa0curse\xa0of\xa0inversion,\xa0number\xa0processing,\xa0\netc.\xa0will\xa0be\xa0analyzed\xa0to\xa0identify\xa0their\xa0causes.\xa0Such\xa0an\xa0analysis\xa0led\xa0to\xa0the\xa0con-\nclusion\xa0about\xa0the\xa0common\xa0causes\xa0of\xa0all\xa0errors,\xa0which\xa0is\xa0that\xa0transformers\xa0do\xa0\nnot\xa0have\xa0deep\xa0analogy,\xa0hierarchy\xa0of\xa0schemes\xa0and\xa0selectivity\xa0of\xa0content\xa0taken\xa0\ninto\xa0account\xa0in\xa0the\xa0inference.\xa0But\xa0the\xa0most\xa0important\xa0conclusion\xa0is\xa0that\xa0trans-\nformers,\xa0like\xa0other\xa0neural\xa0networks,\xa0are\xa0built\xa0on\xa0the\xa0concept\xa0of\xa0processing\xa0\nthe\xa0input\xa0signal,\xa0which\xa0creates\xa0a\xa0strong\xa0dependence\xa0on\xa0superficial\xa0noise\xa0and\xa0\nirrelevant\xa0information\xa0that\xa0the\xa0transformers\xa0attention\xa0layer\xa0cannot\xa0compensate\xa0\nfor.\xa0The\xa0concept\xa0of\xa0neural\xa0networks\xa0was\xa0laid\xa0down\xa0in\xa0the\xa01950s\xa0by\xa0the\xa0idea\xa0of\xa0\nF.\xa0Rosenblatts\xa0perceptron\xa0and\xa0did\xa0not\xa0take\xa0into\xa0account\xa0the\xa0achievements\xa0of\xa0\ncognitive\xa0psychology\xa0that\xa0appeared\xa0later.\xa0According\xa0to\xa0the\xa0constructivist\xa0par-\nadigm,\xa0the\xa0input\xa0word\xa0(or\xa0perception)\xa0is\xa0only\xa0a\xa0way\xa0to\xa0check\xa0the\xa0correctness\xa0\nof\xa0the\xa0constructed\xa0predictive\xa0model\xa0for\xa0possible\xa0situations.\xa0This\xa0is\xa0the\xa0cause\xa0of\xa0\nthe\xa0biggest\xa0problem\xa0of\xa0transformers,\xa0called\xa0hallucinations.\xa0And\xa0its\xa0elimination\xa0\nis\xa0possible\xa0only\xa0by\xa0changing\xa0the\xa0architecture\xa0of\xa0the\xa0neural\xa0network,\xa0but\xa0not\xa0by\xa0\nincreasing\xa0the\xa0amount\xa0of\xa0data\xa0in\xa0training.\nKeywords:\xa0LLM;\xa0transformers;\xa0thinking;\xa0analogy;\xa0cognitive\xa0psycholo-\ngy;\xa0perceptual\xa0cycle.\n\n\xa0 \xa0 \xa0 \xa0 -\n\xa0.\xa0 \xa0\xa0 \xa0\n\xa0\xa0\xa0\xa0\xa0\xa0-\n.\xa0\xa0,\xa0\xa0\xa0,\xa0\xa0\n\xa0(\xa0\xa0\xa0),\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0.\xa0\xa0\n,\xa0,\xa0\xa0\xa0 \xa0\xa0\xa0\n,\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\n\xa0\xa0,\xa0\xa0\xa0\xa0\xa0-\n\xa0\xa0,\xa0\xa0,\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0.\xa0\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n.\xa02 (26) 2024\n89\nURL: https://cyberspace.pgu.ru\n\xa0,\xa0\xa0\xa0\xa0 \xa02024\xa0 .\xa0\xa0\nNeurlPS\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0(\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n).\xa0\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0,\xa0\n\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\n\xa0,\xa0\xa0\xa0[1].\xa0\xa0\xa0\xa0\xa0,\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0,\xa0\n\nCitation:"}], caching=False, client=None, n=1, stream=False, litellm_trace_id='53fb2fd5-e70e-4b0e-bf0a-3244dcdfa44b', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:32:37,171 - DEBUG - 

2025-12-08 14:32:37,171 - DEBUG - self.optional_params: {}
2025-12-08 14:32:37,171 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:32:37,171 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:32:37,171 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:37,171 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:32:37,172 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\n\xa02 (26) 2024\n87\nURL: https://cyberspace.pgu.ru\n 004.8 + 004.93\nDOI 10.17726/philIT.2024.2.6\n   ,  ?\n . ,\n   \n- , \nalexander.xom@gmail.com\n. \xa0\xa0\xa0\xa0\xa0-\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0-\xa0Google\xa0Gemini\xa0\n\xa0\xa0\xa0,\xa0\xa0\xa0-,\xa0\xa0\xa0\xa0\xa0\n.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0LLM,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0.,\xa0\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0-\n\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\n\xa0\xa0\xa0(inference).\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\n\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0,\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa01950-\xa0\xa0\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0.\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0(\xa0)\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0.\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0.\n :\xa0LLM;\xa0;\xa0;\xa0;\xa0-\n\xa0;\xa0\xa0;\xa0.\nWhy dont transformers think like humans?\nAlexander B. Khomyakov,\nMaster of Physical Sciences \nSaint- Petersburg, Russia\nalexander.xom@gmail.com88\n     \n \nAbstract. Large\xa0language\xa0models\xa0in\xa0the\xa0form\xa0of\xa0chatbots\xa0very\xa0realistical-\nly\xa0imitate\xa0a\xa0dialogue\xa0as\xa0an\xa0omniscient\xa0interlocutor\xa0and\xa0therefore\xa0have\xa0become\xa0\nwidespread.\xa0But\xa0even\xa0Google\xa0in\xa0its\xa0Gemini\xa0chatbot\xa0does\xa0not\xa0recommend\xa0trust-\ning\xa0what\xa0the\xa0chatbot\xa0will\xa0write\xa0and\xa0asks\xa0to\xa0check\xa0its\xa0answers.\xa0In\xa0this\xa0review,\xa0\nvarious\xa0types\xa0of\xa0LLM\xa0errors\xa0such\xa0as\xa0the\xa0curse\xa0of\xa0inversion,\xa0number\xa0processing,\xa0\netc.\xa0will\xa0be\xa0analyzed\xa0to\xa0identify\xa0their\xa0causes.\xa0Such\xa0an\xa0analysis\xa0led\xa0to\xa0the\xa0con-\nclusion\xa0about\xa0the\xa0common\xa0causes\xa0of\xa0all\xa0errors,\xa0which\xa0is\xa0that\xa0transformers\xa0do\xa0\nnot\xa0have\xa0deep\xa0analogy,\xa0hierarchy\xa0of\xa0schemes\xa0and\xa0selectivity\xa0of\xa0content\xa0taken\xa0\ninto\xa0account\xa0in\xa0the\xa0inference.\xa0But\xa0the\xa0most\xa0important\xa0conclusion\xa0is\xa0that\xa0trans-\nformers,\xa0like\xa0other\xa0neural\xa0networks,\xa0are\xa0built\xa0on\xa0the\xa0concept\xa0of\xa0processing\xa0\nthe\xa0input\xa0signal,\xa0which\xa0creates\xa0a\xa0strong\xa0dependence\xa0on\xa0superficial\xa0noise\xa0and\xa0\nirrelevant\xa0information\xa0that\xa0the\xa0transformers\xa0attention\xa0layer\xa0cannot\xa0compensate\xa0\nfor.\xa0The\xa0concept\xa0of\xa0neural\xa0networks\xa0was\xa0laid\xa0down\xa0in\xa0the\xa01950s\xa0by\xa0the\xa0idea\xa0of\xa0\nF.\xa0Rosenblatts\xa0perceptron\xa0and\xa0did\xa0not\xa0take\xa0into\xa0account\xa0the\xa0achievements\xa0of\xa0\ncognitive\xa0psychology\xa0that\xa0appeared\xa0later.\xa0According\xa0to\xa0the\xa0constructivist\xa0par-\nadigm,\xa0the\xa0input\xa0word\xa0(or\xa0perception)\xa0is\xa0only\xa0a\xa0way\xa0to\xa0check\xa0the\xa0correctness\xa0\nof\xa0the\xa0constructed\xa0predictive\xa0model\xa0for\xa0possible\xa0situations.\xa0This\xa0is\xa0the\xa0cause\xa0of\xa0\nthe\xa0biggest\xa0problem\xa0of\xa0transformers,\xa0called\xa0hallucinations.\xa0And\xa0its\xa0elimination\xa0\nis\xa0possible\xa0only\xa0by\xa0changing\xa0the\xa0architecture\xa0of\xa0the\xa0neural\xa0network,\xa0but\xa0not\xa0by\xa0\nincreasing\xa0the\xa0amount\xa0of\xa0data\xa0in\xa0training.\nKeywords:\xa0LLM;\xa0transformers;\xa0thinking;\xa0analogy;\xa0cognitive\xa0psycholo-\ngy;\xa0perceptual\xa0cycle.\n\n\xa0 \xa0 \xa0 \xa0 -\n\xa0.\xa0 \xa0\xa0 \xa0\n\xa0\xa0\xa0\xa0\xa0\xa0-\n.\xa0\xa0,\xa0\xa0\xa0,\xa0\xa0\n\xa0(\xa0\xa0\xa0),\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0.\xa0\xa0\n,\xa0,\xa0\xa0\xa0 \xa0\xa0\xa0\n,\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\n\xa0\xa0,\xa0\xa0\xa0\xa0\xa0-\n\xa0\xa0,\xa0\xa0,\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0.\xa0\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n.\xa02 (26) 2024\n89\nURL: https://cyberspace.pgu.ru\n\xa0,\xa0\xa0\xa0\xa0 \xa02024\xa0 .\xa0\xa0\nNeurlPS\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0(\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n).\xa0\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0,\xa0\n\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\n\xa0,\xa0\xa0\xa0[1].\xa0\xa0\xa0\xa0\xa0,\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0,\xa0\n\nCitation:"}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:32:37,172 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:37,172 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:32:37,172 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:37,172 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:37,172 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:37,172 - DEBUG - Using AiohttpTransport...
2025-12-08 14:32:37,172 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:32:37,172 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:32:37,172 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\n\xa02 (26) 2024\n87\nURL: https://cyberspace.pgu.ru\n 004.8 + 004.93\nDOI 10.17726/philIT.2024.2.6\n   ,  ?\n . ,\n   \n- , \nalexander.xom@gmail.com\n. \xa0\xa0\xa0\xa0\xa0-\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0-\xa0Google\xa0Gemini\xa0\n\xa0\xa0\xa0,\xa0\xa0\xa0-,\xa0\xa0\xa0\xa0\xa0\n.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0LLM,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0.,\xa0\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0-\n\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\n\xa0\xa0\xa0(inference).\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\n\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0,\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa01950-\xa0\xa0\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0.\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0(\xa0)\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0.\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0.\n :\xa0LLM;\xa0;\xa0;\xa0;\xa0-\n\xa0;\xa0\xa0;\xa0.\nWhy dont transformers think like humans?\nAlexander B. Khomyakov,\nMaster of Physical Sciences \nSaint- Petersburg, Russia\nalexander.xom@gmail.com88\n     \n \nAbstract. Large\xa0language\xa0models\xa0in\xa0the\xa0form\xa0of\xa0chatbots\xa0very\xa0realistical-\nly\xa0imitate\xa0a\xa0dialogue\xa0as\xa0an\xa0omniscient\xa0interlocutor\xa0and\xa0therefore\xa0have\xa0become\xa0\nwidespread.\xa0But\xa0even\xa0Google\xa0in\xa0its\xa0Gemini\xa0chatbot\xa0does\xa0not\xa0recommend\xa0trust-\ning\xa0what\xa0the\xa0chatbot\xa0will\xa0write\xa0and\xa0asks\xa0to\xa0check\xa0its\xa0answers.\xa0In\xa0this\xa0review,\xa0\nvarious\xa0types\xa0of\xa0LLM\xa0errors\xa0such\xa0as\xa0the\xa0curse\xa0of\xa0inversion,\xa0number\xa0processing,\xa0\netc.\xa0will\xa0be\xa0analyzed\xa0to\xa0identify\xa0their\xa0causes.\xa0Such\xa0an\xa0analysis\xa0led\xa0to\xa0the\xa0con-\nclusion\xa0about\xa0the\xa0common\xa0causes\xa0of\xa0all\xa0errors,\xa0which\xa0is\xa0that\xa0transformers\xa0do\xa0\nnot\xa0have\xa0deep\xa0analogy,\xa0hierarchy\xa0of\xa0schemes\xa0and\xa0selectivity\xa0of\xa0content\xa0taken\xa0\ninto\xa0account\xa0in\xa0the\xa0inference.\xa0But\xa0the\xa0most\xa0important\xa0conclusion\xa0is\xa0that\xa0trans-\nformers,\xa0like\xa0other\xa0neural\xa0networks,\xa0are\xa0built\xa0on\xa0the\xa0concept\xa0of\xa0processing\xa0\nthe\xa0input\xa0signal,\xa0which\xa0creates\xa0a\xa0strong\xa0dependence\xa0on\xa0superficial\xa0noise\xa0and\xa0\nirrelevant\xa0information\xa0that\xa0the\xa0transformers\xa0attention\xa0layer\xa0cannot\xa0compensate\xa0\nfor.\xa0The\xa0concept\xa0of\xa0neural\xa0networks\xa0was\xa0laid\xa0down\xa0in\xa0the\xa01950s\xa0by\xa0the\xa0idea\xa0of\xa0\nF.\xa0Rosenblatts\xa0perceptron\xa0and\xa0did\xa0not\xa0take\xa0into\xa0account\xa0the\xa0achievements\xa0of\xa0\ncognitive\xa0psychology\xa0that\xa0appeared\xa0later.\xa0According\xa0to\xa0the\xa0constructivist\xa0par-\nadigm,\xa0the\xa0input\xa0word\xa0(or\xa0perception)\xa0is\xa0only\xa0a\xa0way\xa0to\xa0check\xa0the\xa0correctness\xa0\nof\xa0the\xa0constructed\xa0predictive\xa0model\xa0for\xa0possible\xa0situations.\xa0This\xa0is\xa0the\xa0cause\xa0of\xa0\nthe\xa0biggest\xa0problem\xa0of\xa0transformers,\xa0called\xa0hallucinations.\xa0And\xa0its\xa0elimination\xa0\nis\xa0possible\xa0only\xa0by\xa0changing\xa0the\xa0architecture\xa0of\xa0the\xa0neural\xa0network,\xa0but\xa0not\xa0by\xa0\nincreasing\xa0the\xa0amount\xa0of\xa0data\xa0in\xa0training.\nKeywords:\xa0LLM;\xa0transformers;\xa0thinking;\xa0analogy;\xa0cognitive\xa0psycholo-\ngy;\xa0perceptual\xa0cycle.\n\n\xa0 \xa0 \xa0 \xa0 -\n\xa0.\xa0 \xa0\xa0 \xa0\n\xa0\xa0\xa0\xa0\xa0\xa0-\n.\xa0\xa0,\xa0\xa0\xa0,\xa0\xa0\n\xa0(\xa0\xa0\xa0),\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0.\xa0\xa0\n,\xa0,\xa0\xa0\xa0 \xa0\xa0\xa0\n,\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\n\xa0\xa0,\xa0\xa0\xa0\xa0\xa0-\n\xa0\xa0,\xa0\xa0,\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0.\xa0\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n.\xa02 (26) 2024\n89\nURL: https://cyberspace.pgu.ru\n\xa0,\xa0\xa0\xa0\xa0 \xa02024\xa0 .\xa0\xa0\nNeurlPS\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0(\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n).\xa0\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0,\xa0\n\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\n\xa0,\xa0\xa0\xa0[1].\xa0\xa0\xa0\xa0\xa0,\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0,\xa0\n\nCitation:"}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:32:39,000 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Khomyakov, Alexander B. \"   ,  ? [Why Dont Transformers Think Like Humans?].\" *      [Philosophical Problems of Information Technology and Cyberspace]*, no. 2 (26), 2024, pp. 87-89. DOI 10.17726/philIT.2024.2.6. Accessed 14 May 2025.\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.052752801927469548
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 2771,
    "candidatesTokenCount": 118,
    "totalTokenCount": 2889,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 2771
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 118
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "ddM2afO3HJKsxN8Plc_2uQ0"
}



2025-12-08 14:32:39,000 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:39,001 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:39,001 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:39,001 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:39,001 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:39,002 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:39,002 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:39,002 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:39,002 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:39,002 - DEBUG - response_cost: 0.0
2025-12-08 14:32:39,003 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:39,003 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:32:39,003 - DEBUG - response_cost: 0.0
2025-12-08 14:32:39,003 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:39,004 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:39,004 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:39,004 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:39,004 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:39,004 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:39,004 - DEBUG - Async Response: ModelResponse(id='ddM2afO3HJKsxN8Plc_2uQ0', created=1765200757, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Khomyakov, Alexander B. "   ,  ? [Why Dont Transformers Think Like Humans?]." *      [Philosophical Problems of Information Technology and Cyberspace]*, no. 2 (26), 2024, pp. 87-89. DOI 10.17726/philIT.2024.2.6. Accessed 14 May 2025.\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=118, prompt_tokens=2771, total_tokens=2889, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=118, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=2771, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:32:39,005 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:32:39,005 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:32:39,005 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:39,005 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:39,005 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:39,005 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:39,005 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:39,005 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:39,006 - DEBUG - Inside async function with retries.
2025-12-08 14:32:39,006 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x120c5f890>>, num_retries - 3
2025-12-08 14:32:39,006 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:39,006 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:39,006 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:39,006 - DEBUG - retrieve cooldown models: []
2025-12-08 14:32:39,006 - DEBUG - async cooldown deployments: []
2025-12-08 14:32:39,006 - DEBUG - cooldown_deployments: []
2025-12-08 14:32:39,006 - DEBUG - cooldown deployments: []
2025-12-08 14:32:39,006 - DEBUG - 

2025-12-08 14:32:39,006 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:32:39,007 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Khomyakov, Alexander B. "   ,  ? [Why Dont Transformers Think Like Humans?]." *      [Philosophical Problems of Information Technology and Cyberspace]*, no. 2 (26), 2024, pp. 87-89. DOI 10.17726/philIT.2024.2.6. Accessed 14 May 2025.\n\n\nCitation JSON:'}], caching=False, client=None, n=1, stream=False, litellm_trace_id='5e633e94-0169-4d58-96bd-99fa1a801ba4', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:32:39,007 - DEBUG - 

2025-12-08 14:32:39,007 - DEBUG - self.optional_params: {}
2025-12-08 14:32:39,007 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:32:39,007 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:32:39,007 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:39,008 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x120c8fd50>>
2025-12-08 14:32:39,008 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:32:39,008 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:39,008 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Khomyakov, Alexander B. "   ,  ? [Why Dont Transformers Think Like Humans?]." *      [Philosophical Problems of Information Technology and Cyberspace]*, no. 2 (26), 2024, pp. 87-89. DOI 10.17726/philIT.2024.2.6. Accessed 14 May 2025.\n\n\nCitation JSON:'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:32:39,008 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:39,008 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:39,008 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:32:39,008 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:39,008 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:39,008 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:39,009 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Khomyakov, Alexander B. "   ,  ? [Why Dont Transformers Think Like Humans?]." *      [Philosophical Problems of Information Technology and Cyberspace]*, no. 2 (26), 2024, pp. 87-89. DOI 10.17726/philIT.2024.2.6. Accessed 14 May 2025.\n\n\nCitation JSON:'}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:32:40,140 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"title\": \"   ,  ? [Why Dont Transformers Think Like Humans?]\",\n  \"authors\": [\n    \"Alexander B. Khomyakov\"\n  ],\n  \"doi\": \"10.17726/philIT.2024.2.6\"\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.0022595176495701433
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 171,
    "candidatesTokenCount": 83,
    "totalTokenCount": 254,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 171
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 83
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "d9M2afGqCPjuxN8PxNTnqA0"
}



2025-12-08 14:32:40,141 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:40,142 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:40,142 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:40,142 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:40,143 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:40,143 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:40,143 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:40,143 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:40,143 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:40,143 - DEBUG - response_cost: 0.0
2025-12-08 14:32:40,143 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:40,144 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:32:40,144 - DEBUG - response_cost: 0.0
2025-12-08 14:32:40,144 - DEBUG - Async Response: ModelResponse(id='d9M2afGqCPjuxN8PxNTnqA0', created=1765200759, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='```json\n{\n  "title": "   ,  ? [Why Dont Transformers Think Like Humans?]",\n  "authors": [\n    "Alexander B. Khomyakov"\n  ],\n  "doi": "10.17726/philIT.2024.2.6"\n}\n```', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=83, prompt_tokens=171, total_tokens=254, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=83, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=171, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:32:40,144 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:40,145 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:32:40,145 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:40,145 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:40,145 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:40,145 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:40,145 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:32:40,145 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:40,146 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:40,146 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:40,147 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:40,175 - DEBUG - Attempting to populate metadata query: {'doi': '10.17726/philIT.2024.2.6', 'authors': ['Alexander B. Khomyakov'], 'title': '   ,  ? [Why Dont Transformers Think Like Humans?]', 'session': <aiohttp.client.ClientSession object at 0x1225077d0>} via providers=[<paperqa.clients.crossref.CrossrefProvider object at 0x12068f510>, <paperqa.clients.semantic_scholar.SemanticScholarProvider object at 0x12068e890>] processors=[<paperqa.clients.journal_quality.JournalQualityPostProcessor object at 0x12068c190>]
2025-12-08 14:32:40,175 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x120ee3ed0>>
2025-12-08 14:32:40,175 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:40,175 - WARNING - SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.
2025-12-08 14:32:40,176 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:40,662 - WARNING - Metadata not found for 10.17726/philIT.2024.2.6 in CrossrefProvider.
2025-12-08 14:32:40,935 - ERROR - Error adding /Users/gabriele/Documents/GitHub/research-agent-cli/library/04351a7a49b36885fcc89a38088d75c4/327-440-1-sm.pdf: This does not look like a text document: /Users/gabriele/Documents/GitHub/research-agent-cli/library/04351a7a49b36885fcc89a38088d75c4/327-440-1-sm.pdf. Pass disable_check to ignore this error.
2025-12-08 14:32:40,936 - DEBUG - Adding PDF: /Users/gabriele/Documents/GitHub/research-agent-cli/library/38a0b433fee8c9dea59792d623fe7285/var-folders-yk-4-ysm4397c313kwrc2q92l0w0000gn-t-tmpax15kxg7.pdf
2025-12-08 14:32:40,936 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:41,004 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'custom_llm_provider': None}
2025-12-08 14:32:41,004 - DEBUG - model_info: {'key': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': None, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': None, 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:41,004 - DEBUG - added/updated model=8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be in litellm.model_cost: 8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be
2025-12-08 14:32:41,004 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:41,004 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:41,004 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:41,004 - DEBUG - added/updated model=gemini/gemini-2.0-flash-exp in litellm.model_cost: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:41,004 - DEBUG - 
Initialized Model List ['gemini/gemini-2.0-flash-exp']
2025-12-08 14:32:41,004 - INFO - Routing strategy: simple-shuffle
2025-12-08 14:32:41,004 - DEBUG - Inside async function with retries.
2025-12-08 14:32:41,004 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x1207118d0>>, num_retries - 3
2025-12-08 14:32:41,004 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:41,004 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:41,005 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:41,005 - DEBUG - retrieve cooldown models: []
2025-12-08 14:32:41,005 - DEBUG - async cooldown deployments: []
2025-12-08 14:32:41,005 - DEBUG - cooldown_deployments: []
2025-12-08 14:32:41,005 - DEBUG - cooldown deployments: []
2025-12-08 14:32:41,005 - DEBUG - 

2025-12-08 14:32:41,005 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:32:41,005 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\nATTENTION IS ALL YOU NEED IN SPEECH SEPARATION\nCem Subakan1, Mirco Ravanelli1, Samuele Cornell2, Mirko Bronzi1, Jianyuan Zhong3\n1Mila-Quebec AI Institute, Canada,\n2Universit`a Politecnica delle Marche, Italy\n3University of Rochester, USA\nABSTRACT\nRecurrent Neural Networks (RNNs) have long been the dominant\narchitecture in sequence-to-sequence learning. RNNs, however, are\ninherently sequential models that do not allow parallelization of their\ncomputations. Transformers are emerging as a natural alternative to\nstandard RNNs, replacing recurrent computations with a multi-head\nattention mechanism.\nIn this paper, we propose the SepFormer, a novel RNN-free\nTransformer-based neural network for speech separation. The Sep-\nFormer learns short and long-term dependencies with a multi-scale\napproach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix\ndatasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an\nSI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the\nparallelization advantages of Transformers and achieves a compet-\nitive performance even when downsampling the encoded represen-\ntation by a factor of 8. It is thus signicantly faster and it is less\nmemory-demanding than the latest speech separation systems with\ncomparable performance.\nIndex Terms speech separation, source separation, trans-\nformer, attention, deep learning.\n1. INTRODUCTION\nRNNs are a crucial component of modern audio processing sys-\ntems and they are used in many different domains, including speech\nrecognition, synthesis, enhancement, and separation, just to name a\nfew. Especially when coupled with multiplicative gate mechanisms\n(like LSTM [1] and GRU [2, 3]), their recurrent connections are es-\nsential to learn long-term dependencies and properly manage speech\ncontexts. Nevertheless, the inherently sequential nature of RNNs\nimpairs an effective parallelization of the computations. This bot-\ntleneck is particularly evident when processing large datasets with\nlong sequences. On the other hand, Transformers [4] completely\navoid this bottleneck by eliminating recurrence and replacing it with\na fully attention-based mechanism. By attending to the whole se-\nquence at once, a direct connection can be established between dis-\ntant elements allowing Transformers to learn long-term dependen-\ncies more easily [5]. For this reason, Transformers are gaining con-\nsiderable popularity for speech processing and recently showed com-\npetitive performance in speech recognition [6], synthesis [7], en-\nhancement [8], diarization [9], as well as speaker recognition [10].\nLittle research has been done so far on Transformer-based mod-\nels for monaural audio source separation. The eld has been revo-\nlutionized by the adoption of deep learning techniques [1116], and\nwith recent works [1723] achieving impressive results by adopt-\ning an end-to-end approach. Most of the current speech separation\ntechniques [14, 15, 1722] require effective modeling of long input\nx Encoder h Masking Net Decoder\ns1\ns2\nm1\nm2\nFig. 1. The high-level description of our system: The encoder block\nestimates a learned-representation for the input signal, while the\nmasking network estimates optimal masks to separate the sources\npresent in the mixtures. The decoder nally reconstructs the esti-\nmated sources in the time domain using the masks provided by the\nmasking network.\nsequences to perform well. Current systems rely, in large part, on the\nlearned-domain masking strategy popularized by Conv-TasNet [15].\nIn this framework, an overcomplete set of analysis and synthesis l-\nters is learned directly from the data, and separation is performed\nby estimating a mask for each source in this learned-domain. Build-\ning on this, Dual-Path RNN (DPRNN) [17] has demonstrated that\nbetter long-term modeling is crucial to improve the separation per-\nformance. This is achieved by splitting the input sequence into mul-\ntiple chunks that are processed locally and globally with different\nRNNs. Nevertheless, due to the use of RNNs, DPRNN still suffers\nfrom the aforementioned limitations of recurrent connections, espe-\ncially regarding the global processing step. An attempt to integrate\ntransformers into the speech separation pipeline has been recently\ndone in [22] where the proposed Dual-Path Transformer Network\n(DPTNet) is shown to outperform the standard DPRNN. Such an ar-\nchitecture, however, still embeds an RNN, effectively negating the\nparallelization capability of pure-attention models.\nIn this paper, we propose a novel model called SepFormer (Sep-\naration Transformer), which is mainly composed of multi-head at-\ntention and feed-forward layers. We adopt the dual-path frame-\nwork introduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the\n\nCitation:"}], caching=False, client=None, n=1, stream=False, litellm_trace_id='6b60024b-2465-40dc-a94b-cf2acff87ef9', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:32:41,005 - DEBUG - 

2025-12-08 14:32:41,005 - DEBUG - self.optional_params: {}
2025-12-08 14:32:41,005 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:32:41,005 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:32:41,005 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:41,005 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:32:41,005 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\nATTENTION IS ALL YOU NEED IN SPEECH SEPARATION\nCem Subakan1, Mirco Ravanelli1, Samuele Cornell2, Mirko Bronzi1, Jianyuan Zhong3\n1Mila-Quebec AI Institute, Canada,\n2Universit`a Politecnica delle Marche, Italy\n3University of Rochester, USA\nABSTRACT\nRecurrent Neural Networks (RNNs) have long been the dominant\narchitecture in sequence-to-sequence learning. RNNs, however, are\ninherently sequential models that do not allow parallelization of their\ncomputations. Transformers are emerging as a natural alternative to\nstandard RNNs, replacing recurrent computations with a multi-head\nattention mechanism.\nIn this paper, we propose the SepFormer, a novel RNN-free\nTransformer-based neural network for speech separation. The Sep-\nFormer learns short and long-term dependencies with a multi-scale\napproach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix\ndatasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an\nSI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the\nparallelization advantages of Transformers and achieves a compet-\nitive performance even when downsampling the encoded represen-\ntation by a factor of 8. It is thus signicantly faster and it is less\nmemory-demanding than the latest speech separation systems with\ncomparable performance.\nIndex Terms speech separation, source separation, trans-\nformer, attention, deep learning.\n1. INTRODUCTION\nRNNs are a crucial component of modern audio processing sys-\ntems and they are used in many different domains, including speech\nrecognition, synthesis, enhancement, and separation, just to name a\nfew. Especially when coupled with multiplicative gate mechanisms\n(like LSTM [1] and GRU [2, 3]), their recurrent connections are es-\nsential to learn long-term dependencies and properly manage speech\ncontexts. Nevertheless, the inherently sequential nature of RNNs\nimpairs an effective parallelization of the computations. This bot-\ntleneck is particularly evident when processing large datasets with\nlong sequences. On the other hand, Transformers [4] completely\navoid this bottleneck by eliminating recurrence and replacing it with\na fully attention-based mechanism. By attending to the whole se-\nquence at once, a direct connection can be established between dis-\ntant elements allowing Transformers to learn long-term dependen-\ncies more easily [5]. For this reason, Transformers are gaining con-\nsiderable popularity for speech processing and recently showed com-\npetitive performance in speech recognition [6], synthesis [7], en-\nhancement [8], diarization [9], as well as speaker recognition [10].\nLittle research has been done so far on Transformer-based mod-\nels for monaural audio source separation. The eld has been revo-\nlutionized by the adoption of deep learning techniques [1116], and\nwith recent works [1723] achieving impressive results by adopt-\ning an end-to-end approach. Most of the current speech separation\ntechniques [14, 15, 1722] require effective modeling of long input\nx Encoder h Masking Net Decoder\ns1\ns2\nm1\nm2\nFig. 1. The high-level description of our system: The encoder block\nestimates a learned-representation for the input signal, while the\nmasking network estimates optimal masks to separate the sources\npresent in the mixtures. The decoder nally reconstructs the esti-\nmated sources in the time domain using the masks provided by the\nmasking network.\nsequences to perform well. Current systems rely, in large part, on the\nlearned-domain masking strategy popularized by Conv-TasNet [15].\nIn this framework, an overcomplete set of analysis and synthesis l-\nters is learned directly from the data, and separation is performed\nby estimating a mask for each source in this learned-domain. Build-\ning on this, Dual-Path RNN (DPRNN) [17] has demonstrated that\nbetter long-term modeling is crucial to improve the separation per-\nformance. This is achieved by splitting the input sequence into mul-\ntiple chunks that are processed locally and globally with different\nRNNs. Nevertheless, due to the use of RNNs, DPRNN still suffers\nfrom the aforementioned limitations of recurrent connections, espe-\ncially regarding the global processing step. An attempt to integrate\ntransformers into the speech separation pipeline has been recently\ndone in [22] where the proposed Dual-Path Transformer Network\n(DPTNet) is shown to outperform the standard DPRNN. Such an ar-\nchitecture, however, still embeds an RNN, effectively negating the\nparallelization capability of pure-attention models.\nIn this paper, we propose a novel model called SepFormer (Sep-\naration Transformer), which is mainly composed of multi-head at-\ntention and feed-forward layers. We adopt the dual-path frame-\nwork introduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the\n\nCitation:"}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:32:41,005 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:41,005 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:32:41,006 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:41,006 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:41,006 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:41,006 - DEBUG - Using AiohttpTransport...
2025-12-08 14:32:41,006 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:32:41,006 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:32:41,006 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\nATTENTION IS ALL YOU NEED IN SPEECH SEPARATION\nCem Subakan1, Mirco Ravanelli1, Samuele Cornell2, Mirko Bronzi1, Jianyuan Zhong3\n1Mila-Quebec AI Institute, Canada,\n2Universit`a Politecnica delle Marche, Italy\n3University of Rochester, USA\nABSTRACT\nRecurrent Neural Networks (RNNs) have long been the dominant\narchitecture in sequence-to-sequence learning. RNNs, however, are\ninherently sequential models that do not allow parallelization of their\ncomputations. Transformers are emerging as a natural alternative to\nstandard RNNs, replacing recurrent computations with a multi-head\nattention mechanism.\nIn this paper, we propose the SepFormer, a novel RNN-free\nTransformer-based neural network for speech separation. The Sep-\nFormer learns short and long-term dependencies with a multi-scale\napproach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix\ndatasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an\nSI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the\nparallelization advantages of Transformers and achieves a compet-\nitive performance even when downsampling the encoded represen-\ntation by a factor of 8. It is thus signicantly faster and it is less\nmemory-demanding than the latest speech separation systems with\ncomparable performance.\nIndex Terms speech separation, source separation, trans-\nformer, attention, deep learning.\n1. INTRODUCTION\nRNNs are a crucial component of modern audio processing sys-\ntems and they are used in many different domains, including speech\nrecognition, synthesis, enhancement, and separation, just to name a\nfew. Especially when coupled with multiplicative gate mechanisms\n(like LSTM [1] and GRU [2, 3]), their recurrent connections are es-\nsential to learn long-term dependencies and properly manage speech\ncontexts. Nevertheless, the inherently sequential nature of RNNs\nimpairs an effective parallelization of the computations. This bot-\ntleneck is particularly evident when processing large datasets with\nlong sequences. On the other hand, Transformers [4] completely\navoid this bottleneck by eliminating recurrence and replacing it with\na fully attention-based mechanism. By attending to the whole se-\nquence at once, a direct connection can be established between dis-\ntant elements allowing Transformers to learn long-term dependen-\ncies more easily [5]. For this reason, Transformers are gaining con-\nsiderable popularity for speech processing and recently showed com-\npetitive performance in speech recognition [6], synthesis [7], en-\nhancement [8], diarization [9], as well as speaker recognition [10].\nLittle research has been done so far on Transformer-based mod-\nels for monaural audio source separation. The eld has been revo-\nlutionized by the adoption of deep learning techniques [1116], and\nwith recent works [1723] achieving impressive results by adopt-\ning an end-to-end approach. Most of the current speech separation\ntechniques [14, 15, 1722] require effective modeling of long input\nx Encoder h Masking Net Decoder\ns1\ns2\nm1\nm2\nFig. 1. The high-level description of our system: The encoder block\nestimates a learned-representation for the input signal, while the\nmasking network estimates optimal masks to separate the sources\npresent in the mixtures. The decoder nally reconstructs the esti-\nmated sources in the time domain using the masks provided by the\nmasking network.\nsequences to perform well. Current systems rely, in large part, on the\nlearned-domain masking strategy popularized by Conv-TasNet [15].\nIn this framework, an overcomplete set of analysis and synthesis l-\nters is learned directly from the data, and separation is performed\nby estimating a mask for each source in this learned-domain. Build-\ning on this, Dual-Path RNN (DPRNN) [17] has demonstrated that\nbetter long-term modeling is crucial to improve the separation per-\nformance. This is achieved by splitting the input sequence into mul-\ntiple chunks that are processed locally and globally with different\nRNNs. Nevertheless, due to the use of RNNs, DPRNN still suffers\nfrom the aforementioned limitations of recurrent connections, espe-\ncially regarding the global processing step. An attempt to integrate\ntransformers into the speech separation pipeline has been recently\ndone in [22] where the proposed Dual-Path Transformer Network\n(DPTNet) is shown to outperform the standard DPRNN. Such an ar-\nchitecture, however, still embeds an RNN, effectively negating the\nparallelization capability of pure-attention models.\nIn this paper, we propose a novel model called SepFormer (Sep-\naration Transformer), which is mainly composed of multi-head at-\ntention and feed-forward layers. We adopt the dual-path frame-\nwork introduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the\n\nCitation:"}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:32:42,022 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Subakan, Cem, et al. \"Attention Is All You Need in Speech Separation.\" *Mila-Quebec AI Institute*, 2020.\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.089869295849519623
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 1235,
    "candidatesTokenCount": 34,
    "totalTokenCount": 1269,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 1235
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 34
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "edM2aaTnF9rkxN8P4Mmy6A4"
}



2025-12-08 14:32:42,023 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:42,023 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:42,023 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:42,024 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,024 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:42,024 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,024 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,024 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:42,024 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,025 - DEBUG - response_cost: 0.0
2025-12-08 14:32:42,025 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:42,025 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:32:42,025 - DEBUG - response_cost: 0.0
2025-12-08 14:32:42,025 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,026 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:42,026 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,026 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,026 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:42,026 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:42,027 - DEBUG - Async Response: ModelResponse(id='edM2aaTnF9rkxN8P4Mmy6A4', created=1765200761, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Subakan, Cem, et al. "Attention Is All You Need in Speech Separation." *Mila-Quebec AI Institute*, 2020.\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=34, prompt_tokens=1235, total_tokens=1269, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=34, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=1235, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:32:42,027 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:32:42,027 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:32:42,028 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:42,028 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:42,028 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,028 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,028 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:42,028 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:42,029 - DEBUG - Inside async function with retries.
2025-12-08 14:32:42,029 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x1207118d0>>, num_retries - 3
2025-12-08 14:32:42,029 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:42,029 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:42,029 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:32:42,029 - DEBUG - retrieve cooldown models: []
2025-12-08 14:32:42,029 - DEBUG - async cooldown deployments: []
2025-12-08 14:32:42,029 - DEBUG - cooldown_deployments: []
2025-12-08 14:32:42,029 - DEBUG - cooldown deployments: []
2025-12-08 14:32:42,029 - DEBUG - 

2025-12-08 14:32:42,029 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:32:42,030 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Subakan, Cem, et al. "Attention Is All You Need in Speech Separation." *Mila-Quebec AI Institute*, 2020.\n\n\nCitation JSON:'}], caching=False, client=None, n=1, stream=False, litellm_trace_id='214d44ae-6246-42d2-91cc-3d3e16460295', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:32:42,030 - DEBUG - 

2025-12-08 14:32:42,030 - DEBUG - self.optional_params: {}
2025-12-08 14:32:42,030 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:32:42,030 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:32:42,031 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:42,031 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:32:42,031 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Subakan, Cem, et al. "Attention Is All You Need in Speech Separation." *Mila-Quebec AI Institute*, 2020.\n\n\nCitation JSON:'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:32:42,031 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1248c0e90>>
2025-12-08 14:32:42,031 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:42,031 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:42,031 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:32:42,031 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:32:42,031 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:42,031 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,033 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,033 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Subakan, Cem, et al. "Attention Is All You Need in Speech Separation." *Mila-Quebec AI Institute*, 2020.\n\n\nCitation JSON:'}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:32:42,861 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"title\": \"Attention Is All You Need in Speech Separation\",\n  \"authors\": [\n    \"Cem Subakan\",\n    \"et al.\"\n  ],\n  \"doi\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.00034868022283682454
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 87,
    "candidatesTokenCount": 52,
    "totalTokenCount": 139,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 87
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 52
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "etM2aaHdCZesxN8P5IPGkQ0"
}



2025-12-08 14:32:42,862 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:42,863 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:42,863 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:42,863 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,863 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:32:42,863 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,864 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,864 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:42,864 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,864 - DEBUG - response_cost: 0.0
2025-12-08 14:32:42,864 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:42,864 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:32:42,864 - DEBUG - response_cost: 0.0
2025-12-08 14:32:42,865 - DEBUG - Async Response: ModelResponse(id='etM2aaHdCZesxN8P5IPGkQ0', created=1765200762, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='```json\n{\n  "title": "Attention Is All You Need in Speech Separation",\n  "authors": [\n    "Cem Subakan",\n    "et al."\n  ],\n  "doi": null\n}\n```', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=52, prompt_tokens=87, total_tokens=139, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=52, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=87, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:32:42,866 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:42,866 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:32:42,867 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,867 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:42,867 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:42,867 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,867 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:32:42,867 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:42,868 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:42,868 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:42,869 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:42,896 - DEBUG - Attempting to populate metadata query: {'authors': ['Cem Subakan', 'et al.'], 'title': 'Attention Is All You Need in Speech Separation', 'session': <aiohttp.client.ClientSession object at 0x120872f90>} via providers=[<paperqa.clients.crossref.CrossrefProvider object at 0x1206a4450>, <paperqa.clients.semantic_scholar.SemanticScholarProvider object at 0x1206a7f10>] processors=[<paperqa.clients.journal_quality.JournalQualityPostProcessor object at 0x1206a66d0>]
2025-12-08 14:32:42,896 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x120e4c050>>
2025-12-08 14:32:42,897 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:32:42,897 - WARNING - SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.
2025-12-08 14:32:42,897 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:32:44,268 - DEBUG - connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2025-12-08 14:32:44,288 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12033fb50>
2025-12-08 14:32:44,288 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1248ae180> server_hostname='raw.githubusercontent.com' timeout=5
2025-12-08 14:32:44,315 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12033ea90>
2025-12-08 14:32:44,315 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:32:44,315 - DEBUG - send_request_headers.complete
2025-12-08 14:32:44,315 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:32:44,315 - DEBUG - send_request_body.complete
2025-12-08 14:32:44,315 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:32:44,337 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'52540'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3cd138251572ce0662c39b6cee3c3fb9dc040f283d3f3d4afe7b0952ce409f78"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'8389:10C5B:28F09C4:2ED595D:6936D36F'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 08 Dec 2025 13:32:44 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-lin1730025-LIN'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'1'), (b'X-Timer', b'S1765200764.399861,VS0,VE2'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'8720398c9b01f4a7fc2605f4027d46dd43a96fa9'), (b'Expires', b'Mon, 08 Dec 2025 13:37:44 GMT'), (b'Source-Age', b'13')])
2025-12-08 14:32:44,337 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:32:44,361 - DEBUG - receive_response_body.complete
2025-12-08 14:32:44,361 - DEBUG - response_closed.started
2025-12-08 14:32:44,361 - DEBUG - response_closed.complete
2025-12-08 14:32:44,361 - DEBUG - close.started
2025-12-08 14:32:44,361 - DEBUG - close.complete
2025-12-08 14:32:44,365 - DEBUG - 

2025-12-08 14:32:44,365 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:32:44,365 - DEBUG - [92mlitellm.aembedding(model='text-embedding-3-small', input=['ATTENTION IS ALL YOU NEED IN SPEECH SEPARATION\nCem Subakan1, Mirco Ravanelli1, Samuele Cornell2, Mirko Bronzi1, Jianyuan Zhong3\n1Mila-Quebec AI Institute, Canada,\n2Universit`a Politecnica delle Marche, Italy\n3University of Rochester, USA\nABSTRACT\nRecurrent Neural Networks (RNNs) have long been the dominant\narchitecture in sequence-to-sequence learning. RNNs, however, are\ninherently sequential models that do not allow parallelization of their\ncomputations. Transformers are emerging as a natural alternative to\nstandard RNNs, replacing recurrent computations with a multi-head\nattention mechanism.\nIn this paper, we propose the SepFormer, a novel RNN-free\nTransformer-based neural network for speech separation. The Sep-\nFormer learns short and long-term dependencies with a multi-scale\napproach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix\ndatasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an\nSI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the\nparallelization advantages of Transformers and achieves a compet-\nitive performance even when downsampling the encoded represen-\ntation by a factor of 8. It is thus signicantly faster and it is less\nmemory-demanding than the latest speech separation systems with\ncomparable performance.\nIndex Terms speech separation, source separation, trans-\nformer, attention, deep learning.\n1. INTRODUCTION\nRNNs are a crucial component of modern audio processing sys-\ntems and they are used in many different domains, including speech\nrecognition, synthesis, enhancement, and separation, just to name a\nfew. Especially when coupled with multiplicative gate mechanisms\n(like LSTM [1] and GRU [2, 3]), their recurrent connections are es-\nsential to learn long-term dependencies and properly manage speech\ncontexts. Nevertheless, the inherently sequential nature of RNNs\nimpairs an effective parallelization of the computations. This bot-\ntleneck is particularly evident when processing large datasets with\nlong sequences. On the other hand, Transformers [4] completely\navoid this bottleneck by eliminating recurrence and replacing it with\na fully attention-based mechanism. By attending to the whole se-\nquence at once, a direct connection can be established between dis-\ntant elements allowing Transformers to learn long-term dependen-\ncies more easily [5]. For this reason, Transformers are gaining con-\nsiderable popularity for speech processing and recently showed com-\npetitive performance in speech recognition [6], synthesis [7], en-\nhancement [8], diarization [9], as well as speaker recognition [10].\nLittle research has been done so far on Transformer-based mod-\nels for monaural audio source separation. The eld has been revo-\nlutionized by the adoption of deep learning techniques [1116], and\nwith recent works [1723] achieving impressive results by adopt-\ning an end-to-end approach. Most of the current speech separation\ntechniques [14, 15, 1722] require effective modeling of long input\nx Encoder h Masking Net Decoder\ns1\ns2\nm1\nm2\nFig. 1. The high-level description of our system: The encoder block\nestimates a learned-representation for the input signal, while the\nmasking network estimates optimal masks to separate the sources\npresent in the mixtures. The decoder nally reconstructs the esti-\nmated sources in the time domain using the masks provided by the\nmasking network.\nsequences to perform well. Current systems rely, in large part, on the\nlearned-domain masking strategy popularized by Conv-TasNet [15].\nIn this framework, an overcomplete set of analysis and synthesis l-\nters is learned directly from the data, and separation is performed\nby estimating a mask for each source in this learned-domain. Build-\ning on this, Dual-Path RNN (DPRNN) [17] has demonstrated that\nbetter long-term modeling is crucial to improve the separation per-\nformance. This is achieved by splitting the input sequence into mul-\ntiple chunks that are processed locally and globally with different\nRNNs. Nevertheless, due to the use of RNNs, DPRNN still suffers\nfrom the aforementioned limitations of recurrent connections, espe-\ncially regarding the global processing step. An attempt to integrate\ntransformers into the speech separation pipeline has been recently\ndone in [22] where the proposed Dual-Path Transformer Network\n(DPTNet) is shown to outperform the standard DPRNN. Such an ar-\nchitecture, however, still embeds an RNN, effectively negating the\nparallelization capability of pure-attention models.\nIn this paper, we propose a novel model called SepFormer (Sep-\naration Transformer), which is mainly composed of multi-head at-\ntention and feed-forward layers. We adopt the dual-path frame-\nwork introduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the', 'oduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the\ndual-path framework process smaller chunks.\nTo the best of our knowledge, this is the rst work showing\nthat we can obtain state-of-the-art performance in separation with an\nRNN-free Transformer-based architecture. The SepFormer achieves\nan SI-SNRi of 22.3 dB on the standard WSJ0-2mix dataset. It also\nachieves the SOTA performance of 19.5 dB SI-SNRi on the WSJ0-\n3mix dataset. The SepFormer not only processes all the time steps in\nparallel but also achieves competitive performance when downsam-\npling the encoded representation by a factor of 8. This makes the\nproposed architecture signicantly faster and less memory demand-\narXiv:2010.13154v2  [eess.AS]  8 Mar 2021ing than the latest RNN-based separation models.\n2. THE MODEL\nThe proposed model is based on the learned-domain masking ap-\nproach [14, 15, 1722] and employs an encoder, a decoder, and a\nmasking network, as shown in Figure 1. The encoder is fully con-\nvolutional, while the masking network employs two Transformers\nembedded inside the dual-path processing block proposed in [17].\nThe decoder nally reconstructs the separated signals in the time\ndomain by using the masks predicted by the masking network. To\nfoster reproducibility, the SepFormer will be made available within\nthe SpeechBrain toolkit1.\n2.1. Encoder\nThe encoder takes in the time-domain mixture-signal x  RT as\ninput, which contains audio from multiple speakers. It learns an\nSTFT-like representation h RFT\nusing a single convolutional\nlayer:\nh = ReLU(conv1d(x)). (1)\nAs we will describe in Sec. 4, the stride factor of this convolution\nimpacts signicantly on the performance, speed, and memory of the\nmodel.\n2.2. Masking Network\nFigure 2 (top) shows the detailed architecture of the masking net-\nwork (Masking Net). The masking network is fed by the encoded\nrepresentations h RFT\nand estimates a mask {m1, . . . , mNs }\nfor each of the Ns speakers in the mixture.\nAs in [15], the encoded inputh is normalized with layer normal-\nization [24] and processed by a linear layer (with dimensionalityF).\nWe then create overlapping chunks of size C by chopping up h on\nthe time axis with an overlap factor of 50%. We denote the output of\nthe chunking operation with hRFCNc , where C is the length\nof each chunk, and Nc is the resulting number of chunks.\nThe representation hfeeds the SepFormer block, which is the\nmain component of the masking network. This block, which will be\ndescribed in detail in Sec. 2.3, employs a pipeline composed of two\ntransformers able to learn short and long-term dependencies.\nThe output of the SepFormer h RFCNc is processed by\nPReLU activations followed by a linear layer. We denote the output\nof this module hR(FNs)CNc , where Ns is the number of\nspeakers. Afterwards we apply the overlap-add scheme described\nin [17] and obtain h RFNsT\n. We pass this representation\nthrough two feed-forward layers and a ReLU activation at the end to\nnally obtain the mask mk for each of the speakers.\n2.3. SepFormer Block\nFigure 2 (Middle) shows the architecture of the SepFormer block.\nThe SepFormer block is designed to model both short and long-\nterm dependencies with the dual-scale approach of DPRNNs [17].\nIn our model, the transformer block which models the short-term\ndependencies is named IntraTransformer (IntraT), and the block for\nlonger-term dependencies is named InterTransformer (InterT). In-\ntraT processes the second dimension of h, and thus acts on each\nchunk independently, modeling the short-term dependencies within\n1speechbrain.github.io/\neach chunk. Next, we permute the last two dimensions (which we\ndenote with P), and the InterT is applied to model the transitions\nacross chunks. This scheme enables effective modelling of long-\nterm dependencies across the chunks. The overall transformation of\nthe SepFormer is therefore dened as follows:\nh= finter(P(fintra(h))), (2)\nwhere we denote the IntraT and InterT with finter(.), and fintra(.),\nrespectively. The overall SepFormer block is repeated N times.\n2.3.1. Intra and Inter Transformers\nFigure 2 (Bottom) shows the architecture of the Transformers used\nfor both the IntraT and InterT blocks. It closely resembles the orig-\ninal one dened in [4]. We use the variable z to denote the input\nto the Transformer. First of all, sinusoidal positional encoding e is\nadded to the input z, such that,\nz= z + e. (3)\nPositional encoding injects information on the order of the various\nelements composing the sequence, thus improving the separation\nperformance. We follow the positional encoding denition in [4].\nWe then apply multiple Transformer layers. Inside each Trans-\nformer layer g(.), we rst apply layer normalization, followed by\nmulti-head attention (MHA):\nz= MultiHeadAttention', 'ation\nperformance. We follow the positional encoding denition in [4].\nWe then apply multiple Transformer layers. Inside each Trans-\nformer layer g(.), we rst apply layer normalization, followed by\nmulti-head attention (MHA):\nz= MultiHeadAttention(LayerNorm(z)). (4)\nAs proposed in [4], each attention head computes the scaled dot-\nproduct attention between all the elements of the sequence. The\nTransformer nally employs a feed-forward network (FFW), which\nis applied to each position independently:\nz= FeedForward(LayerNorm(z+ z)) +z+ z. (5)\nThe overall transformer block is therefore dened as follows:\nf(z) =gK(z + e) +z, (6)\nwhere gK(.) denotes K layers of transformer layer g(.). We use\nK = Nintra layers for the IntraT, and K = Ninter layers for the\nInterT. As shown in Figure 2 (Bottom) and Eq. (6), we add residual\nconnections across the transformer layers, and across the transformer\narchitecture to improve gradient backpropagation.\n2.4. Decoder\nThe decoder simply uses a transposed convolution layer, with the\nsame stride and kernel size of the encoder. The input to the de-\ncoder is the element-wise multiplication between the mask mk of\nthe source k and the output of the encoder h. The transformation of\nthe decoder can therefore be expressed as follows:\nsk = conv1d-transpose(mk h), (7)\nwhere sk RT denotes the separated source k.\n3. EXPERIMENTAL SETUP\n3.1. Dataset\nWe use the popular WSJ0-2mix and WSJ0-3mix datasets [11] for\nsource separation, where mixtures of two speakers and three speak-\ners are created by randomly mixing utterances in the WSJ0 corpus.\nThe relative levels for the sources are sampled uniformly between 0\ndB to 5 dB. Respectively, 30, 10, 5 hours of speech is used for train-\ning, validation, and test. The training and test sets are created with\ndifferent sets of speakers. The waveforms are sampled at 8 kHz.h Norm+Linear Chunking SepFormer PReLU+Linear OverlapAdd FFW+ReLU\nm1\nm2\nh h h h\nRepeat N times\nh IntraTransformer Permute InterTransformer h\nRepeat K times\nz\ne\nLayerNorm MHA LayerNorm FFW z f(z)z z\nFig. 2. (Top) The overall architecture proposed for the masking network. (Middle) The SepFormer Block. (Bottom) The transformer\narchitecture f(.) that is used both in the IntraTransformer block and in the InterTransformer block.\n3.2. Architecture and Training Details\nThe encoder is based on 256 convolutional lters with a kernel size\nof 16 samples and a stride factor of 8 samples. The decoder uses the\nsame kernel size and the stride factors of the encoder.\nIn our best models, the SepFormer masking network processes\nchunks of size C = 250 with a 50 % overlap between them and\nemploys 8 layers of transformers in both IntraT and InterT. The\nIntraT-InterT dual-path processing pipeline is repeatedN = 2times.\nWe used 8 parallel attention heads, and 1024-dimensional positional\nfeed-forward networks within each Transformer layer. The model\nhas a total of 26 million parameters.\nWe explored the use of dynamic mixing (DM) data augmenta-\ntion [23] which consists in on-the-y creation of new mixtures from\nsingle speaker sources. In this work we expanded this powerful tech-\nnique by applying also speed perturbation on the sources before mix-\ning them. The speed randomly changes between 95 % slow-down\nand 105 % speed-up.\nWe used the Adam algorithm [25] as optimizer, with a learn-\ning rate of 15e5. After epoch 65 (after epoch 100 with DM), the\nlearning rate is annealed by halving it if we do not observe any im-\nprovement of the validation performance for 3 successive epochs\n(5 epoch for DM). Gradient clipping is employed to limit the L2\nnorm of the gradients to 5. During training, we used a batch size of\n1, and used the scale-invariant signal-to-noise Ratio (SI-SNR) [26]\nvia utterance-level permutation invariant loss [13], with clipping at\n30dB [23]. We used automatic mixed-precision to speed up training.\nThe system is trained for a maximum of 200 epochs. Each epoch\ntakes approximately 1.5 hours on a single NVIDIA V100 GPU with\n32 GB of memory.\n4. RESULTS\n4.1. Results on WSJ0-2mix\nTable 1 compares the performance achieved by the proposed Sep-\nFormer with the best results reported in the literature on the WSJ0-\n2mix dataset. The SepFormer achieves an SI-SNR improvement (SI-\nSNRi) of 22.3 dB and a Signal-to-Distortion Ratio [30] (SDRi) im-\nprovement of 22.4 dB on the test-set with dynamic mixing. When\nusing dynamic mixing, the proposed architecture achieves state-of-\nthe-art performance. The SepFormer outperforms previous systems\nwithout using dynamic mixing except Wavesplit, which uses speaker\nidentity as additional information.\nTable 1. Best results on the WSJ0-2mix dataset (test-set). DM\nstands for dynamic mixing.\nModel SI-SNRi SDRi # Param Stride\nTasnet [27] 10.8 11.1 n.a 20\nSignPredictionNet [28] 15.3 15.6 55.2M 8\nConvTasnet [15] 15.3 15.6 5.1M 10\nTwo-Step CTN [29] 16.1 n.a. 8.6M 10\nDeepCASA [18] 17.7 18.0 12.8M 1\nFurcaNeXt [19] n.a. 18.4 51.4M n.a.\nDualPathRNN [17] 18.8 19.0 ', '# Param Stride\nTasnet [27] 10.8 11.1 n.a 20\nSignPredictionNet [28] 15.3 15.6 55.2M 8\nConvTasnet [15] 15.3 15.6 5.1M 10\nTwo-Step CTN [29] 16.1 n.a. 8.6M 10\nDeepCASA [18] 17.7 18.0 12.8M 1\nFurcaNeXt [19] n.a. 18.4 51.4M n.a.\nDualPathRNN [17] 18.8 19.0 2.6M 1\nsudo rm -rf [21] 18.9 n.a. 2.6M 10\nVSUNOS [20] 20.1 20.4 7.5M 2\nDPTNet* [22] 20.2 20.6 2.6M 1\nWavesplit** [23] 21.0 21.2 29M 1\nWavesplit** + DM [23] 22.2 22.3 29M 1\nSepFormer 20.4 20.5 26M 8\nSepFormer + DM 22.3 22.4 26M 8\n*only SI-SNR and SDR (without improvement) are reported.\n**uses speaker-ids as additional info.\nTable 2. Ablation of the SepFormer on WSJ0-2Mix (validation set).\nSI-SNRi N Nintra Ninter # Heads DFF PosEnc DM\n22.3 2 8 8 8 1024 Yes Yes\n20.5 2 8 8 8 1024 Yes No\n20.4 2 4 4 16 2048 Yes No\n20.2 2 4 4 8 2048 Yes No\n19.9 2 4 4 8 2048 Yes No\n19.8 3 4 4 8 2048 Yes No\n19.4 2 4 4 8 2048 No No\n19.2 2 4 1 8 2048 Yes No\n19.1 2 3 3 8 2048 Yes No\n19.0 2 3 3 8 2048 No No\n4.2. Ablation Study\nHereafter we study the effect of various hyperparameters and data\naugmentation on the performance of the SepFormer using WSJ0-\n2mix dataset. The results are summarized in Table 2. The reported\nperformance in this table is calculated on the validation set.\nWe observe that the number of InterT and IntraT blocks has an\nimportant impact on the performance. The best results are achieved\nwith 8 layers for both blocks replicated two times. We also would\nlike to point out that a respectable performance of 19.2 dB is ob-\ntained even when we use a single layer transformer for the Inter-0 10 20 30 40\nTraining time (hours)\n8\n10\n12\n14\n16\n18\n20SI-SNRi (dB)\nTraining Speed on WSJ0-2Mix\nSepFormer\nDP-RNN\nDPTNet\n1.0 2.0 3.0 4.0 5.0\nInput sequence length in seconds\n32.0\n55.0\n95.0\n166.0\n288.0\n501.0miliseconds\nAverage Forward-Pass Time\nSepFormer\nDP-RNN\nDPTNet\nWavesplit\n1.0 2.0 3.0 4.0 5.0\nInput sequence length in seconds\n2.0\n3.0\n6.0\n11.0\n21.0\n40.0GBytes\nMemory Usage\nSepFormer\nDP-RNN\nDPTNet\nWavesplit\nFig. 3. ( Left) The traning curves of SepFormer, DPRNN, and DPTNeT on the WSJ0-2mix dataset. (Middle & Right)The comparison of\nforward-pass speed and memory usage in the GPU on inputs ranging 1-5 seconds long sampled at 8kHz.\nTable 3. Best results on the WSJ0-3mix dataset.\nModel SI-SNRi SDRi # Param\nConvTasnet [15] 12.7 13.1 5.1M\nDualPathRNN [17] 14.7 n.a 2.6M\nVSUNOS [20] 16.9 n.a 7.5M\nWavesplit [23] 17.3 17.6 29M\nWavesplit [23] + DM 17.8 18.1 29M\nSepformer 17.6 17.9 26M\nSepformer + DM 19.5 19.7 26M\nTransformer. This suggests that the IntraTransformer, and thus lo-\ncal processing, has a greater inuence on the performance. It also\nemerges that positional encoding is helpful (e.g. see lines 3 and 5 of\nTable 2). A similar outcome has been observed in [31] for speech\nenhancement. As for the number of attention heads, we observe a\nslight performance difference between 8 and 16 heads. Finally, it\ncan be observed that dynamic mixing helps the performance signi-\ncantly.\n4.3. Results on WSJ0-3mix\nTable 3 showcases the best performing models on the WSJ0-3mix\ndataset. SepFormer obtains the state-of-the-art performance with an\nSI-SNRi of 19.5 dB and an SDRi of 19.7 dB. We used here the best\narchitecture found for the WSJ0-2mix dataset. The only difference is\nthat the decoder has now three outputs. It is worth noting that on this\ncorpus the SepFormer outperforms all previously proposed systems.\nOur results on WSJ0-2mix and WSJ0-3mix show that it is pos-\nsible to achieve state-of-the-art performance in separation with an\nRNN-free Transformer-based model. The big advantage of Sep-\nFormer over RNN-based systems like [17,20,22] is the possibility to\nparallelize the computations over different time steps. This leads to\nfaster training and inference, as described in the following section.\n4.4. Speed and Memory Comparison\nWe now compare the training and inference speed of our model with\nDPRNN [17] and DPTNet [22]. Figure 3 (left) shows the training\ncurves of the aforementioned models on the WSJ0-2mix dataset.\nWe plot the performance achieved on the validation set in the rst\n48 hours of training versus the wall-clock time. For a fair com-\nparison, we used the same machine with the same GPU (a single\nNVIDIA V100-32GB) for all the models. Moreover, all the systems\nare trained with a batch size of 1 and employ automatic mixed pre-\ncision. We observe that the SepFormer is faster than DPRNN and\nDPTNeT. Figure 3 (left), highlights that SepFormer reaches above\n17dB levels only after a full day of training, whereas the DPRNN\nmodel requires two days of training to achieve the same level of per-\nformance.\nFigure 3 (middle&right) compares the average computation time\n(in ms) and the total memory allocation (in GB) during inference\nwhen single precision is used. We analyze the speed of our best\nmodel for both WSJ0-2Mix and WSJ0-3Mix datasets. We compare\nour models against DP-RNN, DPTNeT, and Wavesplit. All the mod-\nels are stored in the same NVIDIA RTX8000-48GB GPU and we\nperformed this analysis using the PyTorch proler [32', 'eed of our best\nmodel for both WSJ0-2Mix and WSJ0-3Mix datasets. We compare\nour models against DP-RNN, DPTNeT, and Wavesplit. All the mod-\nels are stored in the same NVIDIA RTX8000-48GB GPU and we\nperformed this analysis using the PyTorch proler [32]. For Waves-\nplit we used the implementation in [33].\nFrom this analysis, it emerges that the SepFormer is not only\nfaster but also less memory demanding than DPTNet, DPRNN, and\nWavesplit. We observed the same behavior using the CPU for infer-\nence also. Such a level of computational efciency is achieved even\nthough the proposed SepFormer employs more parameters than the\nother RNN-based methods (see Table 1). This is not only due to the\nsuperior parallelization capabilities of the proposed model, but also\nbecause the best performance is achieved with a stride factor of 8\nsamples, against a stride of 1 for DPRNN and DPTNet. Increasing\nthe stride of the encoder results in downsampling the input sequence,\nand therefore the model processes less data. In [17], the authors\nshowed that the DPRNN performance degrades when increasing the\nstride factor. The SepFormer, instead, reaches competitive results\neven with a relatively large stride, leading to the aforementioned\nspeed and memory advantages.\n5. CONCLUSIONS\nIn this paper, we proposed a novel neural model for speech sepa-\nration called SepFormer (Separation Transformer). The SepFormer\nis an RNN-free architecture that employs a masking network com-\nposed of transformers only. The masking network learns both short\nand long-term dependencies using a multi-scale approach. Our re-\nsults, reported on the WSJ0-2mix and WSJ0-3mix datasets, high-\nlight that we can reach state-of-the-art performances in source sep-\naration without using RNNs in the network design. This way, com-\nputations over different time-steps can be parallelized. Moreover,\nour model achieves a competitive performance even when subsam-\npling the encoded representation by a factor of 8. These two prop-\nerties lead to a signicant speed-up at training/inference time and\na drastic reduction of memory usage, especially when compared to\nrecent models such as DPRNN, DPTNet, and Wavesplit. As future\nwork, we would like to explore different transformer architectures\nthat could potentially further improve performance, speed, and mem-\nory usage.6. REFERENCES\n[1] S. Hochreiter and J. Schmidhuber, Long short-term memory,\nNeural Computation, vol. 9, no. 8, pp. 17351780, Nov. 1997.\n[2] K. Cho, B. van Merri enboer, D. Bahdanau, and Y . Bengio, On\nthe properties of neural machine translation: Encoderdecoder\napproaches, in Proc. of SSST, 2014, pp. 103111.\n[3] M. Ravanelli, P. Brakel, M. Omologo, and Y . Bengio, Light\ngated recurrent units for speech recognition, IEEE Transac-\ntions on Emerging Topics in Computational Intelligence, vol.\n2, no. 2, pp. 92102, April 2018.\n[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all\nyou need, CoRR, vol. abs/1706.03762, 2017.\n[5] G. Kerg, B. Kanuparthi, A. Goyal, K. Goyette, Y . Bengio, and\nG. Lajoie, Untangling tradeoffs between recurrence and self-\nattention in neural networks, CoRR, vol. abs/2006.09471,\n2020.\n[6] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y . Soplin, R. Yamamoto, X. Wang, S. Watan-\nabe, T. Yoshimura, and W. Zhang, A comparative study on\ntransformer vs rnn in speech applications, in Proc. of ASRU,\n2019, pp. 449456.\n[7] N. Li, S. Liu, Y . Liu, S. Zhao, and M. Liu, Neural speech\nsynthesis with transformer network, in Proc. of AAAI, 2019,\npp. 67066713.\n[8] J. Kim, M. El-Khamy, and J. Lee, T-gsa: Transformer with\ngaussian-weighted self-attention for speech enhancement, in\nProc. of ICASSP, 2020, pp. 66496653.\n[9] Q. Li, F. L. Kreyssig, C. Zhang, and P. C. Woodland, Dis-\ncriminative neural clustering for speaker diarisation, CoRR,\nvol. abs/1910.09703, 2019.\n[10] X. Chang, W. Zhang, Y . Qian, J. Le Roux, and S. Watan-\nabe, End-to-end multi-speaker speech recognition with trans-\nformer, in Proc. of ICASSP, 2020, pp. 61346138.\n[11] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, Deep\nclustering: Discriminative embeddings for segmentation and\nseparation, in Proc. of ICASSP, 2016, pp. 3135.\n[12] D. Yu, M. Kolbk, Z. Tan, and J. Jensen, Permutation in-\nvariant training of deep models for speaker-independent multi-\ntalker speech separation, in Proc. of ICASSP, 2017, pp. 241\n245.\n[13] M. Kolbk, D. Yu, Z.-H. Tan, and J. Jensen, Multitalker\nspeech separation with utterance-level permutation invariant\ntraining of deep recurrent neural networks,IEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing, vol. 25,\nno. 10, pp. 19011913, 2017.\n[14] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis,\nEnd-to-end source separation with adaptive front-ends, in\nProc. of ACSSC, 2018, pp. 684688.\n[15] Y . Luo and N. Mesgarani, Conv-TasNet: Surpassing Ideal\nTimeFrequency Magni', ' 2017.\n[14] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis,\nEnd-to-end source separation with adaptive front-ends, in\nProc. of ACSSC, 2018, pp. 684688.\n[15] Y . Luo and N. Mesgarani, Conv-TasNet: Surpassing Ideal\nTimeFrequency Magnitude Masking for Speech Separation,\nvol. 27, no. 8, pp. 12561266, Aug. 2019.\n[16] P. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,\nDeep learning for monoaural source separation, in Proc. of\nICASSP, 2014, pp. 15621566.\n[17] Y . Luo, Z. Chen, and T. Yoshioka, Dual-path rnn: ef-\ncient long sequence modeling for time-domain single-channel\nspeech separation, in Proc. of ICASSP, 2020, pp. 4650.\n[18] Y . Liu and D. Wang, Divide and conquer: A deep casa\napproach to talker-independent monaural speaker separation,\nIEEE/ACM Transactions on audio, speech, and language pro-\ncessing, vol. 27, no. 12, 2019.\n[19] Z. Shi, H. Lin, L. Liu, R. Liu, J. Han, and A. Shi, Furcanext:\nEnd-to-end monaural speech separation with dynamic gated di-\nlated temporal convolutional networks, in MultiMedia Mod-\neling, 2020, pp. 653665.\n[20] E. Nachmani, Y . Adi, and L. Wolf, V oice separation with\nan unknown number of multiple speakers, ICML, pp. 7164\n7175, 2020.\n[21] E. Tzinis, Z. Wang, and P. Smaragdis, Sudo rm -rf: Efcient\nnetworks for universal audio source separation, in MLSP,\n2020, pp. 16.\n[22] J. Chen, Q. Mao, and D. Liu, Dual-Path Transformer\nNetwork: Direct Context-Aware Modeling for End-to-End\nMonaural Speech Separation, in Proc. of Interspeech 2020,\n2020, pp. 26422646.\n[23] N. Zeghidour and D. Grangier, Wavesplit: End-to-end\nspeech separation by speaker clustering, arXiv preprint\narXiv:2002.08933, 2020.\n[24] L. J. Ba, J. R. Kiros, and G. E. Hinton, Layer normalization,\nCoRR, vol. abs/1607.06450, 2016.\n[25] D. P. Kingma and J. Ba, Adam: A method for stochastic opti-\nmization, arXiv preprint arXiv:1412.6980, 2014.\n[26] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, Sdr\nhalf-baked or well done?, in Proc. of ICASSP. IEEE, 2019,\npp. 626630.\n[27] Y . Luo and N. Mesgarani, TasNet: time-domain audio separa-\ntion network for real-time, single-channel speech separation,\nCoRR, vol. abs/1711.00541, 2017.\n[28] Zhong-Qiu Wang, Ke Tan, and DeLiang Wang, Deep learning\nbased phase reconstruction for speaker separation: A trigono-\nmetric perspective, in Proc. of ICASSP, 2019, pp. 7175.\n[29] E. Tzinis, S. Venkataramani, Z. Wang, C. Subakan, and\nP. Smaragdis, Two-step sound source separation: Training on\nlearned latent targets, in Proc. of ICASSP, 2020, pp. 3135.\n[30] E. Vincent, R. Gribonval, and C F evotte, Performance mea-\nsurement in blind audio source separation, IEEE transactions\non audio, speech, and language processing, vol. 14, no. 4, pp.\n14621469, 2006.\n[31] J. Kim, M. El-Khamy, and J. Lee, T-gsa: Transformer with\ngaussian-weighted self-attention for speech enhancement, in\nProc. of ICASSP, 2020, pp. 66496653.\n[32] Pytorch, Proler, https://pytorch.org/\ntutorials/recipes/recipes/profiler.html,\n2020, Accessed: 2020-10-21.\n[33] M. Pariente, S. Cornell, J. Cosentino, S. Sivasankaran, E. Tzi-\nnis, J. Heitkaemper, M. Olvera, F.-R. St oter, M. Hu, J. M.\nMartn-Donas, D. Ditter, A. Frank, A. Deleforge, and E. Vin-\ncent, Asteroid: the PyTorch-based audio source separation\ntoolkit for researchers, in Proc. of Interspeech, 2020, pp.\n26372641.'], dimensions=1536, num_retries=3, timeout=120)[0m
2025-12-08 14:32:44,365 - DEBUG - 

2025-12-08 14:32:44,366 - DEBUG - self.optional_params: {}
2025-12-08 14:32:44,366 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:32:44,366 - DEBUG - self.optional_params: {'dimensions': 1536}
2025-12-08 14:32:44,366 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://api.openai.com/v1 \
-d '{'model': 'text-embedding-3-small', 'input': ['ATTENTION IS ALL YOU NEED IN SPEECH SEPARATION\nCem Subakan1, Mirco Ravanelli1, Samuele Cornell2, Mirko Bronzi1, Jianyuan Zhong3\n1Mila-Quebec AI Institute, Canada,\n2Universit`a Politecnica delle Marche, Italy\n3University of Rochester, USA\nABSTRACT\nRecurrent Neural Networks (RNNs) have long been the dominant\narchitecture in sequence-to-sequence learning. RNNs, however, are\ninherently sequential models that do not allow parallelization of their\ncomputations. Transformers are emerging as a natural alternative to\nstandard RNNs, replacing recurrent computations with a multi-head\nattention mechanism.\nIn this paper, we propose the SepFormer, a novel RNN-free\nTransformer-based neural network for speech separation. The Sep-\nFormer learns short and long-term dependencies with a multi-scale\napproach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix\ndatasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an\nSI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the\nparallelization advantages of Transformers and achieves a compet-\nitive performance even when downsampling the encoded represen-\ntation by a factor of 8. It is thus signicantly faster and it is less\nmemory-demanding than the latest speech separation systems with\ncomparable performance.\nIndex Terms speech separation, source separation, trans-\nformer, attention, deep learning.\n1. INTRODUCTION\nRNNs are a crucial component of modern audio processing sys-\ntems and they are used in many different domains, including speech\nrecognition, synthesis, enhancement, and separation, just to name a\nfew. Especially when coupled with multiplicative gate mechanisms\n(like LSTM [1] and GRU [2, 3]), their recurrent connections are es-\nsential to learn long-term dependencies and properly manage speech\ncontexts. Nevertheless, the inherently sequential nature of RNNs\nimpairs an effective parallelization of the computations. This bot-\ntleneck is particularly evident when processing large datasets with\nlong sequences. On the other hand, Transformers [4] completely\navoid this bottleneck by eliminating recurrence and replacing it with\na fully attention-based mechanism. By attending to the whole se-\nquence at once, a direct connection can be established between dis-\ntant elements allowing Transformers to learn long-term dependen-\ncies more easily [5]. For this reason, Transformers are gaining con-\nsiderable popularity for speech processing and recently showed com-\npetitive performance in speech recognition [6], synthesis [7], en-\nhancement [8], diarization [9], as well as speaker recognition [10].\nLittle research has been done so far on Transformer-based mod-\nels for monaural audio source separation. The eld has been revo-\nlutionized by the adoption of deep learning techniques [1116], and\nwith recent works [1723] achieving impressive results by adopt-\ning an end-to-end approach. Most of the current speech separation\ntechniques [14, 15, 1722] require effective modeling of long input\nx Encoder h Masking Net Decoder\ns1\ns2\nm1\nm2\nFig. 1. The high-level description of our system: The encoder block\nestimates a learned-representation for the input signal, while the\nmasking network estimates optimal masks to separate the sources\npresent in the mixtures. The decoder nally reconstructs the esti-\nmated sources in the time domain using the masks provided by the\nmasking network.\nsequences to perform well. Current systems rely, in large part, on the\nlearned-domain masking strategy popularized by Conv-TasNet [15].\nIn this framework, an overcomplete set of analysis and synthesis l-\nters is learned directly from the data, and separation is performed\nby estimating a mask for each source in this learned-domain. Build-\ning on this, Dual-Path RNN (DPRNN) [17] has demonstrated that\nbetter long-term modeling is crucial to improve the separation per-\nformance. This is achieved by splitting the input sequence into mul-\ntiple chunks that are processed locally and globally with different\nRNNs. Nevertheless, due to the use of RNNs, DPRNN still suffers\nfrom the aforementioned limitations of recurrent connections, espe-\ncially regarding the global processing step. An attempt to integrate\ntransformers into the speech separation pipeline has been recently\ndone in [22] where the proposed Dual-Path Transformer Network\n(DPTNet) is shown to outperform the standard DPRNN. Such an ar-\nchitecture, however, still embeds an RNN, effectively negating the\nparallelization capability of pure-attention models.\nIn this paper, we propose a novel model called SepFormer (Sep-\naration Transformer), which is mainly composed of multi-head at-\ntention and feed-forward layers. We adopt the dual-path frame-\nwork introduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the', 'oduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the\ndual-path framework process smaller chunks.\nTo the best of our knowledge, this is the rst work showing\nthat we can obtain state-of-the-art performance in separation with an\nRNN-free Transformer-based architecture. The SepFormer achieves\nan SI-SNRi of 22.3 dB on the standard WSJ0-2mix dataset. It also\nachieves the SOTA performance of 19.5 dB SI-SNRi on the WSJ0-\n3mix dataset. The SepFormer not only processes all the time steps in\nparallel but also achieves competitive performance when downsam-\npling the encoded representation by a factor of 8. This makes the\nproposed architecture signicantly faster and less memory demand-\narXiv:2010.13154v2  [eess.AS]  8 Mar 2021ing than the latest RNN-based separation models.\n2. THE MODEL\nThe proposed model is based on the learned-domain masking ap-\nproach [14, 15, 1722] and employs an encoder, a decoder, and a\nmasking network, as shown in Figure 1. The encoder is fully con-\nvolutional, while the masking network employs two Transformers\nembedded inside the dual-path processing block proposed in [17].\nThe decoder nally reconstructs the separated signals in the time\ndomain by using the masks predicted by the masking network. To\nfoster reproducibility, the SepFormer will be made available within\nthe SpeechBrain toolkit1.\n2.1. Encoder\nThe encoder takes in the time-domain mixture-signal x  RT as\ninput, which contains audio from multiple speakers. It learns an\nSTFT-like representation h RFT\nusing a single convolutional\nlayer:\nh = ReLU(conv1d(x)). (1)\nAs we will describe in Sec. 4, the stride factor of this convolution\nimpacts signicantly on the performance, speed, and memory of the\nmodel.\n2.2. Masking Network\nFigure 2 (top) shows the detailed architecture of the masking net-\nwork (Masking Net). The masking network is fed by the encoded\nrepresentations h RFT\nand estimates a mask {m1, . . . , mNs }\nfor each of the Ns speakers in the mixture.\nAs in [15], the encoded inputh is normalized with layer normal-\nization [24] and processed by a linear layer (with dimensionalityF).\nWe then create overlapping chunks of size C by chopping up h on\nthe time axis with an overlap factor of 50%. We denote the output of\nthe chunking operation with hRFCNc , where C is the length\nof each chunk, and Nc is the resulting number of chunks.\nThe representation hfeeds the SepFormer block, which is the\nmain component of the masking network. This block, which will be\ndescribed in detail in Sec. 2.3, employs a pipeline composed of two\ntransformers able to learn short and long-term dependencies.\nThe output of the SepFormer h RFCNc is processed by\nPReLU activations followed by a linear layer. We denote the output\nof this module hR(FNs)CNc , where Ns is the number of\nspeakers. Afterwards we apply the overlap-add scheme described\nin [17] and obtain h RFNsT\n. We pass this representation\nthrough two feed-forward layers and a ReLU activation at the end to\nnally obtain the mask mk for each of the speakers.\n2.3. SepFormer Block\nFigure 2 (Middle) shows the architecture of the SepFormer block.\nThe SepFormer block is designed to model both short and long-\nterm dependencies with the dual-scale approach of DPRNNs [17].\nIn our model, the transformer block which models the short-term\ndependencies is named IntraTransformer (IntraT), and the block for\nlonger-term dependencies is named InterTransformer (InterT). In-\ntraT processes the second dimension of h, and thus acts on each\nchunk independently, modeling the short-term dependencies within\n1speechbrain.github.io/\neach chunk. Next, we permute the last two dimensions (which we\ndenote with P), and the InterT is applied to model the transitions\nacross chunks. This scheme enables effective modelling of long-\nterm dependencies across the chunks. The overall transformation of\nthe SepFormer is therefore dened as follows:\nh= finter(P(fintra(h))), (2)\nwhere we denote the IntraT and InterT with finter(.), and fintra(.),\nrespectively. The overall SepFormer block is repeated N times.\n2.3.1. Intra and Inter Transformers\nFigure 2 (Bottom) shows the architecture of the Transformers used\nfor both the IntraT and InterT blocks. It closely resembles the orig-\ninal one dened in [4]. We use the variable z to denote the input\nto the Transformer. First of all, sinusoidal positional encoding e is\nadded to the input z, such that,\nz= z + e. (3)\nPositional encoding injects information on the order of the various\nelements composing the sequence, thus improving the separation\nperformance. We follow the positional encoding denition in [4].\nWe then apply multiple Transformer layers. Inside each Trans-\nformer layer g(.), we rst apply layer normalization, followed by\nmulti-head attention (MHA):\nz= MultiHeadAttention', 'ation\nperformance. We follow the positional encoding denition in [4].\nWe then apply multiple Transformer layers. Inside each Trans-\nformer layer g(.), we rst apply layer normalization, followed by\nmulti-head attention (MHA):\nz= MultiHeadAttention(LayerNorm(z)). (4)\nAs proposed in [4], each attention head computes the scaled dot-\nproduct attention between all the elements of the sequence. The\nTransformer nally employs a feed-forward network (FFW), which\nis applied to each position independently:\nz= FeedForward(LayerNorm(z+ z)) +z+ z. (5)\nThe overall transformer block is therefore dened as follows:\nf(z) =gK(z + e) +z, (6)\nwhere gK(.) denotes K layers of transformer layer g(.). We use\nK = Nintra layers for the IntraT, and K = Ninter layers for the\nInterT. As shown in Figure 2 (Bottom) and Eq. (6), we add residual\nconnections across the transformer layers, and across the transformer\narchitecture to improve gradient backpropagation.\n2.4. Decoder\nThe decoder simply uses a transposed convolution layer, with the\nsame stride and kernel size of the encoder. The input to the de-\ncoder is the element-wise multiplication between the mask mk of\nthe source k and the output of the encoder h. The transformation of\nthe decoder can therefore be expressed as follows:\nsk = conv1d-transpose(mk h), (7)\nwhere sk RT denotes the separated source k.\n3. EXPERIMENTAL SETUP\n3.1. Dataset\nWe use the popular WSJ0-2mix and WSJ0-3mix datasets [11] for\nsource separation, where mixtures of two speakers and three speak-\ners are created by randomly mixing utterances in the WSJ0 corpus.\nThe relative levels for the sources are sampled uniformly between 0\ndB to 5 dB. Respectively, 30, 10, 5 hours of speech is used for train-\ning, validation, and test. The training and test sets are created with\ndifferent sets of speakers. The waveforms are sampled at 8 kHz.h Norm+Linear Chunking SepFormer PReLU+Linear OverlapAdd FFW+ReLU\nm1\nm2\nh h h h\nRepeat N times\nh IntraTransformer Permute InterTransformer h\nRepeat K times\nz\ne\nLayerNorm MHA LayerNorm FFW z f(z)z z\nFig. 2. (Top) The overall architecture proposed for the masking network. (Middle) The SepFormer Block. (Bottom) The transformer\narchitecture f(.) that is used both in the IntraTransformer block and in the InterTransformer block.\n3.2. Architecture and Training Details\nThe encoder is based on 256 convolutional lters with a kernel size\nof 16 samples and a stride factor of 8 samples. The decoder uses the\nsame kernel size and the stride factors of the encoder.\nIn our best models, the SepFormer masking network processes\nchunks of size C = 250 with a 50 % overlap between them and\nemploys 8 layers of transformers in both IntraT and InterT. The\nIntraT-InterT dual-path processing pipeline is repeatedN = 2times.\nWe used 8 parallel attention heads, and 1024-dimensional positional\nfeed-forward networks within each Transformer layer. The model\nhas a total of 26 million parameters.\nWe explored the use of dynamic mixing (DM) data augmenta-\ntion [23] which consists in on-the-y creation of new mixtures from\nsingle speaker sources. In this work we expanded this powerful tech-\nnique by applying also speed perturbation on the sources before mix-\ning them. The speed randomly changes between 95 % slow-down\nand 105 % speed-up.\nWe used the Adam algorithm [25] as optimizer, with a learn-\ning rate of 15e5. After epoch 65 (after epoch 100 with DM), the\nlearning rate is annealed by halving it if we do not observe any im-\nprovement of the validation performance for 3 successive epochs\n(5 epoch for DM). Gradient clipping is employed to limit the L2\nnorm of the gradients to 5. During training, we used a batch size of\n1, and used the scale-invariant signal-to-noise Ratio (SI-SNR) [26]\nvia utterance-level permutation invariant loss [13], with clipping at\n30dB [23]. We used automatic mixed-precision to speed up training.\nThe system is trained for a maximum of 200 epochs. Each epoch\ntakes approximately 1.5 hours on a single NVIDIA V100 GPU with\n32 GB of memory.\n4. RESULTS\n4.1. Results on WSJ0-2mix\nTable 1 compares the performance achieved by the proposed Sep-\nFormer with the best results reported in the literature on the WSJ0-\n2mix dataset. The SepFormer achieves an SI-SNR improvement (SI-\nSNRi) of 22.3 dB and a Signal-to-Distortion Ratio [30] (SDRi) im-\nprovement of 22.4 dB on the test-set with dynamic mixing. When\nusing dynamic mixing, the proposed architecture achieves state-of-\nthe-art performance. The SepFormer outperforms previous systems\nwithout using dynamic mixing except Wavesplit, which uses speaker\nidentity as additional information.\nTable 1. Best results on the WSJ0-2mix dataset (test-set). DM\nstands for dynamic mixing.\nModel SI-SNRi SDRi # Param Stride\nTasnet [27] 10.8 11.1 n.a 20\nSignPredictionNet [28] 15.3 15.6 55.2M 8\nConvTasnet [15] 15.3 15.6 5.1M 10\nTwo-Step CTN [29] 16.1 n.a. 8.6M 10\nDeepCASA [18] 17.7 18.0 12.8M 1\nFurcaNeXt [19] n.a. 18.4 51.4M n.a.\nDualPathRNN [17] 18.8 19.0 ', '# Param Stride\nTasnet [27] 10.8 11.1 n.a 20\nSignPredictionNet [28] 15.3 15.6 55.2M 8\nConvTasnet [15] 15.3 15.6 5.1M 10\nTwo-Step CTN [29] 16.1 n.a. 8.6M 10\nDeepCASA [18] 17.7 18.0 12.8M 1\nFurcaNeXt [19] n.a. 18.4 51.4M n.a.\nDualPathRNN [17] 18.8 19.0 2.6M 1\nsudo rm -rf [21] 18.9 n.a. 2.6M 10\nVSUNOS [20] 20.1 20.4 7.5M 2\nDPTNet* [22] 20.2 20.6 2.6M 1\nWavesplit** [23] 21.0 21.2 29M 1\nWavesplit** + DM [23] 22.2 22.3 29M 1\nSepFormer 20.4 20.5 26M 8\nSepFormer + DM 22.3 22.4 26M 8\n*only SI-SNR and SDR (without improvement) are reported.\n**uses speaker-ids as additional info.\nTable 2. Ablation of the SepFormer on WSJ0-2Mix (validation set).\nSI-SNRi N Nintra Ninter # Heads DFF PosEnc DM\n22.3 2 8 8 8 1024 Yes Yes\n20.5 2 8 8 8 1024 Yes No\n20.4 2 4 4 16 2048 Yes No\n20.2 2 4 4 8 2048 Yes No\n19.9 2 4 4 8 2048 Yes No\n19.8 3 4 4 8 2048 Yes No\n19.4 2 4 4 8 2048 No No\n19.2 2 4 1 8 2048 Yes No\n19.1 2 3 3 8 2048 Yes No\n19.0 2 3 3 8 2048 No No\n4.2. Ablation Study\nHereafter we study the effect of various hyperparameters and data\naugmentation on the performance of the SepFormer using WSJ0-\n2mix dataset. The results are summarized in Table 2. The reported\nperformance in this table is calculated on the validation set.\nWe observe that the number of InterT and IntraT blocks has an\nimportant impact on the performance. The best results are achieved\nwith 8 layers for both blocks replicated two times. We also would\nlike to point out that a respectable performance of 19.2 dB is ob-\ntained even when we use a single layer transformer for the Inter-0 10 20 30 40\nTraining time (hours)\n8\n10\n12\n14\n16\n18\n20SI-SNRi (dB)\nTraining Speed on WSJ0-2Mix\nSepFormer\nDP-RNN\nDPTNet\n1.0 2.0 3.0 4.0 5.0\nInput sequence length in seconds\n32.0\n55.0\n95.0\n166.0\n288.0\n501.0miliseconds\nAverage Forward-Pass Time\nSepFormer\nDP-RNN\nDPTNet\nWavesplit\n1.0 2.0 3.0 4.0 5.0\nInput sequence length in seconds\n2.0\n3.0\n6.0\n11.0\n21.0\n40.0GBytes\nMemory Usage\nSepFormer\nDP-RNN\nDPTNet\nWavesplit\nFig. 3. ( Left) The traning curves of SepFormer, DPRNN, and DPTNeT on the WSJ0-2mix dataset. (Middle & Right)The comparison of\nforward-pass speed and memory usage in the GPU on inputs ranging 1-5 seconds long sampled at 8kHz.\nTable 3. Best results on the WSJ0-3mix dataset.\nModel SI-SNRi SDRi # Param\nConvTasnet [15] 12.7 13.1 5.1M\nDualPathRNN [17] 14.7 n.a 2.6M\nVSUNOS [20] 16.9 n.a 7.5M\nWavesplit [23] 17.3 17.6 29M\nWavesplit [23] + DM 17.8 18.1 29M\nSepformer 17.6 17.9 26M\nSepformer + DM 19.5 19.7 26M\nTransformer. This suggests that the IntraTransformer, and thus lo-\ncal processing, has a greater inuence on the performance. It also\nemerges that positional encoding is helpful (e.g. see lines 3 and 5 of\nTable 2). A similar outcome has been observed in [31] for speech\nenhancement. As for the number of attention heads, we observe a\nslight performance difference between 8 and 16 heads. Finally, it\ncan be observed that dynamic mixing helps the performance signi-\ncantly.\n4.3. Results on WSJ0-3mix\nTable 3 showcases the best performing models on the WSJ0-3mix\ndataset. SepFormer obtains the state-of-the-art performance with an\nSI-SNRi of 19.5 dB and an SDRi of 19.7 dB. We used here the best\narchitecture found for the WSJ0-2mix dataset. The only difference is\nthat the decoder has now three outputs. It is worth noting that on this\ncorpus the SepFormer outperforms all previously proposed systems.\nOur results on WSJ0-2mix and WSJ0-3mix show that it is pos-\nsible to achieve state-of-the-art performance in separation with an\nRNN-free Transformer-based model. The big advantage of Sep-\nFormer over RNN-based systems like [17,20,22] is the possibility to\nparallelize the computations over different time steps. This leads to\nfaster training and inference, as described in the following section.\n4.4. Speed and Memory Comparison\nWe now compare the training and inference speed of our model with\nDPRNN [17] and DPTNet [22]. Figure 3 (left) shows the training\ncurves of the aforementioned models on the WSJ0-2mix dataset.\nWe plot the performance achieved on the validation set in the rst\n48 hours of training versus the wall-clock time. For a fair com-\nparison, we used the same machine with the same GPU (a single\nNVIDIA V100-32GB) for all the models. Moreover, all the systems\nare trained with a batch size of 1 and employ automatic mixed pre-\ncision. We observe that the SepFormer is faster than DPRNN and\nDPTNeT. Figure 3 (left), highlights that SepFormer reaches above\n17dB levels only after a full day of training, whereas the DPRNN\nmodel requires two days of training to achieve the same level of per-\nformance.\nFigure 3 (middle&right) compares the average computation time\n(in ms) and the total memory allocation (in GB) during inference\nwhen single precision is used. We analyze the speed of our best\nmodel for both WSJ0-2Mix and WSJ0-3Mix datasets. We compare\nour models against DP-RNN, DPTNeT, and Wavesplit. All the mod-\nels are stored in the same NVIDIA RTX8000-48GB GPU and we\nperformed this analysis using the PyTorch proler [32', 'eed of our best\nmodel for both WSJ0-2Mix and WSJ0-3Mix datasets. We compare\nour models against DP-RNN, DPTNeT, and Wavesplit. All the mod-\nels are stored in the same NVIDIA RTX8000-48GB GPU and we\nperformed this analysis using the PyTorch proler [32]. For Waves-\nplit we used the implementation in [33].\nFrom this analysis, it emerges that the SepFormer is not only\nfaster but also less memory demanding than DPTNet, DPRNN, and\nWavesplit. We observed the same behavior using the CPU for infer-\nence also. Such a level of computational efciency is achieved even\nthough the proposed SepFormer employs more parameters than the\nother RNN-based methods (see Table 1). This is not only due to the\nsuperior parallelization capabilities of the proposed model, but also\nbecause the best performance is achieved with a stride factor of 8\nsamples, against a stride of 1 for DPRNN and DPTNet. Increasing\nthe stride of the encoder results in downsampling the input sequence,\nand therefore the model processes less data. In [17], the authors\nshowed that the DPRNN performance degrades when increasing the\nstride factor. The SepFormer, instead, reaches competitive results\neven with a relatively large stride, leading to the aforementioned\nspeed and memory advantages.\n5. CONCLUSIONS\nIn this paper, we proposed a novel neural model for speech sepa-\nration called SepFormer (Separation Transformer). The SepFormer\nis an RNN-free architecture that employs a masking network com-\nposed of transformers only. The masking network learns both short\nand long-term dependencies using a multi-scale approach. Our re-\nsults, reported on the WSJ0-2mix and WSJ0-3mix datasets, high-\nlight that we can reach state-of-the-art performances in source sep-\naration without using RNNs in the network design. This way, com-\nputations over different time-steps can be parallelized. Moreover,\nour model achieves a competitive performance even when subsam-\npling the encoded representation by a factor of 8. These two prop-\nerties lead to a signicant speed-up at training/inference time and\na drastic reduction of memory usage, especially when compared to\nrecent models such as DPRNN, DPTNet, and Wavesplit. As future\nwork, we would like to explore different transformer architectures\nthat could potentially further improve performance, speed, and mem-\nory usage.6. REFERENCES\n[1] S. Hochreiter and J. Schmidhuber, Long short-term memory,\nNeural Computation, vol. 9, no. 8, pp. 17351780, Nov. 1997.\n[2] K. Cho, B. van Merri enboer, D. Bahdanau, and Y . Bengio, On\nthe properties of neural machine translation: Encoderdecoder\napproaches, in Proc. of SSST, 2014, pp. 103111.\n[3] M. Ravanelli, P. Brakel, M. Omologo, and Y . Bengio, Light\ngated recurrent units for speech recognition, IEEE Transac-\ntions on Emerging Topics in Computational Intelligence, vol.\n2, no. 2, pp. 92102, April 2018.\n[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all\nyou need, CoRR, vol. abs/1706.03762, 2017.\n[5] G. Kerg, B. Kanuparthi, A. Goyal, K. Goyette, Y . Bengio, and\nG. Lajoie, Untangling tradeoffs between recurrence and self-\nattention in neural networks, CoRR, vol. abs/2006.09471,\n2020.\n[6] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y . Soplin, R. Yamamoto, X. Wang, S. Watan-\nabe, T. Yoshimura, and W. Zhang, A comparative study on\ntransformer vs rnn in speech applications, in Proc. of ASRU,\n2019, pp. 449456.\n[7] N. Li, S. Liu, Y . Liu, S. Zhao, and M. Liu, Neural speech\nsynthesis with transformer network, in Proc. of AAAI, 2019,\npp. 67066713.\n[8] J. Kim, M. El-Khamy, and J. Lee, T-gsa: Transformer with\ngaussian-weighted self-attention for speech enhancement, in\nProc. of ICASSP, 2020, pp. 66496653.\n[9] Q. Li, F. L. Kreyssig, C. Zhang, and P. C. Woodland, Dis-\ncriminative neural clustering for speaker diarisation, CoRR,\nvol. abs/1910.09703, 2019.\n[10] X. Chang, W. Zhang, Y . Qian, J. Le Roux, and S. Watan-\nabe, End-to-end multi-speaker speech recognition with trans-\nformer, in Proc. of ICASSP, 2020, pp. 61346138.\n[11] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, Deep\nclustering: Discriminative embeddings for segmentation and\nseparation, in Proc. of ICASSP, 2016, pp. 3135.\n[12] D. Yu, M. Kolbk, Z. Tan, and J. Jensen, Permutation in-\nvariant training of deep models for speaker-independent multi-\ntalker speech separation, in Proc. of ICASSP, 2017, pp. 241\n245.\n[13] M. Kolbk, D. Yu, Z.-H. Tan, and J. Jensen, Multitalker\nspeech separation with utterance-level permutation invariant\ntraining of deep recurrent neural networks,IEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing, vol. 25,\nno. 10, pp. 19011913, 2017.\n[14] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis,\nEnd-to-end source separation with adaptive front-ends, in\nProc. of ACSSC, 2018, pp. 684688.\n[15] Y . Luo and N. Mesgarani, Conv-TasNet: Surpassing Ideal\nTimeFrequency Magni', ' 2017.\n[14] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis,\nEnd-to-end source separation with adaptive front-ends, in\nProc. of ACSSC, 2018, pp. 684688.\n[15] Y . Luo and N. Mesgarani, Conv-TasNet: Surpassing Ideal\nTimeFrequency Magnitude Masking for Speech Separation,\nvol. 27, no. 8, pp. 12561266, Aug. 2019.\n[16] P. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,\nDeep learning for monoaural source separation, in Proc. of\nICASSP, 2014, pp. 15621566.\n[17] Y . Luo, Z. Chen, and T. Yoshioka, Dual-path rnn: ef-\ncient long sequence modeling for time-domain single-channel\nspeech separation, in Proc. of ICASSP, 2020, pp. 4650.\n[18] Y . Liu and D. Wang, Divide and conquer: A deep casa\napproach to talker-independent monaural speaker separation,\nIEEE/ACM Transactions on audio, speech, and language pro-\ncessing, vol. 27, no. 12, 2019.\n[19] Z. Shi, H. Lin, L. Liu, R. Liu, J. Han, and A. Shi, Furcanext:\nEnd-to-end monaural speech separation with dynamic gated di-\nlated temporal convolutional networks, in MultiMedia Mod-\neling, 2020, pp. 653665.\n[20] E. Nachmani, Y . Adi, and L. Wolf, V oice separation with\nan unknown number of multiple speakers, ICML, pp. 7164\n7175, 2020.\n[21] E. Tzinis, Z. Wang, and P. Smaragdis, Sudo rm -rf: Efcient\nnetworks for universal audio source separation, in MLSP,\n2020, pp. 16.\n[22] J. Chen, Q. Mao, and D. Liu, Dual-Path Transformer\nNetwork: Direct Context-Aware Modeling for End-to-End\nMonaural Speech Separation, in Proc. of Interspeech 2020,\n2020, pp. 26422646.\n[23] N. Zeghidour and D. Grangier, Wavesplit: End-to-end\nspeech separation by speaker clustering, arXiv preprint\narXiv:2002.08933, 2020.\n[24] L. J. Ba, J. R. Kiros, and G. E. Hinton, Layer normalization,\nCoRR, vol. abs/1607.06450, 2016.\n[25] D. P. Kingma and J. Ba, Adam: A method for stochastic opti-\nmization, arXiv preprint arXiv:1412.6980, 2014.\n[26] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, Sdr\nhalf-baked or well done?, in Proc. of ICASSP. IEEE, 2019,\npp. 626630.\n[27] Y . Luo and N. Mesgarani, TasNet: time-domain audio separa-\ntion network for real-time, single-channel speech separation,\nCoRR, vol. abs/1711.00541, 2017.\n[28] Zhong-Qiu Wang, Ke Tan, and DeLiang Wang, Deep learning\nbased phase reconstruction for speaker separation: A trigono-\nmetric perspective, in Proc. of ICASSP, 2019, pp. 7175.\n[29] E. Tzinis, S. Venkataramani, Z. Wang, C. Subakan, and\nP. Smaragdis, Two-step sound source separation: Training on\nlearned latent targets, in Proc. of ICASSP, 2020, pp. 3135.\n[30] E. Vincent, R. Gribonval, and C F evotte, Performance mea-\nsurement in blind audio source separation, IEEE transactions\non audio, speech, and language processing, vol. 14, no. 4, pp.\n14621469, 2006.\n[31] J. Kim, M. El-Khamy, and J. Lee, T-gsa: Transformer with\ngaussian-weighted self-attention for speech enhancement, in\nProc. of ICASSP, 2020, pp. 66496653.\n[32] Pytorch, Proler, https://pytorch.org/\ntutorials/recipes/recipes/profiler.html,\n2020, Accessed: 2020-10-21.\n[33] M. Pariente, S. Cornell, J. Cosentino, S. Sivasankaran, E. Tzi-\nnis, J. Heitkaemper, M. Olvera, F.-R. St oter, M. Hu, J. M.\nMartn-Donas, D. Ditter, A. Frank, A. Deleforge, and E. Vin-\ncent, Asteroid: the PyTorch-based audio source separation\ntoolkit for researchers, in Proc. of Interspeech, 2020, pp.\n26372641.'], 'dimensions': 1536}'
[0m

2025-12-08 14:32:44,367 - DEBUG - Using AiohttpTransport...
2025-12-08 14:32:44,367 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:32:44,367 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:32:44,367 - DEBUG - RAW RESPONSE:
The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


2025-12-08 14:32:44,367 - DEBUG - Logging Details: logger_fn - None | callable(logger_fn) - False
2025-12-08 14:32:44,368 - DEBUG - Logging Details LiteLLM-Failure Call: [<bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x120112b10>>, <bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x120ca3290>>, <bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x120c5f890>>, <bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x1207118d0>>]
2025-12-08 14:32:44,369 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:32:44,369 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:32:44,369 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:32:44,369 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:32:44,369 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:32:44,369 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:32:44,369 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:32:44,369 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:32:44,372 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:32:44,373 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:32:44,374 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:32:44,375 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:32:44,375 - ERROR - Error adding /Users/gabriele/Documents/GitHub/research-agent-cli/library/38a0b433fee8c9dea59792d623fe7285/var-folders-yk-4-ysm4397c313kwrc2q92l0w0000gn-t-tmpax15kxg7.pdf: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable LiteLLM Retried: 3 times
2025-12-08 14:32:44,378 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:44,381 - DEBUG - connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2025-12-08 14:32:44,399 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1248b9190>
2025-12-08 14:32:44,399 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1201768d0> server_hostname='raw.githubusercontent.com' timeout=5
2025-12-08 14:32:44,421 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1248bba10>
2025-12-08 14:32:44,421 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:32:44,421 - DEBUG - send_request_headers.complete
2025-12-08 14:32:44,421 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:32:44,421 - DEBUG - send_request_body.complete
2025-12-08 14:32:44,421 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:32:44,441 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'52540'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3cd138251572ce0662c39b6cee3c3fb9dc040f283d3f3d4afe7b0952ce409f78"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'8389:10C5B:28F09C4:2ED595D:6936D36F'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 08 Dec 2025 13:32:44 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-lin1730029-LIN'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'1'), (b'X-Timer', b'S1765200765.503466,VS0,VE2'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'33b9d7826a765e5c57a422fb32bf2041c29988de'), (b'Expires', b'Mon, 08 Dec 2025 13:37:44 GMT'), (b'Source-Age', b'13')])
2025-12-08 14:32:44,442 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:32:44,459 - DEBUG - receive_response_body.complete
2025-12-08 14:32:44,460 - DEBUG - response_closed.started
2025-12-08 14:32:44,460 - DEBUG - response_closed.complete
2025-12-08 14:32:44,460 - DEBUG - close.started
2025-12-08 14:32:44,460 - DEBUG - close.complete
2025-12-08 14:32:44,467 - INFO - Query successful: what is a transformer
2025-12-08 14:32:44,468 - INFO - [LoggingWorker] atexit: Flushing 6 remaining events...
2025-12-08 14:32:44,468 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:32:44,468 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:32:44,469 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:44,469 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,469 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,469 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:32:44,469 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:32:44,469 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:44,469 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,469 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,469 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,470 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,470 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,470 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,470 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,470 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,470 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,470 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,470 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,470 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,470 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,470 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,470 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:32:44,471 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:44,471 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,471 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,471 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:32:44,471 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:32:44,471 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:44,471 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,471 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,471 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,471 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,471 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,471 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,471 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,471 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,472 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,472 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,472 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,472 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,472 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,472 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,472 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:32:44,472 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:44,472 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,472 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,472 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:32:44,472 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:32:44,472 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:44,472 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,472 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,473 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,473 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,473 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,473 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,473 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,473 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,473 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,473 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,473 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,473 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,473 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,473 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,473 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:32:44,474 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:44,474 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,474 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,474 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:32:44,474 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:32:44,474 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:44,474 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,474 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,474 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,474 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,474 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,474 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,474 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,474 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,474 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,474 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,475 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,475 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,475 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,475 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,475 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:32:44,475 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:44,475 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,475 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,475 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:32:44,475 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:32:44,475 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:44,475 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,475 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,475 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,475 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,475 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,476 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,476 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,476 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,476 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,476 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,476 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,476 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,476 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,476 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,476 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:32:44,476 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:44,476 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,476 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,476 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:32:44,476 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:32:44,477 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:32:44,477 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,477 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,477 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,477 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,477 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,477 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,477 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,477 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,477 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,477 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,477 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,477 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,477 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:32:44,477 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:32:44,478 - INFO - [LoggingWorker] atexit: Successfully flushed 6 events!
2025-12-08 14:32:44,478 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:33:15,149 - DEBUG - Using AiohttpTransport...
2025-12-08 14:33:15,150 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:33:15,150 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:33:15,177 - DEBUG - connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2025-12-08 14:33:15,200 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x103f93950>
2025-12-08 14:33:15,200 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1042a5a30> server_hostname='raw.githubusercontent.com' timeout=5
2025-12-08 14:33:15,223 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10413b450>
2025-12-08 14:33:15,223 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:33:15,223 - DEBUG - send_request_headers.complete
2025-12-08 14:33:15,223 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:33:15,223 - DEBUG - send_request_body.complete
2025-12-08 14:33:15,223 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:33:15,243 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'52540'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3cd138251572ce0662c39b6cee3c3fb9dc040f283d3f3d4afe7b0952ce409f78"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'39EE:1415:4A44EF:5A1F90:69367C9F'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 08 Dec 2025 13:33:15 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-lin1730038-LIN'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'2'), (b'X-Timer', b'S1765200795.305740,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'a5b6f5ac759c1baa2e762572424fdcf2caf4d8d9'), (b'Expires', b'Mon, 08 Dec 2025 13:38:15 GMT'), (b'Source-Age', b'11')])
2025-12-08 14:33:15,243 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:33:15,264 - DEBUG - receive_response_body.complete
2025-12-08 14:33:15,264 - DEBUG - response_closed.started
2025-12-08 14:33:15,264 - DEBUG - response_closed.complete
2025-12-08 14:33:15,265 - DEBUG - close.started
2025-12-08 14:33:15,265 - DEBUG - close.complete
2025-12-08 14:33:15,517 - DEBUG - [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2025-12-08 14:33:15,606 - DEBUG - Using AiohttpTransport...
2025-12-08 14:33:15,607 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:33:15,607 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:33:15,608 - DEBUG - Using AiohttpTransport...
2025-12-08 14:33:15,608 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:33:15,608 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:33:15,608 - DEBUG - Using AiohttpTransport...
2025-12-08 14:33:15,608 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:33:15,608 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:33:15,608 - DEBUG - Using AiohttpTransport...
2025-12-08 14:33:15,608 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:33:15,608 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:33:15,879 - INFO - Configured paper-qa with Gemini: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:15,880 - DEBUG - Adding PDF: /Users/gabriele/Documents/GitHub/research-agent-cli/library/70cd4cf2090b16a8d00040b736650432/1706.03762v7.pdf
2025-12-08 14:33:15,881 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:33:16,235 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'custom_llm_provider': None}
2025-12-08 14:33:16,235 - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-12-08 14:33:16,235 - DEBUG - added/updated model=8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be in litellm.model_cost: 8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be
2025-12-08 14:33:16,236 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:16,237 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:16,237 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:16,237 - DEBUG - added/updated model=gemini/gemini-2.0-flash-exp in litellm.model_cost: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:16,237 - DEBUG - 
Initialized Model List ['gemini/gemini-2.0-flash-exp']
2025-12-08 14:33:16,237 - INFO - Routing strategy: simple-shuffle
2025-12-08 14:33:16,237 - DEBUG - Inside async function with retries.
2025-12-08 14:33:16,237 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x106c13850>>, num_retries - 3
2025-12-08 14:33:16,237 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:16,237 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:16,237 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:16,237 - DEBUG - retrieve cooldown models: []
2025-12-08 14:33:16,237 - DEBUG - async cooldown deployments: []
2025-12-08 14:33:16,237 - DEBUG - cooldown_deployments: []
2025-12-08 14:33:16,237 - DEBUG - cooldown deployments: []
2025-12-08 14:33:16,237 - DEBUG - 

2025-12-08 14:33:16,237 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:33:16,237 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\nGoogle Brain\navaswani@google.com\nNoam Shazeer\nGoogle Brain\nnoam@google.com\nNiki Parmar\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\nGoogle Research\nusz@google.com\nLlion Jones\nGoogle Research\nllion@google.com\nAidan N. Gomez \nUniversity of Toronto\naidan@cs.toronto.edu\nukasz Kaiser\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin \nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\nWork performed while at Google Brain.\nWork performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, c\n\nCitation:"}], caching=False, client=None, n=1, stream=False, litellm_trace_id='5162d75b-c8f4-4587-96e4-8c48471f21b5', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:33:16,238 - DEBUG - 

2025-12-08 14:33:16,238 - DEBUG - self.optional_params: {}
2025-12-08 14:33:16,238 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:33:16,238 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:33:16,240 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:16,240 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:33:16,240 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\nGoogle Brain\navaswani@google.com\nNoam Shazeer\nGoogle Brain\nnoam@google.com\nNiki Parmar\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\nGoogle Research\nusz@google.com\nLlion Jones\nGoogle Research\nllion@google.com\nAidan N. Gomez \nUniversity of Toronto\naidan@cs.toronto.edu\nukasz Kaiser\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin \nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\nWork performed while at Google Brain.\nWork performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, c\n\nCitation:"}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:33:16,240 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:16,240 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:33:16,240 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:16,240 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:16,241 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:16,241 - DEBUG - Using AiohttpTransport...
2025-12-08 14:33:16,241 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:33:16,241 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:33:16,241 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\nGoogle Brain\navaswani@google.com\nNoam Shazeer\nGoogle Brain\nnoam@google.com\nNiki Parmar\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\nGoogle Research\nusz@google.com\nLlion Jones\nGoogle Research\nllion@google.com\nAidan N. Gomez \nUniversity of Toronto\naidan@cs.toronto.edu\nukasz Kaiser\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin \nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\nWork performed while at Google Brain.\nWork performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, c\n\nCitation:"}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:33:17,373 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Vaswani, Ashish, et al. \"Attention Is All You Need.\" *31st Conference on Neural Information Processing Systems (NIPS 2017)*, arXiv, 2 Aug. 2023. arXiv:1706.03762v7.\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.022742537781596184
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 1161,
    "candidatesTokenCount": 64,
    "totalTokenCount": 1225,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 1161
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 64
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "nNM2acWzGuLWxN8PyuTHqAI"
}



2025-12-08 14:33:17,376 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:17,376 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:17,377 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:17,377 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:17,377 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,377 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,377 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,377 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,377 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:17,378 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:17,378 - DEBUG - response_cost: 0.0
2025-12-08 14:33:17,378 - DEBUG - response_cost: 0.0
2025-12-08 14:33:17,378 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:33:17,379 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:17,379 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,379 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,379 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,379 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:17,380 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:17,380 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:33:17,380 - DEBUG - Async Response: ModelResponse(id='nNM2acWzGuLWxN8PyuTHqAI', created=1765200796, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Vaswani, Ashish, et al. "Attention Is All You Need." *31st Conference on Neural Information Processing Systems (NIPS 2017)*, arXiv, 2 Aug. 2023. arXiv:1706.03762v7.\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=64, prompt_tokens=1161, total_tokens=1225, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=64, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=1161, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:33:17,381 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:17,381 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:33:17,381 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,381 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:17,381 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:17,381 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,381 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:17,382 - DEBUG - Inside async function with retries.
2025-12-08 14:33:17,383 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x106c13850>>, num_retries - 3
2025-12-08 14:33:17,383 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:17,383 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:17,383 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:17,383 - DEBUG - retrieve cooldown models: []
2025-12-08 14:33:17,383 - DEBUG - async cooldown deployments: []
2025-12-08 14:33:17,383 - DEBUG - cooldown_deployments: []
2025-12-08 14:33:17,383 - DEBUG - cooldown deployments: []
2025-12-08 14:33:17,383 - DEBUG - 

2025-12-08 14:33:17,383 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:33:17,383 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Vaswani, Ashish, et al. "Attention Is All You Need." *31st Conference on Neural Information Processing Systems (NIPS 2017)*, arXiv, 2 Aug. 2023. arXiv:1706.03762v7.\n\n\nCitation JSON:'}], caching=False, client=None, n=1, stream=False, litellm_trace_id='493aa432-9869-49bd-b289-b4fad36c53d7', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:33:17,383 - DEBUG - 

2025-12-08 14:33:17,383 - DEBUG - self.optional_params: {}
2025-12-08 14:33:17,383 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:33:17,384 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:33:17,384 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:17,384 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x109507550>>
2025-12-08 14:33:17,385 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:33:17,385 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Vaswani, Ashish, et al. "Attention Is All You Need." *31st Conference on Neural Information Processing Systems (NIPS 2017)*, arXiv, 2 Aug. 2023. arXiv:1706.03762v7.\n\n\nCitation JSON:'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:33:17,385 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:17,385 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:33:17,385 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:17,385 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,387 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:17,387 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:17,387 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,387 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Vaswani, Ashish, et al. "Attention Is All You Need." *31st Conference on Neural Information Processing Systems (NIPS 2017)*, arXiv, 2 Aug. 2023. arXiv:1706.03762v7.\n\n\nCitation JSON:'}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:33:17,388 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:33:17,388 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:17,388 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,388 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:17,389 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:33:17,389 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:33:17,389 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:17,389 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,389 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:17,389 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,390 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:17,390 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:18,392 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"title\": \"Attention Is All You Need\",\n  \"authors\": [\n    \"Ashish Vaswani\",\n    \"et al.\"\n  ],\n  \"doi\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.0052600965207936813
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 117,
    "candidatesTokenCount": 49,
    "totalTokenCount": 166,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 117
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 49
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "ndM2afKHH8ShxN8P2pq-kQ8"
}



2025-12-08 14:33:18,393 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:18,394 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:18,394 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:18,394 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:18,395 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:18,395 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:18,395 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:18,395 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:18,395 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:18,396 - DEBUG - response_cost: 0.0
2025-12-08 14:33:18,396 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:18,396 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:33:18,396 - DEBUG - response_cost: 0.0
2025-12-08 14:33:18,396 - DEBUG - Async Response: ModelResponse(id='ndM2afKHH8ShxN8P2pq-kQ8', created=1765200797, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='```json\n{\n  "title": "Attention Is All You Need",\n  "authors": [\n    "Ashish Vaswani",\n    "et al."\n  ],\n  "doi": null\n}\n```', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=49, prompt_tokens=117, total_tokens=166, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=49, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=117, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:33:18,397 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:18,397 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:33:18,397 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:18,397 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:18,397 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:18,397 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:18,398 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:33:18,398 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:18,398 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:18,400 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:18,400 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:18,441 - DEBUG - Attempting to populate metadata query: {'authors': ['Ashish Vaswani', 'et al.'], 'title': 'Attention Is All You Need', 'session': <aiohttp.client.ClientSession object at 0x10955d5d0>} via providers=[<paperqa.clients.semantic_scholar.SemanticScholarProvider object at 0x1099bb290>, <paperqa.clients.crossref.CrossrefProvider object at 0x1099bb350>] processors=[<paperqa.clients.journal_quality.JournalQualityPostProcessor object at 0x109afec90>]
2025-12-08 14:33:18,441 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x109b4e710>>
2025-12-08 14:33:18,441 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:18,441 - WARNING - SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.
2025-12-08 14:33:18,441 - WARNING - CROSSREF_MAILTO environment variable not set. Crossref API rate limits may apply.
2025-12-08 14:33:18,441 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:18,441 - WARNING - CROSSREF_API_KEY environment variable not set. Crossref API rate limits may apply.
2025-12-08 14:33:18,442 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:33:18,442 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:18,442 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:18,442 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:18,442 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:33:18,442 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:33:18,442 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:18,442 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:18,442 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:18,443 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:18,443 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:18,443 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:20,030 - DEBUG - connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2025-12-08 14:33:20,051 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10c27b0d0>
2025-12-08 14:33:20,051 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x109512840> server_hostname='raw.githubusercontent.com' timeout=5
2025-12-08 14:33:20,077 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10a181650>
2025-12-08 14:33:20,077 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:33:20,077 - DEBUG - send_request_headers.complete
2025-12-08 14:33:20,077 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:33:20,077 - DEBUG - send_request_body.complete
2025-12-08 14:33:20,077 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:33:20,099 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'52540'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3cd138251572ce0662c39b6cee3c3fb9dc040f283d3f3d4afe7b0952ce409f78"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'8389:10C5B:28F09C4:2ED595D:6936D36F'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 08 Dec 2025 13:33:20 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-lin1730061-LIN'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'1'), (b'X-Timer', b'S1765200800.160946,VS0,VE2'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'8ad835fdd2c81141bc80c35be5e6a4d6c3dbc505'), (b'Expires', b'Mon, 08 Dec 2025 13:38:20 GMT'), (b'Source-Age', b'48')])
2025-12-08 14:33:20,099 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:33:20,123 - DEBUG - receive_response_body.complete
2025-12-08 14:33:20,123 - DEBUG - response_closed.started
2025-12-08 14:33:20,123 - DEBUG - response_closed.complete
2025-12-08 14:33:20,123 - DEBUG - close.started
2025-12-08 14:33:20,123 - DEBUG - close.complete
2025-12-08 14:33:20,128 - DEBUG - 

2025-12-08 14:33:20,128 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:33:20,128 - DEBUG - [92mlitellm.aembedding(model='text-embedding-3-small', input=['Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\nGoogle Brain\navaswani@google.com\nNoam Shazeer\nGoogle Brain\nnoam@google.com\nNiki Parmar\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\nGoogle Research\nusz@google.com\nLlion Jones\nGoogle Research\nllion@google.com\nAidan N. Gomez \nUniversity of Toronto\naidan@cs.toronto.edu\nukasz Kaiser\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin \nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\nWork performed while at Google Brain.\nWork performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, c', 'elve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packe', 'ith all keys, divide each by dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V) = softmax(QKT\ndk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1dk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q  k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni )\nWhere the projections are parameter matricesWQ\ni  Rdmodeldk , WK\ni  Rdmodeldk , WV\ni  Rdmodeldv\nand WO  Rhdvdmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n In "encoder-decoder attention" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to ) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedd', 'okens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodel.\n5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2  d) O(1) O(1)\nRecurrent O(n  d2) O(n) O(n)\nConvolutional O(k  n  d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r  n  d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2 to 10000  2. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi  Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6length n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k  n  d + n  d2). Even with k = n, ', 'tween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k  n  d + n  d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with 1 = 0.9, 2 = 0.98 and  = 109. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d0.5\nmodel  min(step_num0.5, step_num  warmup_steps1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0  1020\nGNMT + RL [38] 24.6 39.92 2.3  1019 1.4  1020\nConvS2S [9] 25.16 40.46 9.6  1018 1.5  1020\nMoE [32] 26.03 40.56 2.0  1019 1.2  1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0  1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8  1020 1.1  1021\nConvS2S Ensemble [9] 26.36 41.29 7.7  1019 1.2  1021\nTransformer (base model) 27.3 38.1 3.3  1018\nTransformer (big) 28.4 41.8 2.3  1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty  = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results an', 'th penalty  = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dv Pdrop ls\ntrain PPL BLEU params\nsteps (dev) (dev) 106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and  = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN seque', 'in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, aglar Glehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):17351780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Beng', 'ization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152159. ACL, June 2006.\n[27] Ankur Parikh, Oscar Tckstrm, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):19291958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 24402448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 31043112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Googles neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434443. ACL, August 2013.\n12Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb making, completing the phrase making...more difficult. Attentions here shown only for\nthe word making. Different colors represent different heads. Best viewed in color.\n13Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\n', 'ct\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word its for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15'], dimensions=1536, num_retries=3, timeout=120)[0m
2025-12-08 14:33:20,128 - DEBUG - 

2025-12-08 14:33:20,128 - DEBUG - self.optional_params: {}
2025-12-08 14:33:20,128 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:33:20,129 - DEBUG - self.optional_params: {'dimensions': 1536}
2025-12-08 14:33:20,129 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://api.openai.com/v1 \
-d '{'model': 'text-embedding-3-small', 'input': ['Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\nGoogle Brain\navaswani@google.com\nNoam Shazeer\nGoogle Brain\nnoam@google.com\nNiki Parmar\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\nGoogle Research\nusz@google.com\nLlion Jones\nGoogle Research\nllion@google.com\nAidan N. Gomez \nUniversity of Toronto\naidan@cs.toronto.edu\nukasz Kaiser\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin \nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\nWork performed while at Google Brain.\nWork performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, c', 'elve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packe', 'ith all keys, divide each by dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V) = softmax(QKT\ndk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1dk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q  k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni )\nWhere the projections are parameter matricesWQ\ni  Rdmodeldk , WK\ni  Rdmodeldk , WV\ni  Rdmodeldv\nand WO  Rhdvdmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n In "encoder-decoder attention" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to ) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedd', 'okens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodel.\n5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2  d) O(1) O(1)\nRecurrent O(n  d2) O(n) O(n)\nConvolutional O(k  n  d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r  n  d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2 to 10000  2. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi  Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6length n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k  n  d + n  d2). Even with k = n, ', 'tween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k  n  d + n  d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with 1 = 0.9, 2 = 0.98 and  = 109. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d0.5\nmodel  min(step_num0.5, step_num  warmup_steps1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0  1020\nGNMT + RL [38] 24.6 39.92 2.3  1019 1.4  1020\nConvS2S [9] 25.16 40.46 9.6  1018 1.5  1020\nMoE [32] 26.03 40.56 2.0  1019 1.2  1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0  1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8  1020 1.1  1021\nConvS2S Ensemble [9] 26.36 41.29 7.7  1019 1.2  1021\nTransformer (base model) 27.3 38.1 3.3  1018\nTransformer (big) 28.4 41.8 2.3  1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty  = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results an', 'th penalty  = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dv Pdrop ls\ntrain PPL BLEU params\nsteps (dev) (dev) 106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and  = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN seque', 'in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, aglar Glehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):17351780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Beng', 'ization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152159. ACL, June 2006.\n[27] Ankur Parikh, Oscar Tckstrm, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):19291958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 24402448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 31043112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Googles neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434443. ACL, August 2013.\n12Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb making, completing the phrase making...more difficult. Attentions here shown only for\nthe word making. Different colors represent different heads. Best viewed in color.\n13Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\n', 'ct\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word its for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15'], 'dimensions': 1536}'
[0m

2025-12-08 14:33:20,130 - DEBUG - Using AiohttpTransport...
2025-12-08 14:33:20,130 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:33:20,130 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:33:20,130 - DEBUG - RAW RESPONSE:
The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


2025-12-08 14:33:20,130 - DEBUG - Logging Details: logger_fn - None | callable(logger_fn) - False
2025-12-08 14:33:20,136 - DEBUG - Logging Details LiteLLM-Failure Call: [<bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x106c13850>>]
2025-12-08 14:33:20,138 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:33:20,138 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:33:20,141 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:33:20,141 - ERROR - Error adding /Users/gabriele/Documents/GitHub/research-agent-cli/library/70cd4cf2090b16a8d00040b736650432/1706.03762v7.pdf: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable LiteLLM Retried: 3 times
2025-12-08 14:33:20,141 - DEBUG - Adding PDF: /Users/gabriele/Documents/GitHub/research-agent-cli/library/4603a874ea67c1b582ccff73ff5f82f2/2501.19393v3.pdf
2025-12-08 14:33:20,141 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:33:20,927 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'custom_llm_provider': None}
2025-12-08 14:33:20,927 - DEBUG - model=8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be, custom_llm_provider=None has no input_cost_per_token in model_cost_map. Defaulting to 0.
2025-12-08 14:33:20,927 - DEBUG - model=8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be, custom_llm_provider=None has no output_cost_per_token in model_cost_map. Defaulting to 0.
2025-12-08 14:33:20,927 - DEBUG - model_info: {'key': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': None, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': None, 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:20,927 - DEBUG - added/updated model=8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be in litellm.model_cost: 8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be
2025-12-08 14:33:20,927 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:20,927 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:20,927 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:20,927 - DEBUG - added/updated model=gemini/gemini-2.0-flash-exp in litellm.model_cost: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:20,927 - DEBUG - 
Initialized Model List ['gemini/gemini-2.0-flash-exp']
2025-12-08 14:33:20,928 - INFO - Routing strategy: simple-shuffle
2025-12-08 14:33:20,928 - DEBUG - Inside async function with retries.
2025-12-08 14:33:20,928 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x109a16d50>>, num_retries - 3
2025-12-08 14:33:20,928 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:20,928 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:20,928 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:20,928 - DEBUG - retrieve cooldown models: []
2025-12-08 14:33:20,928 - DEBUG - async cooldown deployments: []
2025-12-08 14:33:20,928 - DEBUG - cooldown_deployments: []
2025-12-08 14:33:20,928 - DEBUG - cooldown deployments: []
2025-12-08 14:33:20,928 - DEBUG - 

2025-12-08 14:33:20,928 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:33:20,928 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\ns1: Simple test-time scaling\nNiklas Muennighoff*134 Zitong Yang*1 Weijia Shi*23 Xiang Lisa Li*1 Li Fei-Fei1 Hannaneh Hajishirzi23\nLuke Zettlemoyer2 Percy Liang1 Emmanuel Cands1 Tatsunori Hashimoto1\nAbstract\nTest-time scaling is a promising new approach to\nlanguage modeling that uses extra test-time com-\nputetoimproveperformance. Recently,OpenAIs\no1 model showed this capability but did not pub-\nlicly share its methodology, leading to many repli-\ncation efforts. We seek the simplest approach to\nachievetest-timescalingandstrongreasoningper-\nformance. First, we curate a small datasets1K\nof 1,000 questions paired with reasoning traces\nrelying on three criteria we validate through abla-\ntions: difficulty,diversity,andquality. Second,we\ndevelop budget forcing to control test-time com-\npute by forcefully terminating the models think-\ning process or lengthening it by appending Wait\nmultiple times to the models generation when it\ntries to end. This can lead the model to double-\ncheck its answer, often fixing incorrect reasoning\nsteps. After supervised finetuning the Qwen2.5-\n32B-Instruct language model ons1Kand equip-\nping it with budget forcing, our models1-32B ex-\nceeds o1-preview on competition math questions\nby up to 27% (MATH and AIME24). Further,\nscaling s1-32B with budget forcing allows extrap-\nolating beyond its performance without test-time\nintervention: from 50% to 57% on AIME24. Our\nmodel, data, and code are open-source athttps:\n//github.com/simplescaling/s1.\n1. Introduction\nPerformance improvements of language models (LMs) over\nthe past years have largely relied on scaling up train-time\ncompute using large-scale self-supervised pretraining (Ka-\nplan et al., 2020; Hoffmann et al., 2022). The creation of\nthese powerful models has set the stage for a new scaling\nparadigm built on top of them:test-time scaling. The aim\n*Equal contribution. ZY and NM started the project. WS, NM\nand ZY collected the prompts, XL, ZY and NM, built the data\npipeline, LZ and WS proposed using a 1K subset and ZY and\nNM built budget forcing.1 Stanford University.2 University of\nWashington, Seattle.3 Allen Institute for AI.4 Contextual AI.\n512 2048\n65\n75\n85\n95Accuracy (%)\nMathematical\nProblem Solving\n(MATH500)\n512 2048 8192\n0\n20\n40\n60\nCompetition\nMath\n(AIME24)\n1024 4096\n40\n50\n60\nPhD-Level\nScience Questions\n(GPQA Diamond)\nAverage thinking time (tokens)\nFigure 1.Test-time scaling with s1-32B.We benchmarks1-32B\non reasoning-intensive tasks and vary test-time compute.\nof this approach is to increase the compute at test time to\nget better results. There has been much work exploring this\nidea (Snell et al., 2024; Welleck et al., 2024), and the via-\nbility of this paradigm was recently validated by OpenAI\no1 (OpenAI, 2024). o1 has demonstrated strong reasoning\nperformance with consistent gains from scaling test-time\ncompute. OpenAI describes their approach as using large-\nscalereinforcementlearning(RL)implyingtheuseofsizable\namounts of data (OpenAI, 2024). This has led to various\nattempts to replicate their models relying on techniques like\nMonte Carlo Tree Search (Gao et al., 2024b; Zhang et al.,\n2024a), multi-agent approaches (Qin et al., 2024), and oth-\ners (Wang et al., 2024a; Huang et al., 2024b; 2025). Among\nthese approaches, DeepSeek R1 (DeepSeek-AI et al., 2025)\nhas successfully replicated o1-level performance, also em-\nploying reinforcement learning via millions of samples and\nmultiple training stages. However, despite the large num-\nber of o1 replication attempts, none have openly replicated\na clear test-time scaling behavior. Thus, we ask: what is\nthe simplest approach to achieve both test-time scaling and\nstrong reasoning performance?\nWeshowthattrainingononly1,000sampleswithnext-token\nprediction and controlling thinking duration via a simple\ntest-time technique we refer to asbudget forcingleads to\na strong reasoning model that scales in performance with\nmore test-time compute. Specifically, we constructs1K,\nwhich consists of 1,000 carefully curated questions paired\nwith reasoning traces and answers distilled from Gemini\nThinking Experimental (Google, 2024). We perform super-\nvised fine-tuning (SFT) of an off-the-shelf pretrained model\n1\narXiv:2501.19393v3  [cs.CL]  1 Mar 2025s1: Simple test-time scaling\non our small dataset requiring just 26 minutes of training\non 16 H100 GPUs. After training, we control the amount\nof test-time compute our model spends usingbudget forc-\ning: (I) If the model generates more thinking tokens than\na desired limit, we forcefully end the thinking process by\nappending an end-of-thinking token delimiter. Ending the\nthinking this way makes the model transition to generating\nitsanswer. (II)Ifwewantthemodeltospendmoretest-time\ncompute on a problem, we suppress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our mode\n\nCitation:"}], caching=False, client=None, n=1, stream=False, litellm_trace_id='9782dc11-677e-4185-b9fb-c447c2903f97', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:33:20,928 - DEBUG - 

2025-12-08 14:33:20,928 - DEBUG - self.optional_params: {}
2025-12-08 14:33:20,928 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:33:20,928 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:33:20,929 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:20,929 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:33:20,929 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\ns1: Simple test-time scaling\nNiklas Muennighoff*134 Zitong Yang*1 Weijia Shi*23 Xiang Lisa Li*1 Li Fei-Fei1 Hannaneh Hajishirzi23\nLuke Zettlemoyer2 Percy Liang1 Emmanuel Cands1 Tatsunori Hashimoto1\nAbstract\nTest-time scaling is a promising new approach to\nlanguage modeling that uses extra test-time com-\nputetoimproveperformance. Recently,OpenAIs\no1 model showed this capability but did not pub-\nlicly share its methodology, leading to many repli-\ncation efforts. We seek the simplest approach to\nachievetest-timescalingandstrongreasoningper-\nformance. First, we curate a small datasets1K\nof 1,000 questions paired with reasoning traces\nrelying on three criteria we validate through abla-\ntions: difficulty,diversity,andquality. Second,we\ndevelop budget forcing to control test-time com-\npute by forcefully terminating the models think-\ning process or lengthening it by appending Wait\nmultiple times to the models generation when it\ntries to end. This can lead the model to double-\ncheck its answer, often fixing incorrect reasoning\nsteps. After supervised finetuning the Qwen2.5-\n32B-Instruct language model ons1Kand equip-\nping it with budget forcing, our models1-32B ex-\nceeds o1-preview on competition math questions\nby up to 27% (MATH and AIME24). Further,\nscaling s1-32B with budget forcing allows extrap-\nolating beyond its performance without test-time\nintervention: from 50% to 57% on AIME24. Our\nmodel, data, and code are open-source athttps:\n//github.com/simplescaling/s1.\n1. Introduction\nPerformance improvements of language models (LMs) over\nthe past years have largely relied on scaling up train-time\ncompute using large-scale self-supervised pretraining (Ka-\nplan et al., 2020; Hoffmann et al., 2022). The creation of\nthese powerful models has set the stage for a new scaling\nparadigm built on top of them:test-time scaling. The aim\n*Equal contribution. ZY and NM started the project. WS, NM\nand ZY collected the prompts, XL, ZY and NM, built the data\npipeline, LZ and WS proposed using a 1K subset and ZY and\nNM built budget forcing.1 Stanford University.2 University of\nWashington, Seattle.3 Allen Institute for AI.4 Contextual AI.\n512 2048\n65\n75\n85\n95Accuracy (%)\nMathematical\nProblem Solving\n(MATH500)\n512 2048 8192\n0\n20\n40\n60\nCompetition\nMath\n(AIME24)\n1024 4096\n40\n50\n60\nPhD-Level\nScience Questions\n(GPQA Diamond)\nAverage thinking time (tokens)\nFigure 1.Test-time scaling with s1-32B.We benchmarks1-32B\non reasoning-intensive tasks and vary test-time compute.\nof this approach is to increase the compute at test time to\nget better results. There has been much work exploring this\nidea (Snell et al., 2024; Welleck et al., 2024), and the via-\nbility of this paradigm was recently validated by OpenAI\no1 (OpenAI, 2024). o1 has demonstrated strong reasoning\nperformance with consistent gains from scaling test-time\ncompute. OpenAI describes their approach as using large-\nscalereinforcementlearning(RL)implyingtheuseofsizable\namounts of data (OpenAI, 2024). This has led to various\nattempts to replicate their models relying on techniques like\nMonte Carlo Tree Search (Gao et al., 2024b; Zhang et al.,\n2024a), multi-agent approaches (Qin et al., 2024), and oth-\ners (Wang et al., 2024a; Huang et al., 2024b; 2025). Among\nthese approaches, DeepSeek R1 (DeepSeek-AI et al., 2025)\nhas successfully replicated o1-level performance, also em-\nploying reinforcement learning via millions of samples and\nmultiple training stages. However, despite the large num-\nber of o1 replication attempts, none have openly replicated\na clear test-time scaling behavior. Thus, we ask: what is\nthe simplest approach to achieve both test-time scaling and\nstrong reasoning performance?\nWeshowthattrainingononly1,000sampleswithnext-token\nprediction and controlling thinking duration via a simple\ntest-time technique we refer to asbudget forcingleads to\na strong reasoning model that scales in performance with\nmore test-time compute. Specifically, we constructs1K,\nwhich consists of 1,000 carefully curated questions paired\nwith reasoning traces and answers distilled from Gemini\nThinking Experimental (Google, 2024). We perform super-\nvised fine-tuning (SFT) of an off-the-shelf pretrained model\n1\narXiv:2501.19393v3  [cs.CL]  1 Mar 2025s1: Simple test-time scaling\non our small dataset requiring just 26 minutes of training\non 16 H100 GPUs. After training, we control the amount\nof test-time compute our model spends usingbudget forc-\ning: (I) If the model generates more thinking tokens than\na desired limit, we forcefully end the thinking process by\nappending an end-of-thinking token delimiter. Ending the\nthinking this way makes the model transition to generating\nitsanswer. (II)Ifwewantthemodeltospendmoretest-time\ncompute on a problem, we suppress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our mode\n\nCitation:"}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:33:20,929 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:20,929 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:33:20,929 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:20,929 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:20,929 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:20,929 - DEBUG - Using AiohttpTransport...
2025-12-08 14:33:20,929 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:33:20,929 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:33:20,929 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\ns1: Simple test-time scaling\nNiklas Muennighoff*134 Zitong Yang*1 Weijia Shi*23 Xiang Lisa Li*1 Li Fei-Fei1 Hannaneh Hajishirzi23\nLuke Zettlemoyer2 Percy Liang1 Emmanuel Cands1 Tatsunori Hashimoto1\nAbstract\nTest-time scaling is a promising new approach to\nlanguage modeling that uses extra test-time com-\nputetoimproveperformance. Recently,OpenAIs\no1 model showed this capability but did not pub-\nlicly share its methodology, leading to many repli-\ncation efforts. We seek the simplest approach to\nachievetest-timescalingandstrongreasoningper-\nformance. First, we curate a small datasets1K\nof 1,000 questions paired with reasoning traces\nrelying on three criteria we validate through abla-\ntions: difficulty,diversity,andquality. Second,we\ndevelop budget forcing to control test-time com-\npute by forcefully terminating the models think-\ning process or lengthening it by appending Wait\nmultiple times to the models generation when it\ntries to end. This can lead the model to double-\ncheck its answer, often fixing incorrect reasoning\nsteps. After supervised finetuning the Qwen2.5-\n32B-Instruct language model ons1Kand equip-\nping it with budget forcing, our models1-32B ex-\nceeds o1-preview on competition math questions\nby up to 27% (MATH and AIME24). Further,\nscaling s1-32B with budget forcing allows extrap-\nolating beyond its performance without test-time\nintervention: from 50% to 57% on AIME24. Our\nmodel, data, and code are open-source athttps:\n//github.com/simplescaling/s1.\n1. Introduction\nPerformance improvements of language models (LMs) over\nthe past years have largely relied on scaling up train-time\ncompute using large-scale self-supervised pretraining (Ka-\nplan et al., 2020; Hoffmann et al., 2022). The creation of\nthese powerful models has set the stage for a new scaling\nparadigm built on top of them:test-time scaling. The aim\n*Equal contribution. ZY and NM started the project. WS, NM\nand ZY collected the prompts, XL, ZY and NM, built the data\npipeline, LZ and WS proposed using a 1K subset and ZY and\nNM built budget forcing.1 Stanford University.2 University of\nWashington, Seattle.3 Allen Institute for AI.4 Contextual AI.\n512 2048\n65\n75\n85\n95Accuracy (%)\nMathematical\nProblem Solving\n(MATH500)\n512 2048 8192\n0\n20\n40\n60\nCompetition\nMath\n(AIME24)\n1024 4096\n40\n50\n60\nPhD-Level\nScience Questions\n(GPQA Diamond)\nAverage thinking time (tokens)\nFigure 1.Test-time scaling with s1-32B.We benchmarks1-32B\non reasoning-intensive tasks and vary test-time compute.\nof this approach is to increase the compute at test time to\nget better results. There has been much work exploring this\nidea (Snell et al., 2024; Welleck et al., 2024), and the via-\nbility of this paradigm was recently validated by OpenAI\no1 (OpenAI, 2024). o1 has demonstrated strong reasoning\nperformance with consistent gains from scaling test-time\ncompute. OpenAI describes their approach as using large-\nscalereinforcementlearning(RL)implyingtheuseofsizable\namounts of data (OpenAI, 2024). This has led to various\nattempts to replicate their models relying on techniques like\nMonte Carlo Tree Search (Gao et al., 2024b; Zhang et al.,\n2024a), multi-agent approaches (Qin et al., 2024), and oth-\ners (Wang et al., 2024a; Huang et al., 2024b; 2025). Among\nthese approaches, DeepSeek R1 (DeepSeek-AI et al., 2025)\nhas successfully replicated o1-level performance, also em-\nploying reinforcement learning via millions of samples and\nmultiple training stages. However, despite the large num-\nber of o1 replication attempts, none have openly replicated\na clear test-time scaling behavior. Thus, we ask: what is\nthe simplest approach to achieve both test-time scaling and\nstrong reasoning performance?\nWeshowthattrainingononly1,000sampleswithnext-token\nprediction and controlling thinking duration via a simple\ntest-time technique we refer to asbudget forcingleads to\na strong reasoning model that scales in performance with\nmore test-time compute. Specifically, we constructs1K,\nwhich consists of 1,000 carefully curated questions paired\nwith reasoning traces and answers distilled from Gemini\nThinking Experimental (Google, 2024). We perform super-\nvised fine-tuning (SFT) of an off-the-shelf pretrained model\n1\narXiv:2501.19393v3  [cs.CL]  1 Mar 2025s1: Simple test-time scaling\non our small dataset requiring just 26 minutes of training\non 16 H100 GPUs. After training, we control the amount\nof test-time compute our model spends usingbudget forc-\ning: (I) If the model generates more thinking tokens than\na desired limit, we forcefully end the thinking process by\nappending an end-of-thinking token delimiter. Ending the\nthinking this way makes the model transition to generating\nitsanswer. (II)Ifwewantthemodeltospendmoretest-time\ncompute on a problem, we suppress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our mode\n\nCitation:"}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:33:21,979 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Muennighoff, Niklas, et al. \"s1: Simple Test-Time Scaling.\" *arXiv*, 1 Mar. 2025, arxiv.org/abs/2501.19393v3.\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.025062035631250451
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 1471,
    "candidatesTokenCount": 54,
    "totalTokenCount": 1525,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 1471
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 54
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "odM2aaDuB5W9xN8PpNnkkQ8"
}



2025-12-08 14:33:21,980 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:21,980 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:21,981 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:21,981 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:21,981 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:21,981 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:21,981 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:21,982 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:21,982 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:21,982 - DEBUG - response_cost: 0.0
2025-12-08 14:33:21,982 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:21,982 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:33:21,982 - DEBUG - response_cost: 0.0
2025-12-08 14:33:21,983 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:21,983 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:21,983 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:21,983 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:21,983 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:21,983 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:21,984 - DEBUG - Async Response: ModelResponse(id='odM2aaDuB5W9xN8PpNnkkQ8', created=1765200800, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Muennighoff, Niklas, et al. "s1: Simple Test-Time Scaling." *arXiv*, 1 Mar. 2025, arxiv.org/abs/2501.19393v3.\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=54, prompt_tokens=1471, total_tokens=1525, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=54, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=1471, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:33:21,984 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:33:21,984 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:33:21,985 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:21,985 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:21,985 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:21,985 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:21,985 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:21,986 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:21,987 - DEBUG - Inside async function with retries.
2025-12-08 14:33:21,987 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x109a16d50>>, num_retries - 3
2025-12-08 14:33:21,987 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:21,987 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:21,987 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:21,987 - DEBUG - retrieve cooldown models: []
2025-12-08 14:33:21,987 - DEBUG - async cooldown deployments: []
2025-12-08 14:33:21,987 - DEBUG - cooldown_deployments: []
2025-12-08 14:33:21,987 - DEBUG - cooldown deployments: []
2025-12-08 14:33:21,987 - DEBUG - 

2025-12-08 14:33:21,987 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:33:21,987 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Muennighoff, Niklas, et al. "s1: Simple Test-Time Scaling." *arXiv*, 1 Mar. 2025, arxiv.org/abs/2501.19393v3.\n\n\nCitation JSON:'}], caching=False, client=None, n=1, stream=False, litellm_trace_id='6c69c3f2-5514-41d7-8648-36cff3ee9bd2', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:33:21,987 - DEBUG - 

2025-12-08 14:33:21,988 - DEBUG - self.optional_params: {}
2025-12-08 14:33:21,988 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:33:21,988 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:33:21,989 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:21,989 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x109a30390>>
2025-12-08 14:33:21,989 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:33:21,989 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:21,989 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Muennighoff, Niklas, et al. "s1: Simple Test-Time Scaling." *arXiv*, 1 Mar. 2025, arxiv.org/abs/2501.19393v3.\n\n\nCitation JSON:'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:33:21,989 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:21,989 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:21,989 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:33:21,989 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:21,990 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:21,991 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:21,991 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Muennighoff, Niklas, et al. "s1: Simple Test-Time Scaling." *arXiv*, 1 Mar. 2025, arxiv.org/abs/2501.19393v3.\n\n\nCitation JSON:'}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:33:22,060 - DEBUG - Logging Details: logger_fn - None | callable(logger_fn) - False
2025-12-08 14:33:22,068 - DEBUG - Logging Details LiteLLM-Failure Call: [<bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x106c13850>>, <bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x109a16d50>>]
2025-12-08 14:33:22,071 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:33:22,071 - DEBUG - checks 'should_run_cooldown_logic'
2025-12-08 14:33:22,071 - DEBUG - Attempting to add 8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be to cooldown list
2025-12-08 14:33:22,072 - DEBUG - percent fails for deployment = 8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be, percent fails = 0.3333333333333333, num successes = 2, num fails = 1
2025-12-08 14:33:22,072 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:33:22,072 - DEBUG - checks 'should_run_cooldown_logic'
2025-12-08 14:33:22,072 - DEBUG - Attempting to add 8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be to cooldown list
2025-12-08 14:33:22,072 - DEBUG - percent fails for deployment = 8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be, percent fails = 1.0, num successes = 0, num fails = 1
2025-12-08 14:33:22,075 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[31m Exception litellm.RateLimitError: litellm.RateLimitError: GeminiException - {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota. Please migrate to Gemini 2.5 Flash Image (models/gemini-2.5-flash-image) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_requests_per_model",
            "quotaId": "GenerateRequestsPerMinutePerProjectPerModel",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-2.0-flash-exp"
            },
            "quotaValue": "10"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "37s"
      }
    ]
  }
}
[0m
2025-12-08 14:33:22,075 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:22,075 - DEBUG - retrieve cooldown models: []
2025-12-08 14:33:22,075 - DEBUG - Retrying request with num_retries: 3
2025-12-08 14:33:27,498 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:27,499 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:27,499 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:27,499 - DEBUG - retrieve cooldown models: []
2025-12-08 14:33:27,499 - DEBUG - async cooldown deployments: []
2025-12-08 14:33:27,499 - DEBUG - cooldown_deployments: []
2025-12-08 14:33:27,499 - DEBUG - cooldown deployments: []
2025-12-08 14:33:27,500 - DEBUG - 

2025-12-08 14:33:27,500 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:33:27,500 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Muennighoff, Niklas, et al. "s1: Simple Test-Time Scaling." *arXiv*, 1 Mar. 2025, arxiv.org/abs/2501.19393v3.\n\n\nCitation JSON:'}], caching=False, client=None, n=1, stream=False, litellm_trace_id='6c69c3f2-5514-41d7-8648-36cff3ee9bd2', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None, 'previous_models': [{'exception_type': 'RateLimitError', 'exception_string': 'litellm.RateLimitError: litellm.RateLimitError: GeminiException - {\n  "error": {\n    "code": 429,\n    "message": "You exceeded your current quota. Please migrate to Gemini 2.5 Flash Image (models/gemini-2.5-flash-image) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.",\n    "status": "RESOURCE_EXHAUSTED",\n    "details": [\n      {\n        "@type": "type.googleapis.com/google.rpc.Help",\n        "links": [\n          {\n            "description": "Learn more about Gemini API quotas",\n            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"\n          }\n        ]\n      },\n      {\n        "@type": "type.googleapis.com/google.rpc.QuotaFailure",\n        "violations": [\n          {\n            "quotaMetric": "generativelanguage.googleapis.com/generate_requests_per_model",\n            "quotaId": "GenerateRequestsPerMinutePerProjectPerModel",\n            "quotaDimensions": {\n              "location": "global",\n              "model": "gemini-2.0-flash-exp"\n            },\n            "quotaValue": "10"\n          }\n        ]\n      },\n      {\n        "@type": "type.googleapis.com/google.rpc.RetryInfo",\n        "retryDelay": "37s"\n      }\n    ]\n  }\n}\n', 'n': 1, 'model': 'gemini/gemini-2.0-flash-exp', 'stream': False, 'litellm_trace_id': '6c69c3f2-5514-41d7-8648-36cff3ee9bd2', 'metadata': {'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}}]}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:33:27,500 - DEBUG - 

2025-12-08 14:33:27,501 - DEBUG - self.optional_params: {}
2025-12-08 14:33:27,501 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:33:27,501 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:33:27,502 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:27,502 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:33:27,502 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Muennighoff, Niklas, et al. "s1: Simple Test-Time Scaling." *arXiv*, 1 Mar. 2025, arxiv.org/abs/2501.19393v3.\n\n\nCitation JSON:'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:33:27,502 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:27,502 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:33:27,503 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:27,503 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:27,503 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:27,503 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Muennighoff, Niklas, et al. "s1: Simple Test-Time Scaling." *arXiv*, 1 Mar. 2025, arxiv.org/abs/2501.19393v3.\n\n\nCitation JSON:'}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:33:28,627 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"title\": \"s1: Simple Test-Time Scaling\",\n  \"authors\": [\n    \"Niklas Muennighoff\"\n  ],\n  \"doi\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.0023679534594217935
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 107,
    "candidatesTokenCount": 48,
    "totalTokenCount": 155,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 107
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 48
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "p9M2aZfWMtfUxN8P1KbwuA4"
}



2025-12-08 14:33:28,629 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:28,629 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:28,629 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:28,630 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:28,630 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:28,630 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:28,630 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:28,630 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:28,631 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:28,631 - DEBUG - response_cost: 0.0
2025-12-08 14:33:28,631 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:28,631 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:33:28,631 - DEBUG - response_cost: 0.0
2025-12-08 14:33:28,632 - DEBUG - Async Response: ModelResponse(id='p9M2aZfWMtfUxN8P1KbwuA4', created=1765200807, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='```json\n{\n  "title": "s1: Simple Test-Time Scaling",\n  "authors": [\n    "Niklas Muennighoff"\n  ],\n  "doi": null\n}\n```', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=48, prompt_tokens=107, total_tokens=155, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=48, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=107, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:33:28,632 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:28,632 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:33:28,633 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:28,633 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:28,633 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:28,633 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:28,633 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:33:28,633 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:28,634 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:28,635 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:28,635 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:28,664 - DEBUG - Attempting to populate metadata query: {'authors': ['Niklas Muennighoff'], 'title': 's1: Simple Test-Time Scaling', 'session': <aiohttp.client.ClientSession object at 0x10c627f50>} via providers=[<paperqa.clients.semantic_scholar.SemanticScholarProvider object at 0x109a85f50>, <paperqa.clients.crossref.CrossrefProvider object at 0x109a84ed0>] processors=[<paperqa.clients.journal_quality.JournalQualityPostProcessor object at 0x109a84190>]
2025-12-08 14:33:28,664 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x109a52b50>>
2025-12-08 14:33:28,665 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:28,665 - WARNING - SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.
2025-12-08 14:33:28,665 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:31,701 - DEBUG - connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2025-12-08 14:33:31,721 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x109faf350>
2025-12-08 14:33:31,721 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10a083c80> server_hostname='raw.githubusercontent.com' timeout=5
2025-12-08 14:33:31,746 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x109fafcd0>
2025-12-08 14:33:31,746 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:33:31,746 - DEBUG - send_request_headers.complete
2025-12-08 14:33:31,746 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:33:31,746 - DEBUG - send_request_body.complete
2025-12-08 14:33:31,746 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:33:31,767 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'52540'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3cd138251572ce0662c39b6cee3c3fb9dc040f283d3f3d4afe7b0952ce409f78"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'8389:10C5B:28F09C4:2ED595D:6936D36F'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 08 Dec 2025 13:33:31 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-lin1730049-LIN'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'1'), (b'X-Timer', b'S1765200812.829715,VS0,VE1'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'db90f653281d3a926839e0b3bac27a9c37f7ae63'), (b'Expires', b'Mon, 08 Dec 2025 13:38:31 GMT'), (b'Source-Age', b'60')])
2025-12-08 14:33:31,768 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:33:31,793 - DEBUG - receive_response_body.complete
2025-12-08 14:33:31,793 - DEBUG - response_closed.started
2025-12-08 14:33:31,793 - DEBUG - response_closed.complete
2025-12-08 14:33:31,793 - DEBUG - close.started
2025-12-08 14:33:31,793 - DEBUG - close.complete
2025-12-08 14:33:31,797 - DEBUG - 

2025-12-08 14:33:31,797 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:33:31,797 - DEBUG - [92mlitellm.aembedding(model='text-embedding-3-small', input=['s1: Simple test-time scaling\nNiklas Muennighoff*134 Zitong Yang*1 Weijia Shi*23 Xiang Lisa Li*1 Li Fei-Fei1 Hannaneh Hajishirzi23\nLuke Zettlemoyer2 Percy Liang1 Emmanuel Cands1 Tatsunori Hashimoto1\nAbstract\nTest-time scaling is a promising new approach to\nlanguage modeling that uses extra test-time com-\nputetoimproveperformance. Recently,OpenAIs\no1 model showed this capability but did not pub-\nlicly share its methodology, leading to many repli-\ncation efforts. We seek the simplest approach to\nachievetest-timescalingandstrongreasoningper-\nformance. First, we curate a small datasets1K\nof 1,000 questions paired with reasoning traces\nrelying on three criteria we validate through abla-\ntions: difficulty,diversity,andquality. Second,we\ndevelop budget forcing to control test-time com-\npute by forcefully terminating the models think-\ning process or lengthening it by appending Wait\nmultiple times to the models generation when it\ntries to end. This can lead the model to double-\ncheck its answer, often fixing incorrect reasoning\nsteps. After supervised finetuning the Qwen2.5-\n32B-Instruct language model ons1Kand equip-\nping it with budget forcing, our models1-32B ex-\nceeds o1-preview on competition math questions\nby up to 27% (MATH and AIME24). Further,\nscaling s1-32B with budget forcing allows extrap-\nolating beyond its performance without test-time\nintervention: from 50% to 57% on AIME24. Our\nmodel, data, and code are open-source athttps:\n//github.com/simplescaling/s1.\n1. Introduction\nPerformance improvements of language models (LMs) over\nthe past years have largely relied on scaling up train-time\ncompute using large-scale self-supervised pretraining (Ka-\nplan et al., 2020; Hoffmann et al., 2022). The creation of\nthese powerful models has set the stage for a new scaling\nparadigm built on top of them:test-time scaling. The aim\n*Equal contribution. ZY and NM started the project. WS, NM\nand ZY collected the prompts, XL, ZY and NM, built the data\npipeline, LZ and WS proposed using a 1K subset and ZY and\nNM built budget forcing.1 Stanford University.2 University of\nWashington, Seattle.3 Allen Institute for AI.4 Contextual AI.\n512 2048\n65\n75\n85\n95Accuracy (%)\nMathematical\nProblem Solving\n(MATH500)\n512 2048 8192\n0\n20\n40\n60\nCompetition\nMath\n(AIME24)\n1024 4096\n40\n50\n60\nPhD-Level\nScience Questions\n(GPQA Diamond)\nAverage thinking time (tokens)\nFigure 1.Test-time scaling with s1-32B.We benchmarks1-32B\non reasoning-intensive tasks and vary test-time compute.\nof this approach is to increase the compute at test time to\nget better results. There has been much work exploring this\nidea (Snell et al., 2024; Welleck et al., 2024), and the via-\nbility of this paradigm was recently validated by OpenAI\no1 (OpenAI, 2024). o1 has demonstrated strong reasoning\nperformance with consistent gains from scaling test-time\ncompute. OpenAI describes their approach as using large-\nscalereinforcementlearning(RL)implyingtheuseofsizable\namounts of data (OpenAI, 2024). This has led to various\nattempts to replicate their models relying on techniques like\nMonte Carlo Tree Search (Gao et al., 2024b; Zhang et al.,\n2024a), multi-agent approaches (Qin et al., 2024), and oth-\ners (Wang et al., 2024a; Huang et al., 2024b; 2025). Among\nthese approaches, DeepSeek R1 (DeepSeek-AI et al., 2025)\nhas successfully replicated o1-level performance, also em-\nploying reinforcement learning via millions of samples and\nmultiple training stages. However, despite the large num-\nber of o1 replication attempts, none have openly replicated\na clear test-time scaling behavior. Thus, we ask: what is\nthe simplest approach to achieve both test-time scaling and\nstrong reasoning performance?\nWeshowthattrainingononly1,000sampleswithnext-token\nprediction and controlling thinking duration via a simple\ntest-time technique we refer to asbudget forcingleads to\na strong reasoning model that scales in performance with\nmore test-time compute. Specifically, we constructs1K,\nwhich consists of 1,000 carefully curated questions paired\nwith reasoning traces and answers distilled from Gemini\nThinking Experimental (Google, 2024). We perform super-\nvised fine-tuning (SFT) of an off-the-shelf pretrained model\n1\narXiv:2501.19393v3  [cs.CL]  1 Mar 2025s1: Simple test-time scaling\non our small dataset requiring just 26 minutes of training\non 16 H100 GPUs. After training, we control the amount\nof test-time compute our model spends usingbudget forc-\ning: (I) If the model generates more thinking tokens than\na desired limit, we forcefully end the thinking process by\nappending an end-of-thinking token delimiter. Ending the\nthinking this way makes the model transition to generating\nitsanswer. (II)Ifwewantthemodeltospendmoretest-time\ncompute on a problem, we suppress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our mode', 'ress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our models1-\n32Bexhibits test-time scaling (Figure 1). Further,s1-32Bis\nthe most sample-efficient reasoning model and outperforms\nclosed-source models like OpenAIs o1-preview (Figure 2).\nWe conduct extensive ablation experiments targeting (a) our\nselection of 1,000 (1K) reasoning samples and (b) our test-\ntime scaling. For (a), we find that jointly incorporating\ndifficulty, diversity, and quality measures into our selec-\ntion algorithm is important. Random selection, selecting\nsamples with the longest reasoning traces, or only selecting\nmaximally diverse samples all lead to significantly worse\nperformance (around30% on AIME24 on average). Train-\ning on our full data pool of 59K examples, a superset of\ns1K, does not offer substantial gains over our 1K selection.\nThis highlights the importance of careful data selection and\nechoes prior findings for instruction tuning (Zhou et al.,\n2023). For(b), we define desiderata for test-time scaling\nmethods to compare different approaches. Budget forcing\nleads to the best scaling as it has perfect controllability with\na clear positive slope leading to strong performance.\nInsummary,ourcontributionsare: Wedevelopsimplemeth-\nods for creating a sample-efficient reasoning dataset (2)\nand test-time scaling (3); Based on these we builds1-32B\nwhich is competitive with o1-preview (4); We ablate sub-\ntleties of data (5.1) and test-time scaling (5.2). We end\nwith a discussion to motivate future work on simple rea-\nsoning (6). Our code, model, and data are open-source at\nhttps://github.com/simplescaling/s1.\n2. Reasoning data curation to create s1K\nIn this section, we describe our process for creating a large\ndataset first in 2.1 and then filtering it down tos1Kin 2.2.\n2.1. Initial collection of 59K samples\nWecollectaninitial59,029questionsfrom16sourcesfollow-\ning three guiding principles.Quality: Datasets should be\nhigh-quality; we always inspect samples and ignore datasets\nwith, e.g., poor formatting;Difficulty: Datasets should be\nchallenging and require significant reasoning effort;Diver-\nsity: Datasetsshouldstemfromvariousfieldstocoverdiffer-\nent reasoning tasks. We collect datasets of two categories:\nCuration of existing datasetsOur largest source is Nu-\nminaMATH (LI et al., 2024) with 30,660 mathematical\nproblems from online websites. We also include histori-\ncal AIME problems (1983-2021). To enhance diversity,\nwe add OlympicArena (Huang et al., 2024a) with 4,250\nquestions spanning Astronomy, Biology, Chemistry, Com-\nputer Science, Geography, Mathematics, and Physics from\nvarious Olympiads. OmniMath (Gao et al., 2024a) adds\n4,238 competition-level mathematics problems. We also\ninclude 2,385 problems from AGIEval (Zhong et al., 2023),\nwhich features questions from standardized tests like SAT\nand LSAT, covering English, Law, and Logic. We refer to\nTable 7 in C for our other sources.\nNew datasets in quantitative reasoning To comple-\nment these existing datasets, we create two original\ndatasets. s1-prob consists of 182 questions from the prob-\nability section of Stanford Universitys Statistics Depart-\nments PhD Qualifying Exams (https://statistics.\nstanford.edu), accompanied by handwritten solutions\nthat cover difficult proofs. The probability qualifying exam\nis held yearly and requires professional-level mathemati-\ncal problem-solving. s1-teasers comprises 23 challenging\nbrain-teasers commonly used in interview questions for\nquantitative trading positions. Each sample consists of a\nproblem and solution taken from PuzzledQuant (https:\n//www.puzzledquant.com/). Weonlytakeexamples\nwith the highest difficulty level ("Hard").\nFor each question, we generate a reasoning trace and solu-\ntion using the Google Gemini Flash Thinking API (Google,\n2024)extractingitsreasoningtraceandresponse. Thisyields\n59K triplets of a question, generated reasoning trace, and\ngenerated solution. Examples from our dataset are in D.2.\nWe decontaminate all samples against our evaluation ques-\ntions (MATH500, GPQA Diamond, AIME24; C.5) using\n8-grams and deduplicate the data.\n2.2. Final selection of 1K samples\nWe could directly train on our pool of 59K questions, how-\never, our goal is to find thesimplest approach with minimal\nresources. Thus, we go through three stages of filtering to\narriveataminimalsetof1,000samplesrelyingonourthree\nguiding data principles: Quality, Difficulty, and Diversity.\nQuality We first remove any questions where we ran into\nanyAPIerrorsreducingourdatasetto54,116samples. Next,\nwefilteroutlow-qualityexamplesbycheckingiftheycontain\nany string patterns with formatting issues, such as ASCII\nart diagrams, non-existent image references, or inconsistent\nquestionnumberingreducingourdatasetto51,581examples.\n2s1: Simple test-time scaling\nGeometry', '-qualityexamplesbycheckingiftheycontain\nany string patterns with formatting issues, such as ASCII\nart diagrams, non-existent image references, or inconsistent\nquestionnumberingreducingourdatasetto51,581examples.\n2s1: Simple test-time scaling\nGeometry Number\ntheory\nCombin\natorics\nReal\nfunctions\nBiology\nComplex\nfunctions\nQuantum\ntheory\nField\ntheory\nCalculus of\nvariations\nDifference\nequations\nElectro\ndynamics\nGroup\ntheory\nLinear\nalgebra\nProbability\ntheory\nAlgebraic\nsystems\nMechanics\nThermo\ndynamics\nDifferential\nequations\nComputer\nscience\nNumerical\nanalysis\nCalculus\nAlgebraic\nstructures\nAstronomy\nDynamical\nsystems\nStatistical\nmechanics\nOperations\nresearch\nMath\nematics\neducation\nMeasure\ntheory\nConvex\ngeometry\nFluid\nmechanics\nAlgebraic\ngeometry\nStatistics General\ntopology\nEconomics\nAssociative\nrings\nGeneral\nrelativity\nDifferential\ngeometry\nMath\nematical\nlogic\nPartial\ndifferential\nequations\nInformation\ntheory\nSolid\nmech\nanics\nFunctional\nanalysis\nSpecial\nfunctions\nComm\nutative\nalgebra\nIntegral\nequations\nIntegral\ntransform\nApproxi\nmation\ntheory\nPotential\ntheory\nHarmonic\nanalysis\nControl\ntheory\nGeo\nphysics\n1000 17000 800000 N/A\nNumber of Examples\n80\n85\n90\n95\n100MATH500 Accuracy (%)\ns1\nr1-distill\nSky-T1\nQwQ\nBespoke-Stratos\no1-preview\nMost sample-efficient\nFigure 2.s1Kands1-32B. (left)s1Kisadatasetof1,000high-quality,diverse,anddifficultquestionswithreasoningtraces. (right)s1-32B,\na 32B parameter model finetuned ons1K is on the sample-efficiency frontier. See Table 1 for details on other models.\nFrom this pool, we identify 384 samples for our final 1,000\nsamples from datasets that we perceive as high-quality and\nnot in need of further filtering (see C.4 for details).\nDifficulty Fordifficulty, weusetwoindicators: modelper-\nformance and reasoning trace length. We evaluate two mod-\nels on each question: Qwen2.5-7B-Instruct and Qwen2.5-\n32B-Instruct (Qwen et al., 2024), with correctness assessed\nby Claude 3.5 Sonnet comparing each attempt against the\nreference solution (see C.3 for the grading protocol). We\nmeasure the token length of each reasoning trace to indicate\nproblem difficulty using the Qwen2.5 tokenizer. This relies\non the assumption that more difficult problems require more\nthinkingtokens. Basedonthegrading,weremovequestions\nthat either Qwen2.5-7B-Instruct or Qwen2.5-32B-Instruct\ncan solve correctly and thus may be too easy. By using two\nmodels we reduce the likelihood of an easy sample slipping\nthrough our filtering due to a rare mistake on an easy ques-\ntion of one of the models. This brings our total samples\ndown to 24,496, setting the stage for the next round of sub-\nsampling based on diversity. While filtering with these two\nmodels may be optimized for our setup as we will also use\nQwen2.5-32B-Instruct as our model to finetune, the idea of\nmodel-based filtering generalizes to other setups.\nDiversity To quantify diversity, we classify questions into\ndomainsusingClaude3.5SonnetbasedontheMathematics\nSubject Classification (MSC) system (e.g., geometry, com-\nbinatorics, etc.) from the American Mathematical Society.1\nThe taxonomy focuses on topics in mathematics but also\nincludes other sciences such as biology, physics, and eco-\n1https://mathscinet.ams.org/mathscinet/\nmsc/msc2020.html\nnomics. Toselectourfinalexamplesfromthepoolof24,496\nquestions, we first choose one domain uniformly at random.\nThen, we sample one problem from this domain according\ntoadistributionthatfavorslongerreasoningtraces(seeC.4\nfordetails)asmotivatedin Difficulty. Werepeatthisprocess\nuntil we have 1,000 total samples spanning 50 domains.\nIn 5.1, we will show that using our three criteria in combi-\nnation is important, as only relying on quality, diversity, or\ndifficulty in isolation leads to worse datasets. Some distilled\ngenerations are incorrect, which we allow in our data as we\nfocusoncapturingthereasoningprocessratherthanentirely\ncorrect solutions. Our grader (C.3) deems 53.6% correct\nin s1Kand 63.0% in our follow-ups1K-1.1(see A).\n3. Test-time scaling\n3.1. Method\nWe classify test-time scaling methods into1) Sequential,\nwherelatercomputationsdependonearlierones(e.g.,along\nreasoningtrace),and 2)Parallel,wherecomputationsrunin-\ndependently(e.g.,majorityvoting)(Snelletal.,2024;Brown\netal.,2024). Wefocusonsequentialscalingasintuitivelywe\nbelieve it should scale better, since later computations can\nbuild on intermediate results, allowing for deeper reasoning\nanditerativerefinement. Weproposenewsequentialscaling\nmethods and ways to benchmark them.\nBudget forcing We propose a simple decoding-time in-\ntervention by forcing a maximum and/or minimum number\nof thinking tokens. Specifically, we enforce a maximum\ntoken count by simply appending the end-of-thinking token\ndelimiter and optionally Final Answer: to early exit\n3s1: Simple test-time scaling\nHo w many r in r aspberry ?\nLet \' s br eak do wn the pr ocess of counting the letter \' r \' in the \nwor d " r aspberry " ...\n* First letter: \' r \' - This is an \' r \', count = 1.\n* Secon', 'nally Final Answer: to early exit\n3s1: Simple test-time scaling\nHo w many r in r aspberry ?\nLet \' s br eak do wn the pr ocess of counting the letter \' r \' in the \nwor d " r aspberry " ...\n* First letter: \' r \' - This is an \' r \', count = 1.\n* Second letter: \' a \' - Not an \' r \', count r emains 1 ...\n* Sixth letter: \' e \' - Not an \' r \', count r emains 1.\n* Se venth letter: \' r \' - This is an \' r \', count = 2.\n* Eighth letter: \'y \' - Not an \' r \', count r emains 2 ...\nThe number of \' r \' s in " r aspberry " is 2.\n, let \' s r e-r ead the question car efully . It asks "Ho w many r \nin r aspberry ?" ... * r - a - s - p - b - e - r - r - y ... * First \' r \' ... * \nSecond \' r \' ... * Thir d \' r \' ... Count = 3 ...\nW ait\nMy initial answer of 2 was incorr ect due to a quick r eading of \nthe wor d. Final Answer: The final answer is\nQuestion\nReasoning tr ace\nResponse\nFigure 3.Budget forcing with s1-32B.The model tries to stop\nafter ...is 2., but we suppress the end-of-thinking token delimiter\ninstead appending Wait leadings1-32Bto self-correct its answer.\nthe thinking stage and make the model provide its current\nbest answer. To enforce a minimum, we suppress the gener-\nation of the end-of-thinking token delimiter and optionally\nappend the string Wait to the models current reasoning\ntrace to encourage the model to reflect on its current gen-\neration. Figure 3 contains an example of how this simple\napproach can lead the model to arrive at a better answer.\nBaselines We benchmark budget forcing with:(I) Con-\nditional length-control methods, which rely on telling the\nmodel in the prompt how long it should generate for. We\ngroup them by granularity into (a) Token-conditional con-\ntrol: We specify an upper bound of thinking tokens in the\nprompt; (b) Step-conditional control: We specify an upper\nbound of thinking steps, where each step is around 100 to-\nkens; (c) Class-conditional control: We write two generic\npromptsthattellthemodeltoeitherthinkforashortorlong\namount of time (see E.1 for details).(II) Rejection sam-\npling, which samples until a generation fits a predetermined\ncompute budget. This oracle captures the posterior over\nresponses conditioned on its length.\n3.2. Metrics\nWe establish a set of desiderata as evaluation metrics to\nmeasure test-time scaling across methods. Importantly, we\ndo not only care about the accuracy a method can achieve\nbut also its controllability and test-time scaling slope. For\neach method we consider, we run a set of evaluations\n\ue22d varying test-time compute on a fixed benchmark, e.g.\nAIME24. This produces a piece-wise linear function\nwith compute as the x-axis measured in thinking tokens and\naccuracyasthey-axis(seeFigure1,wheretherightmostdot\nfor AIME24 corresponds to(7320)=57% ). We measure\nthree metrics:\nControl = 1\n|\ue22d|\n\n\ue22d\n(min   max) (1)\nwhere min,max refer to a pre-specified minimum and max-\nimum amount of test-time compute; in our case thinking\ntokens. We usually only constrainmax. As tokens gener-\nated correspond to the amount of test-time compute spent,\nthis metric measures the extent to which a method allows\ncontrollability over the use of that test-time compute. We\nreport it as a percentage with 100% being perfect control.\nScaling = 1(|\ue22d|\n2\n)\n\n,\ue22d\n>\n() ()\n (2)\nScalingistheaverageslopeofthepiece-wiselinearfunction.\nIt must be positive for useful methods and larger is better.\nPerformance =max\n\ue22d\n() (3)\nPerformance is simply the maximum performance the\nmethod achieves on the benchmark. A method with mono-\ntonically increasing scaling achieves 100% performance on\nany benchmark in the limit. However, the methods we in-\nvestigate eventually flatten out or further scaling fails due to\ncontrol or context window limitations.\n4. Results\n4.1. Setup\nTraining We perform supervised finetuning on Qwen2.5-\n32B-Instruct usings1Kto obtain our models1-32Busing\nbasic hyperparameters outlined in D. Finetuning took 26\nminutes on 16 NVIDIA H100 GPUs with PyTorch FSDP.\nEvaluation We select three representative reasoning\nbenchmarks widely used in the field:AIME24(of America,\n2024)has30problemsthatwereusedinthe2024American\nInvitational Mathematics Examination (AIME) held from\nJanuary 31  February 1, 2024. AIME tests mathematical\nproblem-solving with arithmetic, algebra, counting, geome-\ntry, number theory, probability, and other secondary school\nmathtopics. High-scoringhighschoolstudentsinthetestare\ninvitedtoparticipateintheUnitedStatesofAmericaMathe-\nmaticsOlympiad(USAMO).AllAIMEanswersareintegers\nranging from000to999, inclusive. Some AIME problems\nrely on figures that we provide to our model using the vec-\ntor graphics language Asymptote as it cannot take image\ninputs. MATH500(Hendrycks et al., 2021) is a benchmark\nof competition math problems of varying difficulty. We\nevaluate on the same 500 samples selected by OpenAI in\nprior work (Lightman et al., 2023).GPQA Diamond(Rein\net al., 2023) consists of 198 PhD-level science questions\n4s1: Simple test-time scaling\nF', ' competition math problems of varying difficulty. We\nevaluate on the same 500 samples selected by OpenAI in\nprior work (Lightman et al., 2023).GPQA Diamond(Rein\net al., 2023) consists of 198 PhD-level science questions\n4s1: Simple test-time scaling\nFor cing 2048/4096 max thinking \ntokens\nIgnoring end-of-thinking 2x/4x/6x and \nappending  W ait  thus for cing it to continue \nr easoning when it tries to stop\n(a) Sequential scaling via budget forcing\nPar allel scaling via Majority \nvoting with base model \n(Qwen2.5-32B-Instruct)\nSequential scaling \nvia For cing with s1\n60% accur acy (b) Parallel scaling via majority voting\nFigure 4.Sequential and parallel test-time scaling.(a): Budget forcing shows clear scaling trends and extrapolates to some extent. For\nthe three rightmost dots, we prevent the model from stopping its thinking 2/4/6 times, each time appending Wait to its current reasoning\ntrace. (b): For Qwen2.5-32B-Instruct we perform 64 evaluations for each sample with a temperature of 1 and visualize the performance\nwhen majority voting across 2, 4, 8, 16, 32, and 64 of these.\nfromBiology,ChemistryandPhysics. ExpertswithPhDsin\nthe corresponding domains only achieved 69.7% on GPQA\nDiamond (OpenAI, 2024). When we write GPQA in the\ncontext of evaluation in this work, we always refer to the\nDiamond subset. We build on the lm-evaluation-harness\nframework (Gao et al., 2021; Biderman et al., 2024). Unless\notherwise specified, we evaluate with a temperature of 0\n(greedy) and measure accuracy (equivalent to pass@1).\nOther models We benchmarks1-32Bagainst: OpenAI\no1 series(OpenAI, 2024), closed-source models that pop-\nularized test-time scaling;DeepSeek r1 series(DeepSeek-\nAI et al., 2025), open-weight reasoning models with up to\no1-level performance; QwensQwQ-32B-preview(Team,\n2024), a 32B open-weight reasoning model without dis-\nclosed methodology;Sky-T1-32B-Preview(Team, 2025)\nandBespoke-32B(Labs, 2025), open models with open rea-\nsoningdatadistilledfromQwQ-32B-previewandr1; Google\nGemini 2.0 Flash Thinking Experimental(Google, 2024),\nthe API that we distill from. As it has no official evaluation\nscores, we use the Gemini API to benchmark it ourselves.\nHowever, the recitation error of the Gemini API makes\nevaluation challenging.2 We circumvent this, by manually\ninsertingall30AIME24questionsinitswebinterfacewhere\ntheerrordoesnotappear. However,weleaveoutMATH500\n(500 questions) and GPQA Diamond (198 questions), thus\nthey are N.A. in Table 1. Our model,s1-32B, is fully open\nincluding weights, reasoning data, and code.\n2https://github.com/google/\ngenerative-ai-docs/issues/257\nTable 1.s1-32B is a strong open reasoning model.We evaluate\ns1-32B, Qwen, and Gemini (some entries are unknown (N.A.), see\n4). Other results are from the respective reports (Qwen et al.,\n2024; Team, 2024; OpenAI, 2024; DeepSeek-AI et al., 2025; Labs,\n2025; Team, 2025). # ex. = number examples used for reasoning\nfinetuning; BF = budget forcing. See A for our betters1.1 model.\nModel # ex. AIME\n2024\nMATH\n500\nGPQA\nDiamond\nAPI only\no1-preview N.A. 44.6 85.5 73.3\no1-mini N.A. 70.0 90.0 60.0\no1 N.A. 74.4 94.8 77.3\nGemini 2.0 N.A. 60.0 N.A. N.A.Flash Think.\nOpen Weights\nQwen2.5- N.A. 26.7 84.0 49.032B-Instruct\nQwQ-32B N.A. 50.0 90.6 54.5\nr1 800K 79.8 97.3 71.5\nr1-distill 800K 72.6 94.3 62.1\nOpen Weights and Open Data\nSky-T1 17K 43.3 82.4 56.8\nBespoke-32B 17K 63.3 93.0 58.1\ns1 w/o BF 1K 50.0 92.6 56.6\ns1-32B 1K 56.7 93.0 59.6\n5s1: Simple test-time scaling\n4.2. Performance\nTest-time scaling Figure 1 shows the performance ofs1-\n32Bwithbudgetforcingscaleswithmoretest-timecompute.\nInFigure4(left),weexpandtheplotfromFigure1(middle)\nshowing that while we can improve AIME24 performance\nusing our budget forcing technique (3) and more test-time\ncomputeitdoeseventuallyflattenoutatsixtimes. Suppress-\ningtheend-of-thinkingtokendelimitertoooftencanleadthe\nmodel into repetitive loops instead of continued reasoning.\nIn Figure 4 (right), we show that after training Qwen2.5-\n32B-Instruct on our 1,000 samples to produces1-32Band\nequipping it with the simple budget forcing technique, it\noperates in a different scaling paradigm. Scaling test-time\ncomputeonthebasemodelviamajorityvotingcannotcatch\nup with the performance ofs1-32B which validates our\nintuition from 3 that sequential scaling is more effective\nthan parallel. We provide example generations ofs1-32B in\nFigure 5.\nSample-efficiency In Figure 2 (right) and Table 1 we\ncompare s1-32B with other models. We find that s1-\n32B is the most sample-efficient open data reasoning\nmodel. It performs significantly better than our base model\n(Qwen2.5-32B-Instruct) despite just training it on an ad-\nditional 1,000 samples. The concurrently released r1-32B\nshows stronger performance thans1-32B while also only\nusingSFT(DeepSeek-AIetal.,2025). However,itistrained\non 800more reasoning samples. It is an open question\nwhether one can achieve their performance with just 1,000\nsamples. Finally, our model nearly matches Gemin', 'rformance thans1-32B while also only\nusingSFT(DeepSeek-AIetal.,2025). However,itistrained\non 800more reasoning samples. It is an open question\nwhether one can achieve their performance with just 1,000\nsamples. Finally, our model nearly matches Gemini 2.0\nThinking on AIME24. As the data fors1-32B is distilled\nfrom Gemini 2.0, this shows our distillation procedure was\nlikely effective.\n5. Ablations\n5.1. Data Quantity, Diversity, and Difficulty\nIn 2 we outlined our three guiding principles in curating\ns1K: Quality, Difficulty, and Diversity. Here we test the\nimportanceofcombiningthemandtheoverallefficacyofour\nselection. Only Quality (1K-random): After obtaining our\nhigh-quality reasoning chains from Gemini, we select 1,000\nsamplesatrandom;notrelyingonourdifficultyanddiversity\nfiltering at all. Table 2 shows this approach performs much\nworse thans1K across all benchmarks. Only Diversity\n(1K-diverse): For this dataset, we sample uniformly across\ndomains to maximize diversity disregarding any notion of\ndifficulty. This approach also leads to poor performance\nsimilar to 1K-random.Only Difficulty (1K-longest): Here\nwerelyononeofourdifficultyindicatorsintroducedin2by\nselectingthe1,000sampleswiththelongestreasoningtraces.\nThis approach significantly boosts GPQA performance but\noverall still falls short of usings1K.Maximize Quantity:\nTable 2.s1K data ablations.We budget force (BF) a maximum\nof around 30,000 thinking tokens for all scores in this table. This\nperforms slightly better than the scores without BF (Table 1) as\nit allows the model to finish with a best guess when stuck in an\ninfinite loop. We report 95% paired bootstrap confidence intervals\nfor differences relative to thes1K model using 10,000 bootstrap\nsamples. E.g., the interval [-13%, 20%] means that, with 95% con-\nfidence, the true difference between 59K-full ands1Kis between\n-13% and +20%. If the entire interval is negative, e.g. [-27%, -3%],\nwe can confidently say that the performance is worse thans1K.\nModel AIME\n2024\nMATH\n500\nGPQA\nDiamond\n1K-random 36.7 90.6 52.0\n[-26.7%, -3.3%] [-4.8%, 0.0%] [-12.6%, 2.5%]\n1K-diverse 26.7 91.2 54.6\n[-40.0%, -10.0%] [-4.0%, 0.2%] [-10.1%, 5.1%]\n1K-longest 33.3 90.4 59.6\n[-36.7%, 0.0%] [-5.0%, -0.2%] [-5.1%, 10.1%]\n59K-full 53.3 92.8 58.1\n[-13.3%, 20.0%] [-2.6%, 2.2%] [-6.6%, 8.6%]\ns1K 50.0 93.0 57.6\nFinally, we compare with just training on all of our 59K\nsamples, a superset of all the 1K-sample versions. This\nleads to a strong model but uses much more resources. To\nfinetune on 59K samples, we use 394 H100 GPU hours\nwhiles1-32Bonly required 7 H100 GPU hours. Moreover,\nrelyingonlyon s1Kisextremelycompetitiveasshownin2.\nOverall, combining all three criteria Quality, Difficulty,\nDiversity  via our methodology in 2 is key for sample-\nefficient reasoning training.\n5.2. Test-time scaling methods\nTable 3.Ablations on methods to scale test-time compute on\nAIME24. |\ue22d| refers to the number of evaluation runs used to\nestimate the properties; thus a higher value indicates more robust-\nness. Bold indicates our chosen method and the best values. BF\n= budget forcing, TCC/SCC/CCC = token/step/class-conditional\ncontrol, RS = rejection sampling.\nMethod Control Scaling Performance |\ue22d|\nBF 100% 15 56.7 5\nTCC 40% -24 40.0 5\nTCC + BF 100% 13 40.0 5\nSCC 60% 3 36.7 5\nSCC + BF 100% 6 36.7 5\nCCC 50% 25 36.7 2\nRS 100% -35 40.0 5\n6s1: Simple test-time scaling\n[AIME24]\nAlice and Bob play the following game. A stack of\ntokens lies before them. The players take turns\nwith Alice going first. On each turn, the player\nremoves either1token or4tokens from the stack.\nWhoever removes the last token wins. Find the\nnumber of positive integersless than or equal to\n2024for which there exists a strategy for Bob that\nguarantees that Bob will win the game regardless\nof Alices play.\nThe problem asks for the number of positive integers\n 2024such that Bob has a winning strategy. This\nis a typical impartial game that can be analyzed using\nthe Sprague-Grundy theorem or by directly finding\nthe winning and losing positions. Let be the set of\nwinning positions (number of tokenssuch that the\nplayer whose turn it is can force a win) andbe the\nset of losing positions. [...]\nLet()be the statement "the player whose turn it is\nwhen there aretokens has a winning strategy.(0)\nis false (the player whose turn it is and faces0tokens\nhas already lost). Consider the small values of:\n = 1: The player whose turn it is can take1token\nand win. So(1)is true.1 .\n = 2: The player whose turn it is can take1token,\nleaving1token for the opponent. Since1 , the\nopponent wins. So(2)is false.2 .\n = 3: The player whose turn it is can take1token,\nleaving2tokens for the opponent. Since2 , the\nplayer whose turn it is wins. So(3)is true. 3 \n.[...]\nWe are looking for the number of positive integers\n 2024for which Bob has a winning strategy. This\nmeans that if the game starts withtokens, and Alice\ngoes first, Bob can win. This happens if and only if\nis a losing pos', 'So(3)is true. 3 \n.[...]\nWe are looking for the number of positive integers\n 2024for which Bob has a winning strategy. This\nmeans that if the game starts withtokens, and Alice\ngoes first, Bob can win. This happens if and only if\nis a losing position (because if, then Alices\nfirst move must lead to a winning position for Bob).\n[...]\nLets try to find a pattern in the losing positions\n={0,2,5,7,10,12,15,17,20,22,...}. Differences\nbetween consecutive terms: 2,3,2,3,2,3,2,3,2,...\nThe pattern of differences is alternating2,3. [...]\nThe losing positions are of the form5or 5+2,\nwhere  0. We need to find the number of positive\nintegers 2024thatareinthesetoflosingpositions\n. [...]\nCase 1:  = 5, where  1. 1  5  2024\n15    20245 = 404.8Since is an integer,\n1  404. The number of values ofis404.\nCase 2: = 5+2, where  0. 5+2  2024\n5  2022  20225 = 404.4Since is an in-\nteger, 0    404. The number of values ofis\n4040+1=405 .\nThe total number of positive integers  2024that\nare losing positions is404+405= 809 .\n[MATH500]\nFind the projection ofonto  =(2\n6\n3\n)\nif=8.\nTheprojectionofvector  ontovector\n is given by the formula:\nproj =  \n2\nWearegiven  =\n(2\n6\n3\n)\nand  =8.\nFirst, we need to find the magnitude\nsquared of, which is2.\n2 = =22+62+32 =4+36+9=49 .\nNow we can plug the values into the\nprojection formula:\nproj = 8\n49\nSubstitute the vector:\nproj = 8\n49\n(2\n6\n3\n)\nMultiply the scalar by each compo-\nnent of the vector:\nproj =\n\n\n\n\n8\n49 2\n8\n49 6\n8\n49 3\n\n\n\n\n=\n\n\n\n\n16\n4948\n4924\n49\n\n\n\n\nFinal Answer: The final answer is\n\n\n\n\n16\n4948\n4924\n49\n\n\n\n\n[GPQA]\nA quantum mechanical particle of\nmassmoves in two dimensions in\nthe following potential, as a function\nof the polar coordinates(,):\n(,)= 1\n22 + 3\n22cos2()\nFind the energy spectrum. Hint:\nWrite the potential in Cartesian co-\nordinates.\nThe problem asks for the energy spec-\ntrum of a particle in a given 2D po-\ntential. The potential is given in polar\ncoordinates(,):\n(,)= 1\n22 + 3\n22cos2().\nWe can rewrite the potential as:\n(,)= 1\n22(1+3cos 2()).\nThe Hamiltonian for a particle in 2D\nis given by:\n = 2\n2\n(\n2\n2 + 2\n2\n)\n+(,).\nIn polar coordinates(,), the Lapla-\ncian operator is:\n2 = 1\n\n\n\n(\n \n\n)\n+ 1\n2\n2\n2.\nSo the Hamiltonian in polar coordi-\nnates is:\n = 2\n2\n[\n1\n\n\n\n(\n \n\n)\n+ 1\n2\n2\n2\n]\n+(,).\nThe time-independent Schrdinger\nequation is = . We can try\nto separate variables by assuming a so-\nlution of the form(,)= ()().\n[...]\nThis confirms the energy spectrum is:\n =\n\n\n\n(\n2 + + 3\n2\n)\n.\nFinal Answer:  =\n\n\n\n\n(\n2 + + 3\n2\n)\n.\nFigure 5.Example model outputs.We pick a question from AIME24 (left), MATH500 (middle), and GPQA (right), where our model\ngenerates the correct answer. The black text is the prompt, the light blue text is the reasoning trace, and the blue text is the answer of\ns1-32B. The gray ellipsis [...] indicates that the text was trimmed to fit this page, but the generated text is actually longer.\n7s1: Simple test-time scaling\nBudget forcing In Table 3 we compare the test-time scal-\ning methods we have introduced in 3. Overall, we find that\nbudget forcingprovides perfect control, good scaling, and\nleads to our best AIME24 score. Thus, this is the method\nwe use fors1-32B in Figure 1 and in 4. In Table 4, we\ncomparedifferentstringsforextrapolatingperformance. We\nfind that Wait generally gives the best performance.\nClass-conditional controlWe provide benchmark scores\nfor this method in E.1 and summarize three findings here:\n(1)Token-conditionalcontrolfailswithoutbudgetforcing,as\nour model cannot reliably count tokens - even when trained\nto do so.(2) Under step-conditional control, the model gen-\nerates a similar total number of tokens when given different\nstep targets, as the model goes from few steps with many\ntokens per step, to many steps with few tokens in each step.\nThus, the model learns to hack its way around the com-\npute constraint making the controllability of this method\nmediocre. (3) Class-conditional control can work - telling a\nmodel to simply think longer can increase its test-time com-\npute and performance, which leads good scaling in Table 3.\nTable 4.Budget forcing extrapolation ablations.We compare\nignoring the end-of-thinking delimiter twice and appending none\nor various strings.\nModel AIME\n2024\nMATH\n500\nGPQA\nDiamond\nNo extrapolation 50.0 93.0 57.6\n2x without string 50.0 90.2 55.1\n2x Alternatively 50.0 92.2 59.6\n2x Hmm 50.0 93.0 59.6\n2x Wait 53.3 93.0 59.6\nRejectionsampling Surprisingly,wefindthatsimplysam-\npling until the generation fits a specific length leads to an\ninverse scaling trend as depicted in Figure 6. In E.2 we in-\n3072 4096 5120\nAverage thinking time (tokens)\n22\n27\n32\n37\n42Accuracy (%)\nFigure 6.Rejection sampling on AIME24 with s1-32B.We sam-\nplewithatemperatureof1untilallgenerationshavelessthan(from\nleft to right) 3500, 4000, 5000,', 'in Figure 6. In E.2 we in-\n3072 4096 5120\nAverage thinking time (tokens)\n22\n27\n32\n37\n42Accuracy (%)\nFigure 6.Rejection sampling on AIME24 with s1-32B.We sam-\nplewithatemperatureof1untilallgenerationshavelessthan(from\nleft to right) 3500, 4000, 5000, 8000, and 16000 thinking tokens\nrequiring an average of 655, 97, 8, 3, 2, and 1 tries per sample.\nspectaquestion,whichwasansweredcorrectlybythemodel\nwhen rejection sampling for 4000, but not for the 8000\ntokensetting. Inthe  4000settingthemodeldirectlyjumps\nto the correct approach, while for the 8000setting it back-\ntracks a lot. We hypothesize that there is a correlation such\nthat shorter generations tend to be the ones where the model\nwas on the right track from the start, whereas longer ones\ntend to be ones where the model made mistakes and thus\nbacktracks or questions itself. This leads to longer samples\noften being wrong when rejection sampling and thus the\ninverse scaling trend.\n6. Discussion and related work\n6.1. Sample-efficient reasoning\nModels There are a number of concurrent efforts to build\nmodelsthatreplicatetheperformanceofo1(OpenAI,2024).\nFor example, DeepSeek-r1 and k1.5 (DeepSeek-AI et al.,\n2025; Team et al., 2025) are built with reinforcement learn-\ning methods, while others rely on SFT using tens of thou-\nsands of distilled examples (Team, 2025; Xu et al., 2025;\nLabs, 2025). We show that SFT on only 1,000 examples\nsuffices to build a competitive reasoning model matching\no1-preview and produces a model that lies on the pareto\nfrontier (Figure 2). Further, we introduce budget forcing\nwhich combined with our reasoning model leads to the first\nreproduction of OpenAIs test-time scaling curves (OpenAI,\n2024). Why does supervised finetuning on just 1,000 sam-\nples lead to such performance gains? We hypothesize that\nthe model is already exposed to large amounts of reason-\ning data during pretraining which spans trillions of tokens.\nThus, the ability to perform reasoning is already present in\nour model. Our sample-efficient finetuning stage just acti-\nvatesitandwescaleitfurtherattesttimewithbudgetforcing.\nThis is similar to the "Superficial Alignment Hypothesis"\npresented in LIMA (Zhou et al., 2023), where the authors\nfind that 1,000 examples can be sufficient to align a model\nto adhere to user preferences.\nBenchmarks and methods To evaluate and push the lim-\nits of these models, increasingly challenging benchmarks\nhave been introduced, such as Olympiad-level science com-\npetitions (He et al., 2024; Jain et al., 2024; Zhong et al.,\n2023)andothers(Srivastavaetal.,2023;Glazeretal.,2024;\nSu et al., 2024; Kim et al., 2024; Phan et al., 2025). To\nenhancemodelsperformanceonreasoning-relatedtasks,re-\nsearchers have pursued several strategies: Prior works have\nexplored continuing training language models on special-\nized corpora related to mathematics and science (Azerbayev\netal.,2023;Yangetal.,2024),sometimesevensynthetically\ngenerated data (Yu et al., 2024). Others have developed\ntraining methodologies specifically aimed at reasoning per-\nformance (Zelikman et al., 2022; 2024; Luo et al., 2025;\n8s1: Simple test-time scaling\nYuanetal.,2025;Wuetal.,2024a). Anothersignificantline\nof work focuses on prompting-based methods to elicit and\nimprove reasoning abilities, including methods like Chain-\nof-Thought prompting (Wei et al., 2023; Yao et al., 2023a;b;\nBi et al., 2023; Fu et al., 2023; Zhang et al., 2024b; Xi-\nang et al., 2025; Hu et al., 2024; Diao et al., 2024). These\ncombined efforts aim to advance the reasoning ability of\nlanguage models, enabling them to handle more complex\nand abstract tasks effectively.\n6.2. Test-time scaling\nMethods Asweintroducein3,wedifferentiatetwometh-\nodstoscaletest-timecompute: parallelandsequential. The\nformer relies on multiple solution attempts generated in par-\nallel and selecting the best outcome via specific criteria.\nThese criteria include choosing the most frequent response\nfor majority voting or the best response based on an exter-\nnal reward for Best-of-N (Brown et al., 2024; Irvine et al.,\n2023; Levi, 2024). Unlike repeated sampling, previous se-\nquential scaling methods let the model generate solution\nattempts sequentially based on previous attempts, allowing\nit to refine each attempt based on previous outcomes (Snell\net al., 2024; Hou et al., 2025; Lee et al., 2025). Tree-based\nsearch methods (Gandhi et al., 2024; Wu et al., 2024b) offer\na hybrid approach between sequential and parallel scaling,\nsuch as Monte-Carlo Tree Search (MCTS) (Liu et al., 2024;\nZhang et al., 2023; Zhou et al., 2024; Choi et al., 2023) and\nguided beam search (Xie et al., 2023).REBASE (Wu et al.,\n2024b) employs a process reward model to balance exploita-\ntion and pruning during tree search. Empirically,REBASE\nhas been shown to outperform sampling-based methods and\nMCTS (Wu et al., 2024b). Reward models (Lightman et al.,\n2023; Wang et al., 2024b;c) play a key role in these meth-\nods. They come in two variants: outcome reward models\nand p', 'Empirically,REBASE\nhas been shown to outperform sampling-based methods and\nMCTS (Wu et al., 2024b). Reward models (Lightman et al.,\n2023; Wang et al., 2024b;c) play a key role in these meth-\nods. They come in two variants: outcome reward models\nand process reward models. Outcome reward models (Xin\net al., 2024; Ankner et al., 2024) assign a score to complete\nsolutions and are particularly useful in Best-of-N selection,\nwhile process reward models (Lightman et al., 2023; Wang\net al., 2024b; Wu et al., 2024b) assess individual reasoning\nsteps and are effective in guiding tree-based search methods.\nLimits to further test-time scalingWe have shown that\nbudget forcing allows extrapolating test-time compute in\n4, e.g., improving AIME24 performance from 50% to 57%.\nHowever, it has two key limitations when scaling further: it\neventuallyflattens out(Figure 4), and thecontext window\nof the underlying language model constrains it. Despite\nthese constraints, our work shows test-time scaling across a\nwide range of accuracies (Figure 1), partly because scaling\ndown test-time compute behaves predictably and does not\nsuffer from these constraints.\nContinuing test-time scaling will require approaches that\ncan further extrapolate test-time compute. How can we get\n2048 8192 32768 131072\nAverage thinking time (tokens)\n30\n40\n50\n60Accuracy (%)\nREBASE\nMajority\nSequential\nscaling\nFigure 7.Scalingfurtherwithparallelscalingmethods. Allmet-\nrics averaged over the 30 questions in AIME24. Average thinking\ntokensfor REBASE donotaccountfortheadditionalcomputefrom\nthe reward model. For sequential scaling, we prompt the model\nto use up to (from left to right) 32, 64, 256, and 512 steps. For\nREBASE and majority voting we generate 16 parallel trajectories\nto aggregate across.\nsuch extrapolation? There may be improvements to budget\nforcing such as rotating through different strings, not only\nWait, or combining it with frequency penalties or higher\ntemperature to avoid repetitive loops. An exciting direc-\ntion for future work is also researching whether applying\nbudget forcing to a reasoning model trained with reinforce-\nmentlearningyieldsbetterextrapolation; orifRLallowsfor\nnew ways of test-time scaling beyond budget forcing. Our\nwork definestheright metrics (3.2) Control, Scaling, and\nPerformance  to enable future research and progress on\nextrapolating test-time compute.\nParallel scaling as a solutionParallel scaling offers one\nsolution to the limits of sequential scaling, thus we aug-\nment our sequentially scaled model with two methods:(I)\nMajority voting: After generating solutions, the final\nsolution is the most frequent one across generations;(II)\nTree search viaREBASE:We use theREBASE process\nreward model, which is initialized from LLaMA-34B and\nfurther finetuned on a synthetic process reward modeling\ndataset (Wu et al., 2024b). We then aggregate the solutions\ngenerated byREBASE via majority voting. As shown in\nFigure7,augmentingourmodelwith REBASE scalesbetter\nthan majority voting, and even sequential scaling in this sce-\nnario. However,REBASE requires an additional forward\npassateachstepfortherewardmodeladdingsomecomputa-\ntionoverhead. Forsequentialscaling,whenpromptedtouse\nup to 512 steps, for 12 out of the 30 evaluation questions the\nmodelgeneratesaresponsethatexceedsthecontextwindow\nleading to a large performance drop. Overall, we find that\nthese parallel scaling methods complement sequential scal-\ning thus they offer an avenue for scaling test-time compute\neven further; beyond fixed context windows.\n9s1: Simple test-time scaling\nImpact Statement\nLanguage models with strong reasoning capabilities have\nthe potential to greatly enhance human productivity, from\nassisting in complex decision-making to driving scientific\nbreakthroughs. However,recentadvancesinreasoning,such\nas OpenAIs o1 and DeepSeeks r1, lack transparency, limit-\ning broader research progress. Our work aims to push the\nfrontier of reasoning in a fully open manner, fostering in-\nnovation and collaboration to accelerate advancements that\nultimately benefit society.\nAcknowledgements\nWethankRyanMartenforgeneratingtracesfromDeepSeek\nr1 for s1.1 using Bespoke Curator (Marten et al., 2025).\nThisworkwaspartlyconductedusingtheStanfordMarlowe\nGPUcluster(Kapferetal.,2025),madepossiblebyfinancial\nsupport from Stanford University. We thank Alexander M.\nRush, Andrew Ilyas, Banghua Zhu, Chenglei Si, Chunting\nZhou, John Yang, Ludwig Schmidt, Samy Jelassi, Suhas\nKotha,TengyuMa,XuechenLi,YuSun,andYueZhangfor\nvery constructive discussions.\nReferences\nAnkner, Z., Paul, M., Cui, B., Chang, J. D., and Am-\nmanabrolu, P. Critique-out-loud reward models, 2024.\nURL https://arxiv.org/abs/2408.11791.\nArora, D., Singh, H. G., and Mausam. Have llms advanced\nenough? a challenging problem solving benchmark for\nlarge language models, 2023. URLhttps://arxiv.\norg/abs/2305.15074.\nAzerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D.,\nMcAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and\nWell', 'llms advanced\nenough? a challenging problem solving benchmark for\nlarge language models, 2023. URLhttps://arxiv.\norg/abs/2305.15074.\nAzerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D.,\nMcAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and\nWelleck, S. Llemma: An open language model for math-\nematics, 2023.\nBi, Z., Zhang, N., Jiang, Y., Deng, S., Zheng, G., and\nChen, H. When do program-of-thoughts work for reason-\ning?,2023. URLhttps://arxiv.org/abs/2308.\n15452.\nBiderman, S., Schoelkopf, H., Sutawika, L., Gao, L., Tow,\nJ., Abbasi, B., Aji, A. F., Ammanamanchi, P. S., Black,\nS., Clive, J., DiPofi, A., Etxaniz, J., Fattori, B., Forde,\nJ. Z., Foster, C., Hsu, J., Jaiswal, M., Lee, W. Y., Li, H.,\nLovering, C., Muennighoff, N., Pavlick, E., Phang, J.,\nSkowron, A., Tan, S., Tang, X., Wang, K. A., Winata,\nG. I., Yvon, F., and Zou, A. Lessons from the trenches on\nreproducible evaluation of language models, 2024.\nBrown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., R,\nC., and Mirhoseini, A. Large language monkeys: Scaling\ninference compute with repeated sampling, 2024. URL\nhttps://arxiv.org/abs/2407.21787.\nCesista, F.L. Multimodalstructuredgeneration: Cvprs2nd\nmmfm challenge technical report, 2024. URLhttps:\n//arxiv.org/abs/2406.11403.\nChen, W., Yin, M., Ku, M., Lu, P., Wan, Y., Ma, X.,\nXu, J., Wang, X., and Xia, T. Theoremqa: A theorem-\ndriven question answering dataset, 2023. URLhttps:\n//arxiv.org/abs/2305.12524.\nChoi,S.,Fang,T.,Wang,Z.,andSong,Y. Kcts: Knowledge-\nconstrained tree search decoding with token-level halluci-\nnation detection, 2023. URLhttps://arxiv.org/\nabs/2310.09044.\nDeepSeek-AI,Guo,D.,Yang,D.,Zhang,H.,Song,J.,Zhang,\nR., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X.,\nYu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao,\nZ., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu,\nC., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D.,\nChen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao,\nG., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang,\nH., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J.,\nLi, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai,\nJ. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao,\nK., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L.,\nZhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang,\nM., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian,\nN., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q.,\nGe, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin,\nR. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang,\nS., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu,\nS., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W.,\nZhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang,\nW., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X.,\nNie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X.,\nLi, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen,\nX., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X.,\nShan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y.,\nXu,Y., Li,Y., Zhao,Y., Sun,Y., Wang, Y.,Yu, Y.,Zhang,\nY., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan,\nY., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong,\nY., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y.,\nZhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng,\nY., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z.,\nRen, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao,\nZ., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li,\nZ., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang,\nZ., and Zhang, Z. Deepseek-r1: Incentivizing reasoning\ncapability in llms via reinforcement learning, 2025. URL\nhttps://arxiv.org/abs/2501.12948.\nDiao, S., Wang, P., Lin, Y., Pan, R., Liu, X., and Zhang,\nT. Active prompting with chain-of-thought for large lan-\n10s1: Simple test-time scaling\nguage models, 2024. URL https://arxiv.org/\nabs/2302.12246.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A.,\nLetman, A., Mathur, A., Schelten, A., Yang, A., Fan, A.,\nGoyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravanku-\nmar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A.,\nRodriguez, A., Gregerson, A., et al. The llama 3 herd\nof models, 2024. URLhttps://arxiv.org/abs/\n2407.21783.\nFu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot,\nT. Complexity-based prompting for multi-step reason-\ning, 2023. URLhttps://arxiv.org/abs/2210.\n00720.\nGandhi,K.,Lee,D.,Grand,G.,Liu,M.,Cheng,W.,Sharma,\nA.,andGoodman,N.D. Streamofsearch(sos): Learning\nto search in language, 2024. URLhttps://arxiv.\norg/abs/2404.03683.\nGao, B., Song, F., Yang, Z., Cai, Z., Miao, Y., Dong, Q., Li,\nL., Ma, C., Chen, L., Xu, R., Tang, Z., Wang, B., Zan, D.,\nQuan, S., Zhang, G., Sha, L., Zhang, Y., Ren, X., Liu, T.,\nand Chang, B. Omni-math: A universal olympiad level\nmathematic benchmark for large language models, 2024a.\nURL https://arxiv.org/abs/2410.07985.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,\nC., Golding, L., Hsu, J., McDonell, K., Muennighoff, N.,\nPhang, J., Reynolds, L., Tang, E., Thite, A., Wang, B.,\n', 'k for large language models, 2024a.\nURL https://arxiv.org/abs/2410.07985.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,\nC., Golding, L., Hsu, J., McDonell, K., Muennighoff, N.,\nPhang, J., Reynolds, L., Tang, E., Thite, A., Wang, B.,\nWang,K.,andZou,A.Aframeworkforfew-shotlanguage\nmodel evaluation, September 2021. URLhttps://\ndoi.org/10.5281/zenodo.5371628.\nGao,Z.,Niu,B.,He,X.,Xu,H.,Liu,H.,Liu,A.,Hu,X.,and\nWen, L. Interpretable contrastive monte carlo tree search\nreasoning,2024b. URLhttps://arxiv.org/abs/\n2410.01707.\nGlazer, E., Erdil, E., Besiroglu, T., Chicharro, D., Chen,\nE., Gunning, A., Olsson, C. F., Denain, J.-S., Ho, A.,\nde Oliveira Santos, E., Jrviniemi, O., Barnett, M., San-\ndler, R., Vrzala, M., Sevilla, J., Ren, Q., Pratt, E., Levine,\nL., Barkley, G., Stewart, N., Grechuk, B., Grechuk, T.,\nEnugandla,S.V.,andWildon,M. Frontiermath: Abench-\nmark for evaluating advanced mathematical reasoning in\nai, 2024. URL https://arxiv.org/abs/2411.\n04872.\nGoogle. Gemini 2.0 flash thinking mode (gemini-2.0-\nflash-thinking-exp-1219), December 2024. URL\nhttps://cloud.google.com/vertex-ai/\ngenerative-ai/docs/thinking-mode.\nGroeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney,\nR., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I.,\nWang, Y., Arora, S., Atkinson, D., Authur, R., Chandu,\nK.R., Cohan,A., Dumas,J., Elazar, Y., Gu,Y., Hessel,J.,\nKhot,T.,Merrill,W.,Morrison,J.,Muennighoff,N.,Naik,\nA., Nam, C., Peters, M. E., Pyatkin, V., Ravichander, A.,\nSchwenk,D.,Shah,S.,Smith,W.,Strubell,E.,Subramani,\nN., Wortsman, M., Dasigi, P., Lambert, N., Richardson,\nK.,Zettlemoyer,L.,Dodge,J.,Lo,K.,Soldaini,L.,Smith,\nN. A., and Hajishirzi, H. Olmo: Accelerating the science\nof language models, 2024.\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J.,\nHu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L.,\nLiu, Z., and Sun, M. Olympiadbench: A challeng-\ning benchmark for promoting agi with olympiad-level\nbilingual multimodal scientific problems, 2024. URL\nhttps://arxiv.org/abs/2402.14008.\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS.,Tang,E.,Song,D.,andSteinhardt,J. Measuringmath-\nematical problem solving with the math dataset, 2021.\nURL https://arxiv.org/abs/2103.03874.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai,T.,Rutherford,E.,deLasCasas,D.,Hendricks,L.A.,\nWelbl, J., Clark, A., Hennigan, T., Noland, E., Millican,\nK., van den Driessche, G., Damoc, B., Guy, A., Osindero,\nS., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and\nSifre, L. Training compute-optimal large language mod-\nels, 2022. URLhttps://arxiv.org/abs/2203.\n15556.\nHou, Z., Lv, X., Lu, R., Zhang, J., Li, Y., Yao, Z., Li, J.,\nTang, J., and Dong, Y. Advancing language model rea-\nsoning through reinforcement learning and inference scal-\ning, 2025. URLhttps://arxiv.org/abs/2501.\n11651.\nHu, Y., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettle-\nmoyer, L., Smith, N. A., and Krishna, R. Visual sketch-\npad: Sketchingasavisualchainofthoughtformultimodal\nlanguage models, 2024. URLhttps://arxiv.org/\nabs/2406.09403.\nHuang, Z., Wang, Z., Xia, S., Li, X., Zou, H., Xu, R., Fan,\nR.-Z., Ye, L., Chern, E., Ye, Y., Zhang, Y., Yang, Y., Wu,\nT.,Wang,B.,Sun,S.,Xiao,Y.,Li,Y.,Zhou,F.,Chern,S.,\nQin, Y., Ma, Y., Su, J., Liu, Y., Zheng, Y., Zhang, S., Lin,\nD., Qiao, Y., and Liu, P. Olympicarena: Benchmarking\nmulti-discipline cognitive reasoning for superintelligent\nai, 2024a. URLhttps://arxiv.org/abs/2406.\n12753.\nHuang, Z., Zou, H., Li, X., Liu, Y., Zheng, Y., Chern, E.,\nXia, S., Qin, Y., Yuan, W., and Liu, P. O1 replication\njourney  part 2: Surpassing o1-preview through simple\ndistillation, big progress or bitter lesson?, 2024b. URL\nhttps://arxiv.org/abs/2411.16489.\n11s1: Simple test-time scaling\nHuang, Z., Geng, G., Hua, S., Huang, Z., Zou, H., Zhang,\nS., Liu, P., and Zhang, X. O1 replication journey  part 3:\nInference-time scaling for medical reasoning, 2025. URL\nhttps://arxiv.org/abs/2501.06458.\nIrvine, R., Boubert, D., Raina, V., Liusie, A., Zhu, Z.,\nMudupalli, V., Korshuk, A., Liu, Z., Cremer, F., As-\nsassi, V., Beauchamp, C.-C., Lu, X., Rialan, T., and\nBeauchamp, W. Rewarding chatbots for real-world en-\ngagement with millions of users, 2023. URLhttps:\n//arxiv.org/abs/2303.06135.\nJain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang,\nT., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I.\nLivecodebench: Holistic and contamination free evalu-\nation of large language models for code, 2024. URL\nhttps://arxiv.org/abs/2403.07974.\nKapfer, C., Stine, K., Narasimhan, B., Mentzel, C., and\nCandes,E. Marlowe: Stanfordsgpu-basedcomputational\ninstrument, January 2025. URLhttps://doi.org/\n10.5281/zenodo.14751899.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J.,\nand Amodei, D. Scaling laws for neural language mod-\nels, 2020. URLhttps://arxiv.org/abs/2001.\n08361.\nKim,E.,Suk,J.,Kim,S.,Muennighoff,N.,Kim,D.,andOh,\nA. Llm-as-an-interviewer: Beyond static testing through\ndynamic llm ev', 'y, S., Radford, A., Wu, J.,\nand Amodei, D. Scaling laws for neural language mod-\nels, 2020. URLhttps://arxiv.org/abs/2001.\n08361.\nKim,E.,Suk,J.,Kim,S.,Muennighoff,N.,Kim,D.,andOh,\nA. Llm-as-an-interviewer: Beyond static testing through\ndynamic llm evaluation, 2024. URLhttps://arxiv.\norg/abs/2412.10424.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Effi-\ncientmemorymanagementforlargelanguagemodelserv-\ning with pagedattention, 2023. URLhttps://arxiv.\norg/abs/2309.06180.\nLabs, B. Bespoke-stratos: The unreasonable effectiveness\nof reasoning distillation, 2025. URL https://hf.\nco/bespokelabs/Bespoke-Stratos-32B. Ac-\ncessed: 2025-01-22.\nLee, K.-H., Fischer, I., Wu, Y.-H., Marwood, D., Baluja,\nS., Schuurmans, D., and Chen, X. Evolving deeper llm\nthinking, 2025. URL https://arxiv.org/abs/\n2501.09891.\nLevi, N. A simple model of inference scaling laws, 2024.\nURL https://arxiv.org/abs/2410.16377.\nLI, J., Beeching, E., Tunstall, L., Lipkin, B., Solet-\nskyi, R., Huang, S. C., Rasul, K., Yu, L., Jiang, A.,\nShen, Z., Qin, Z., Dong, B., Zhou, L., Fleureau, Y.,\nLample, G., and Polu, S. Numinamath, 2024. URL\nhttps://github.com/project-numina/\naimo-progress-prize/blob/main/report/\nnumina_dataset.pdf.\nLightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker,\nB., Lee, T., Leike, J., Schulman, J., Sutskever, I., and\nCobbe, K. Lets verify step by step, 2023. URLhttps:\n//arxiv.org/abs/2305.20050.\nLing,W.,Yogatama,D.,Dyer,C.,andBlunsom,P. Program\ninduction by rationale generation : Learning to solve and\nexplain algebraic word problems, 2017. URLhttps:\n//arxiv.org/abs/1705.04146.\nLiu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang,\nY. Logiqa: A challenge dataset for machine reading com-\nprehension with logical reasoning, 2020. URLhttps:\n//arxiv.org/abs/2007.08124.\nLiu, J., Cohen, A., Pasunuru, R., Choi, Y., Hajishirzi,\nH., and Celikyilmaz, A. Dont throw away your value\nmodel! generatingmorepreferabletextwithvalue-guided\nmonte-carlo tree search decoding, 2024. URLhttps:\n//arxiv.org/abs/2309.15028.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization, 2019.\nLuo,H.,Sun,Q.,Xu,C.,Zhao,P.,Lou,J.,Tao,C.,Geng,X.,\nLin, Q., Chen, S., Tang, Y., and Zhang, D. Wizardmath:\nEmpowering mathematical reasoning for large language\nmodels via reinforced evol-instruct, 2025. URLhttps:\n//arxiv.org/abs/2308.09583.\nMarten, R., Vu, T., Ji, C. C.-J., Sharma, K., Pimpalgaonkar,\nS., Dimakis, A., and Sathiamoorthy, M. Curator: A tool\nfor synthetic data creation.https://github.com/\nbespokelabsai/curator, January 2025.\nMuennighoff, N., Soldaini, L., Groeneveld, D., Lo, K., Mor-\nrison,J.,Min,S.,Shi,W.,Walsh,P.,Tafjord,O.,Lambert,\nN., Gu, Y., Arora, S., Bhagia, A., Schwenk, D., Wadden,\nD., Wettig, A., Hui, B., Dettmers, T., Kiela, D., Farhadi,\nA., Smith, N. A., Koh, P. W., Singh, A., and Hajishirzi, H.\nOlmoe: Open mixture-of-experts language models, 2024.\nURL https://arxiv.org/abs/2409.02060.\nof America, M. A. Aime, February 2024. URLhttps:\n//artofproblemsolving.com/wiki/index.\nphp/AIME_Problems_and_Solutions/.\nOpenAI. Learning to reason with llms, Septem-\nber 2024. URL https://openai.com/index/\nlearning-to-reason-with-llms/ .\nOpenAI. Openaio3-mini,2025. URLhttps://openai.\ncom/index/openai-o3-mini/. Accessed: 2025-\n02-24.\n12s1: Simple test-time scaling\nPhan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Shi,\nS., Choi, M., Agrawal, A., Chopra, A., et al. Humanitys\nlast exam, 2025. URLhttps://arxiv.org/abs/\n2501.14249.\nQin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye,\nY., Yuan, W., Liu, H., Li, Y., and Liu, P. O1 replication\njourney: A strategic progress report  part 1, 2024. URL\nhttps://arxiv.org/abs/2410.18982.\nQwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng,\nB., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H.,\nYang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J.,\nLin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li,\nM., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li,\nT., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y.,\nWan,Y.,Liu,Y.,Cui,Z.,Zhang,Z.,andQiu,Z. Qwen2.5\ntechnical report, 2024. URLhttps://arxiv.org/\nabs/2412.15115.\nRein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y.,\nDirani, J., Michael, J., and Bowman, S. R. Gpqa: A\ngraduate-level google-proof q&a benchmark, 2023. URL\nhttps://arxiv.org/abs/2311.12022.\nShi, Q., Tang, M., Narasimhan, K., and Yao, S. Can lan-\nguagemodels solveolympiad programming?, 2024. URL\nhttps://arxiv.org/abs/2404.10952.\nSnell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-\ntime compute optimally can be more effective than scal-\ning model parameters, 2024. URLhttps://arxiv.\norg/abs/2408.03314.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., et al. Beyond the imitation game:\nQuantifyingandextrapolatingthecapabilitiesoflanguage\nmodels, 2023.\nSu,H.,Yen,H.,Xia,M.,Shi,W.,Muennighoff,N.,yuWang,\nH.,Liu,H.,Shi,Q', ' M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., et al. Beyond the imitation game:\nQuantifyingandextrapolatingthecapabilitiesoflanguage\nmodels, 2023.\nSu,H.,Yen,H.,Xia,M.,Shi,W.,Muennighoff,N.,yuWang,\nH.,Liu,H.,Shi,Q.,Siegel,Z.S.,Tang,M.,Sun,R.,Yoon,\nJ., Arik, S. O., Chen, D., and Yu, T. Bright: A realis-\ntic and challenging benchmark for reasoning-intensive\nretrieval, 2024. URL https://arxiv.org/abs/\n2407.12883.\nSun,L.,Han,Y.,Zhao,Z.,Ma,D.,Shen,Z.,Chen,B.,Chen,\nL., and Yu, K. Scieval: A multi-level large language\nmodel evaluation benchmark for scientific research, 2024.\nURL https://arxiv.org/abs/2308.13149.\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li,\nC., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang,\nD., Yuan, E., Lu, E., Tang, F., Sung, F., Wei, G., Lai, G.,\nGuo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H.,\nYao, H., Zhao, H., Lu, H., Li, H., Yu, H., Gao, H., Zheng,\nH., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J.,\nZhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S.,\nMa, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W.,\nXiong, W., He, W., Huang, W., Wu, W., He, W., Wei, X.,\nJia,X.,Wu,X.,Xu,X.,Zu,X.,Zhou,X.,Pan,X.,Charles,\nY.,Li,Y.,Hu,Y.,Liu,Y.,Chen,Y.,Wang,Y.,Liu,Y.,Qin,\nY., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y.,\nZhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang, Z., Wang, Z.,\nYang,Z.,Huang,Z.,Huang,Z.,Xu,Z.,andYang,Z. Kimi\nk1.5: Scaling reinforcement learning with llms, 2025.\nURL https://arxiv.org/abs/2501.12599.\nTeam, N. Sky-t1: Fully open-source reasoning\nmodel with o1-preview performance in $450 budget,\n2025. URL https://novasky-ai.github.io/\nposts/sky-t1. Accessed: 2025-01-09.\nTeam, Q. Qwq: Reflect deeply on the boundaries of the\nunknown, November 2024. URLhttps://qwenlm.\ngithub.io/blog/qwq-32b-preview/.\nWang, J., Meng, F., Liang, Y., and Zhou, J. Drt-o1: Op-\ntimized deep reasoning translation via long chain-of-\nthought, 2024a. URLhttps://arxiv.org/abs/\n2412.17498.\nWang, P., Li, L., Shao, Z., Xu, R. X., Dai, D., Li, Y., Chen,\nD., Wu, Y., and Sui, Z. Math-shepherd: Verify and rein-\nforcellmsstep-by-stepwithouthumanannotations,2024b.\nURL https://arxiv.org/abs/2312.08935.\nWang, S., Liu, Z., Zhong, W., Zhou, M., Wei, Z., Chen, Z.,\nand Duan, N. From lsat: The progress and challenges\nof complex reasoning, 2021. URLhttps://arxiv.\norg/abs/2108.00648.\nWang, Z., Dong, Y., Delalleau, O., Zeng, J., Shen, G., Egert,\nD., Zhang, J. J., Sreedhar, M. N., and Kuchaiev, O. Help-\nsteer2: Open-source dataset for training top-performing\nreward models, 2024c. URLhttps://arxiv.org/\nabs/2406.08673.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter,\nB., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-\nthought prompting elicits reasoning in large language\nmodels, 2023. URL https://arxiv.org/abs/\n2201.11903.\nWelleck,S.,Bertsch,A.,Finlayson,M.,Schoelkopf,H.,Xie,\nA., Neubig, G., Kulikov, I., and Harchaoui, Z. From de-\ncoding to meta-generation: Inference-time algorithms for\nlarge language models, 2024. URLhttps://arxiv.\norg/abs/2406.16838.\n13s1: Simple test-time scaling\nWu, T., Lan, J., Yuan, W., Jiao, J., Weston, J., and\nSukhbaatar, S. Thinking llms: General instruction fol-\nlowing with thought generation, 2024a. URLhttps:\n//arxiv.org/abs/2410.10630.\nWu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y. Infer-\nence scaling laws: An empirical analysis of compute-\noptimalinferenceforproblem-solvingwithlanguagemod-\nels,2024b. URLhttps://arxiv.org/abs/2408.\n00724.\nXiang, V., Snell, C., Gandhi, K., Albalak, A., Singh, A.,\nBlagden, C., Phung, D., Rafailov, R., Lile, N., Mahan,\nD., Castricato, L., Franken, J.-P., Haber, N., and Finn, C.\nTowards system 2 reasoning in llms: Learning how to\nthink with meta chain-of-thought, 2025. URLhttps:\n//arxiv.org/abs/2501.04682.\nXie, Y., Kawaguchi, K., Zhao, Y., Zhao, X., Kan, M.-Y., He,\nJ., and Xie, Q. Self-evaluation guided beam search for\nreasoning, 2023. URLhttps://arxiv.org/abs/\n2305.00633.\nXin, H., Guo, D., Shao, Z., Ren, Z., Zhu, Q., Liu, B., Ruan,\nC., Li, W., and Liang, X. Deepseek-prover: Advancing\ntheorem proving in llms through large-scale synthetic\ndata,2024. URLhttps://arxiv.org/abs/2405.\n14333.\nXu, H., Wu, X., Wang, W., Li, Z., Zheng, D., Chen, B.,\nHu, Y., Kang, S., Ji, J., Zhang, Y., Guo, Z., Yang, Y.,\nZhang,M.,andZhang,D. Redstar: Doesscalinglong-cot\ndata unlock better slow-reasoning systems?, 2025. URL\nhttps://arxiv.org/abs/2501.11284.\nYang, Z., Band, N., Li, S., Cands, E., and Hashimoto, T.\nSynthetic continued pretraining, 2024. URLhttps:\n//arxiv.org/abs/2409.07431.\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao,\nY., and Narasimhan, K. Tree of thoughts: Deliberate\nproblemsolvingwithlargelanguagemodels,2023a. URL\nhttps://arxiv.org/abs/2305.10601.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK., and Cao, Y. React: Synergizing reasoning and acting\nin language models, 2023b. URLhttps://arxiv.\norg/abs/2210', 'msolvingwithlargelanguagemodels,2023a. URL\nhttps://arxiv.org/abs/2305.10601.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK., and Cao, Y. React: Synergizing reasoning and acting\nin language models, 2023b. URLhttps://arxiv.\norg/abs/2210.03629.\nYe, Y., Huang, Z., Xiao, Y., Chern, E., Xia, S., and Liu, P.\nLimo: Less is more for reasoning, 2025a. URLhttps:\n//arxiv.org/abs/2502.03387.\nYe, Y., Xiao, Y., Mi, T., and Liu, P. Aime-preview: A\nrigorous and immediate evaluation framework for ad-\nvanced mathematical reasoning. https://github.\ncom/GAIR-NLP/AIME-Preview, 2025b. GitHub\nrepository.\nYu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok,\nJ.T.,Li,Z.,Weller,A.,andLiu,W. Metamath: Bootstrap\nyour own mathematical questions for large language mod-\nels, 2024. URLhttps://arxiv.org/abs/2309.\n12284.\nYuan, S., Chen, Z., Xi, Z., Ye, J., Du, Z., and Chen, J.\nAgent-r: Training language model agents to reflect via\niterative self-training, 2025. URLhttps://arxiv.\norg/abs/2501.11425.\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. D. Star:\nBootstrapping reasoning with reasoning, 2022. URL\nhttps://arxiv.org/abs/2203.14465.\nZelikman, E., Harik, G., Shao, Y., Jayasiri, V., Haber, N.,\nand Goodman, N. D. Quiet-star: Language models can\nteach themselves to think before speaking, 2024. URL\nhttps://arxiv.org/abs/2403.09629.\nZhang, H. and Chen, C. Test-time compute scaling laws,\n2024. URLhttps://github.com/hughbzhang/\no1_inference_scaling_laws.\nZhang, S., Chen, Z., Shen, Y., Ding, M., Tenenbaum, J. B.,\nand Gan, C. Planning with large language models for\ncode generation, 2023. URLhttps://arxiv.org/\nabs/2303.05510.\nZhang, Y., Wu, S., Yang, Y., Shu, J., Xiao, J., Kong, C., and\nSang, J. o1-coder: an o1 replication for coding, 2024a.\nURL https://arxiv.org/abs/2412.00154.\nZhang, Y., Yang, J., Yuan, Y., and Yao, A. C.-C. Cumula-\ntive reasoning with large language models, 2024b. URL\nhttps://arxiv.org/abs/2308.04371.\nZhong, H., Xiao, C., Tu, C., Zhang, T., Liu, Z., and Sun, M.\nJec-qa: A legal-domain question answering dataset, 2019.\nURL https://arxiv.org/abs/1911.12011.\nZhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang,\nY., Saied, A., Chen, W., and Duan, N. Agieval: A\nhuman-centric benchmark for evaluating foundation mod-\nels, 2023. URLhttps://arxiv.org/abs/2304.\n06364.\nZhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H.,\nand Wang, Y.-X. Language agent tree search unifies rea-\nsoning acting and planning in language models, 2024.\nURL https://arxiv.org/abs/2310.04406.\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M.,\nZettlemoyer, L., and Levy, O. Lima: Less is more for\nalignment, 2023. URLhttps://arxiv.org/abs/\n2305.11206.\n14s1: Simple test-time scaling\nContents\n1 Introduction 1\n2 Reasoning data curation to create s1K 2\n2.1 Initial collection of 59K samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n2.2 Final selection of 1K samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n3 Test-time scaling 3\n3.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n3.2 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n4 Results 4\n4.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n4.2 Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n5 Ablations 6\n5.1 Data Quantity, Diversity, and Difficulty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n5.2 Test-time scaling methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n6 Discussion and related work 8\n6.1 Sample-efficient reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n6.2 Test-time scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\nA s1.1 16\nB Evaluation determinism 16\nC s1K details 17\nC.1 s1K summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nC.2 Dataset composition for full 59K questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nC.3 s1K grading prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC.4 s1K diversity selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC.5 Decontamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nD Training details 21\nD.1 Training Ablations: Sequence length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nD.2 Training Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nE Test-time sca', 'D.1 Training Ablations: Sequence length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nD.2 Training Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nE Test-time scaling details 35\nE.1 Sequential scaling ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\nE.2 Examples for rejection sampling ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nF Version Control 46\n15s1: Simple test-time scaling\nA. s1.1\nSeven days after our release of s1, we released s1.1. We regenerated traces for our 1,000 samples ins1Kusing DeepSeek\nr1 (DeepSeek-AI et al., 2025) to creates1K-1.1. We use the same training procedure to train our models1.1. Other updates\nsince our launch include the release of o3 (OpenAI, 2025), LIMO (Ye et al., 2025a), and AIME 2025. We consider all these\nnew developments in Table 5. We find that s1.1 performs significantly better than s1. We also tried distilling from Claude\n3.7, which led to worse performance than from r1 (not reported).\nTable 5.s1-32B is an open and sample-efficient reasoning model.We evaluates1-32B, Qwen, and Gemini (some entries are unknown\n(N.A.), see 4). Other results are from the respective reports (Qwen et al., 2024; Team, 2024; OpenAI, 2024; 2025; DeepSeek-AI et al.,\n2025; Labs, 2025; Team, 2025) except for AIME 2025 (Ye et al., 2025b). # ex. = number examples used for reasoning finetuning; BF =\nbudget forcing.\nModel # Examples MATH500 GPQA AIME 2024 AIME 2025\nAPI only\no3-mini-low N/A 95.8 70.6 56.3 42.1\no3-mini-medium N/A 97.3 76.8 75.8 70.4\no3-mini-high N/A 97.9 79.7 83.8 80.9\nOpen Weights\nQwQ-32B N.A. 90.6 54.5 46.7 32.7\nr1 800K 07.3 71.5 79.8 70.0\nr1-distill-Llama-70B 800K 94.5 65.2 57.1 56.3\nr1-distill-Qwen-14B 800K 93.9 59.1 61.7 48.0\nr1-distill-Qwen-32B 800K 94.3 62.1 58.3 49.6\nOpen Weights and Open Data\nLIMO 817 94.8 66.7 56.3 44.6\ns1 w/o BF 1K 92.6 56.6 50.0 26.7\ns1 with Budget Forcing Wait 1x 1K 92.8 59.6 53.3 30.0\ns1 with Budget Forcing Wait 2x 1K 93.0 59.6 53.3 33.3\ns1 with Budget Forcing Wait 4x 1K 92.2 58.6 56.7 36.7\ns1.1 w/o BF 1K 94.4 60.6 56.7 50.0\ns1.1 with Budget Forcing Wait 1x 1K 95.4 62.6 56.7 50.0\ns1.1 with Budget Forcing Wait 2x 1K 95.4 63.6 56.7 50.0\nB. Evaluation determinism\nWe run our evaluations using vLLM (Kwon et al., 2023) as it is faster than the alternatives we tried. However, we find that\neven when using the same random seeds and greedy sampling, evaluation scores can change significantly across runs:\n Different batch sizes causing different results seehttps://github.com/vllm-project/vllm/issues/\n5898\n Continuing generations causing different results seehttps://github.com/vllm-project/vllm/issues/\n11783\n Changes in tensor parallelism causing different results\nAs our model generates long reasoning traces prior to its answer, small numeric changes can snowball into large differences.\nWe encounter many generations that are exactly the same for thousands of tokens and then suddenly differ in one token\neventually ending up with an entirely different answer. To partly counter this issue we generally run our final evaluations\nusing full precision unless otherwise indicated.\n16s1: Simple test-time scaling\nC. s1K details\nC.1. s1K summary\nTable 6.Summary of our dataset s1K. Token count measured by the Qwen-2.5 tokenizer. We prompt Claude to produce keywords given\nseveral questions from the domain.\nDomain #questions Total token count Keywords\nGeometry 109 560.2K Area, Triangle, Distance\nNumber theory 98 522.5K Sequences, Divisibility\nCombinatorics 75 384.7K Permutations, Counting\nReal functions 43 234.8K Trigonometry, Calculus\nBiology 41 120.9K Organic reactions\nComplex functions 32 170.2K Complex roots\nQuantum theory 32 127.9K Particles, Wave functions\nField theory 28 150.1K Polynomials, Roots\nCalculus of variations 28 155.5K Optimization, Control\nDifference equations 24 132.5K Recurrence, Recursion\nElectromagnetic theory 23 95.8K Optics, Waves, Diffraction\nGroup theory 22 100.0K Groups, Automorphisms\nLinear algebra 22 128.3K Matrices, Determinants\nProbability theory 20 114.6K Random walk, Expectation\nAlgebraic systems 19 109.9K Functional equations\nMechanics 19 103.6K Forces, Motion, Energy\nThermodynamics 19 74.2K Heat engines, Entropy\nDifferential equations 18 89.6K Substitution, Existence\nComputer science 18 34.2K Complexity theory, Algorithms\nNumerical analysis 18 76.5K Error analysis, Stability\nCalculus 17 96.3K Convergence, Summation\nAlgebraic structures 17 90.4K Inequalities, Sets\nAstronomy 16 37.7K Stellar populations, Orbits\nRemaining 27 domains 242 982.2K Domains with  16 questions\nAll domains (51) 1000 4.7M s1K\nC.2. Dataset composition for full 59K questions\n17s1: Simple test-time scaling\nTable 7.Composition of full 59K questions.Thinking and response lengths are measured in tokens using the Qwen2.5-32B-Instruct\ntokenizer (Qwen et al., 2024). In addition to excluding our eval', 'ition for full 59K questions\n17s1: Simple test-time scaling\nTable 7.Composition of full 59K questions.Thinking and response lengths are measured in tokens using the Qwen2.5-32B-Instruct\ntokenizer (Qwen et al., 2024). In addition to excluding our evaluation benchmark, AIME24, we also exclude AIME questions from\n2022-2023 as we use these 90 questions during our development stage ofs1-32B.\nSource Description #Samples Avg.\nthinking\nlength\nNuminaMATH (LI et al., 2024) Math problems from online websites 30660 4.1K\nMATH (Hendrycks et al., 2021) Math problems from competitions 11999 2.9K\nOlympicArena (Huang et al.,\n2024a)\nAstronomy, Biology, Chemistry, Com-\nputer Science, Geography, Math, and\nPhysics olympiad questions\n4250 3.2K\nOmniMath (Gao et al., 2024a) Math problems from competitions 4238 4.4K\nAGIEval (Zhong et al., 2023;\nLingetal.,2017;Hendrycksetal.,\n2021; Liu et al., 2020; Zhong\net al., 2019; Wang et al., 2021)\nEnglish, Law, Logic and Math prob-\nlems from the SAT, LSAT and other\nexams\n2385 1.2K\nxword Crossword puzzles 999 0.7K\nOlympiadBench (He et al., 2024) Math and Physics olympiad questions 896 3.9K\nAIME (1983-2021) AmericanInvitationalMathematicsEx-\namination\n890 4.7K\nTheoremQA (Chen et al., 2023) Computer Science, Finance, Math, and\nPhysics university-level questions relat-\ning to theorems\n747 2.1K\nUSACO (Shi et al., 2024) Code problems from the USA Comput-\ning Olympiad\n519 3.6K\nJEEBench (Arora et al., 2023) Chemistry,Math,andPhysicsproblems\nused in the university entrance exami-\nnation of the Indian Institute of Tech-\nnology\n515 2.9K\nGPQA (Rein et al., 2023) PhD-Level Science Questions 348 2.9K\nSciEval (Sun et al., 2024) Biology, Chemistry, and Physics prob-\nlems from various sources\n227 0.7K\ns1-prob Stanford statistics qualifying exams 182 4.0K\nLiveCodeBench (Jain et al., 2024)Code problems from coding websites\n(LeetCode, AtCoder, and CodeForces)\n151 3.5K\ns1-teasers Math brain-teasers crawled from the In-\nternet\n23 4.1K\nAll 59K questions Composite of the above datasets with\nreasoning traces and solutions\n59029 3.6K\n18s1: Simple test-time scaling\nC.3. s1K grading prompt\nTogradewhetheranexampleiscorrectforourdatasetselectionin2,weusethepromptinFigure8. WegradeusingClaude\n3.5 except for the correctness among the final 1,000 samples, which we graded with Claude 3.7.\nYou are an AI assistant for grading a science problem. The user will provide you with the question itself, an attempt made by\na student and the correct answer to the problem. Your job is to judge whether the attempt is correct by comparing it with the\ncorrect answer. If the expected solution concludes with a number or choice, there should be no ambiguity. If the expected\nsolution involves going through the entire reasoning process, you should judge the attempt based on whether the reasoning\nprocess is correct with correct answer if helpful.\nThe user will provide the attempt and the correct answer in the following format:\n# Problem\n{problem}\n## Attempt\n{attempt}\n## Correct answer\n{solution}\nExplain your reasoning, and end your response on a new line with only "Yes" or "No" (without quotes).\nFigure 8.Grading prompt.\nC.4. s1K diversity selection\nAlgorithm 1 provides our algorithm for selecting data in our diversity selection stage. As mentioned in 2, we also include\nsamples from some specific benchmarks we perceive as high-quality. None of the samples overlap with our final evaluation.\nC.5. Decontamination\nWe filter all samples by checking for an 8-gram overlap between the selected examples and the evaluation benchmarks:\nMATH500, GPTQA Diamond, and AIME24. We exclude questions with more than an 8-gram overlap.\n19s1: Simple test-time scaling\nAlgorithm 1Two-stage sampling fors1K\n1: Input: \ue23d := Set of 24,496 questions with features\n2: Output: \ue23f := Set of 1,000 selected questions\n3: \ue23f   Initialize the output set (only tracks unique elements)\n4: for\ue23d do\n5: if IsGeminiCorrect()and (IsAIME()or IsGPQA()) then\n6: \ue23f  \ue23f {}\n7: Select all correct AIME/GPQA solutions\n8: else ifIsGeminiCorrect()andIsMATH() andThinkingLength() > 5600then\n9: \ue23f  \ue23f {}\n10: Select correct MATH500 solutions with long chains\n11: end if\n12: end for\n13: \ue230  All available domains\n14: Initialize domain pool\n15: while|\ue23f| <1000do\n16:   RandomChoice(\ue230)\n17: Randomly select a domain\n18:   Questions in domain\n19: Get questions from this domain\n20: ranks  RankByThinkingLength()\n21: Rank by thinking length\n22: weights 2ranks\n23: Apply power-law weighting\n24:  WeightedSample(, weights)\n25: Sample favoring longer chains\n26: \ue23f  \ue23f {}\n27: Add selected question\n28:     {}\n29: if  = then\n30: \ue230  \ue230  {}\n31: Remove exhausted domains\n32: end if\n33: end while\n20s1: Simple test-time scaling\nD. Training details\nWe take a model that has already been pretrained and instruction tuned and further finetune it for reasoning. Specifically, we\nuse Qwen2.5-32B-Instruct (Qwen et al., 2024), which on math tasks generally matches or outperforms the larger Qwen2.5-\n72B-Instru'], dimensions=1536, num_retries=3, timeout=120)[0m
2025-12-08 14:33:31,798 - DEBUG - 

2025-12-08 14:33:31,799 - DEBUG - self.optional_params: {}
2025-12-08 14:33:31,799 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:33:31,799 - DEBUG - self.optional_params: {'dimensions': 1536}
2025-12-08 14:33:31,799 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://api.openai.com/v1 \
-d '{'model': 'text-embedding-3-small', 'input': ['s1: Simple test-time scaling\nNiklas Muennighoff*134 Zitong Yang*1 Weijia Shi*23 Xiang Lisa Li*1 Li Fei-Fei1 Hannaneh Hajishirzi23\nLuke Zettlemoyer2 Percy Liang1 Emmanuel Cands1 Tatsunori Hashimoto1\nAbstract\nTest-time scaling is a promising new approach to\nlanguage modeling that uses extra test-time com-\nputetoimproveperformance. Recently,OpenAIs\no1 model showed this capability but did not pub-\nlicly share its methodology, leading to many repli-\ncation efforts. We seek the simplest approach to\nachievetest-timescalingandstrongreasoningper-\nformance. First, we curate a small datasets1K\nof 1,000 questions paired with reasoning traces\nrelying on three criteria we validate through abla-\ntions: difficulty,diversity,andquality. Second,we\ndevelop budget forcing to control test-time com-\npute by forcefully terminating the models think-\ning process or lengthening it by appending Wait\nmultiple times to the models generation when it\ntries to end. This can lead the model to double-\ncheck its answer, often fixing incorrect reasoning\nsteps. After supervised finetuning the Qwen2.5-\n32B-Instruct language model ons1Kand equip-\nping it with budget forcing, our models1-32B ex-\nceeds o1-preview on competition math questions\nby up to 27% (MATH and AIME24). Further,\nscaling s1-32B with budget forcing allows extrap-\nolating beyond its performance without test-time\nintervention: from 50% to 57% on AIME24. Our\nmodel, data, and code are open-source athttps:\n//github.com/simplescaling/s1.\n1. Introduction\nPerformance improvements of language models (LMs) over\nthe past years have largely relied on scaling up train-time\ncompute using large-scale self-supervised pretraining (Ka-\nplan et al., 2020; Hoffmann et al., 2022). The creation of\nthese powerful models has set the stage for a new scaling\nparadigm built on top of them:test-time scaling. The aim\n*Equal contribution. ZY and NM started the project. WS, NM\nand ZY collected the prompts, XL, ZY and NM, built the data\npipeline, LZ and WS proposed using a 1K subset and ZY and\nNM built budget forcing.1 Stanford University.2 University of\nWashington, Seattle.3 Allen Institute for AI.4 Contextual AI.\n512 2048\n65\n75\n85\n95Accuracy (%)\nMathematical\nProblem Solving\n(MATH500)\n512 2048 8192\n0\n20\n40\n60\nCompetition\nMath\n(AIME24)\n1024 4096\n40\n50\n60\nPhD-Level\nScience Questions\n(GPQA Diamond)\nAverage thinking time (tokens)\nFigure 1.Test-time scaling with s1-32B.We benchmarks1-32B\non reasoning-intensive tasks and vary test-time compute.\nof this approach is to increase the compute at test time to\nget better results. There has been much work exploring this\nidea (Snell et al., 2024; Welleck et al., 2024), and the via-\nbility of this paradigm was recently validated by OpenAI\no1 (OpenAI, 2024). o1 has demonstrated strong reasoning\nperformance with consistent gains from scaling test-time\ncompute. OpenAI describes their approach as using large-\nscalereinforcementlearning(RL)implyingtheuseofsizable\namounts of data (OpenAI, 2024). This has led to various\nattempts to replicate their models relying on techniques like\nMonte Carlo Tree Search (Gao et al., 2024b; Zhang et al.,\n2024a), multi-agent approaches (Qin et al., 2024), and oth-\ners (Wang et al., 2024a; Huang et al., 2024b; 2025). Among\nthese approaches, DeepSeek R1 (DeepSeek-AI et al., 2025)\nhas successfully replicated o1-level performance, also em-\nploying reinforcement learning via millions of samples and\nmultiple training stages. However, despite the large num-\nber of o1 replication attempts, none have openly replicated\na clear test-time scaling behavior. Thus, we ask: what is\nthe simplest approach to achieve both test-time scaling and\nstrong reasoning performance?\nWeshowthattrainingononly1,000sampleswithnext-token\nprediction and controlling thinking duration via a simple\ntest-time technique we refer to asbudget forcingleads to\na strong reasoning model that scales in performance with\nmore test-time compute. Specifically, we constructs1K,\nwhich consists of 1,000 carefully curated questions paired\nwith reasoning traces and answers distilled from Gemini\nThinking Experimental (Google, 2024). We perform super-\nvised fine-tuning (SFT) of an off-the-shelf pretrained model\n1\narXiv:2501.19393v3  [cs.CL]  1 Mar 2025s1: Simple test-time scaling\non our small dataset requiring just 26 minutes of training\non 16 H100 GPUs. After training, we control the amount\nof test-time compute our model spends usingbudget forc-\ning: (I) If the model generates more thinking tokens than\na desired limit, we forcefully end the thinking process by\nappending an end-of-thinking token delimiter. Ending the\nthinking this way makes the model transition to generating\nitsanswer. (II)Ifwewantthemodeltospendmoretest-time\ncompute on a problem, we suppress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our mode', 'ress the generation of the\nend-of-thinking token delimiter and instead append Wait\nto the models current reasoning trace to encourage more\nexploration. Equipped with this simple recipe  SFT on\n1,000 samples and test-time budget forcing  our models1-\n32Bexhibits test-time scaling (Figure 1). Further,s1-32Bis\nthe most sample-efficient reasoning model and outperforms\nclosed-source models like OpenAIs o1-preview (Figure 2).\nWe conduct extensive ablation experiments targeting (a) our\nselection of 1,000 (1K) reasoning samples and (b) our test-\ntime scaling. For (a), we find that jointly incorporating\ndifficulty, diversity, and quality measures into our selec-\ntion algorithm is important. Random selection, selecting\nsamples with the longest reasoning traces, or only selecting\nmaximally diverse samples all lead to significantly worse\nperformance (around30% on AIME24 on average). Train-\ning on our full data pool of 59K examples, a superset of\ns1K, does not offer substantial gains over our 1K selection.\nThis highlights the importance of careful data selection and\nechoes prior findings for instruction tuning (Zhou et al.,\n2023). For(b), we define desiderata for test-time scaling\nmethods to compare different approaches. Budget forcing\nleads to the best scaling as it has perfect controllability with\na clear positive slope leading to strong performance.\nInsummary,ourcontributionsare: Wedevelopsimplemeth-\nods for creating a sample-efficient reasoning dataset (2)\nand test-time scaling (3); Based on these we builds1-32B\nwhich is competitive with o1-preview (4); We ablate sub-\ntleties of data (5.1) and test-time scaling (5.2). We end\nwith a discussion to motivate future work on simple rea-\nsoning (6). Our code, model, and data are open-source at\nhttps://github.com/simplescaling/s1.\n2. Reasoning data curation to create s1K\nIn this section, we describe our process for creating a large\ndataset first in 2.1 and then filtering it down tos1Kin 2.2.\n2.1. Initial collection of 59K samples\nWecollectaninitial59,029questionsfrom16sourcesfollow-\ning three guiding principles.Quality: Datasets should be\nhigh-quality; we always inspect samples and ignore datasets\nwith, e.g., poor formatting;Difficulty: Datasets should be\nchallenging and require significant reasoning effort;Diver-\nsity: Datasetsshouldstemfromvariousfieldstocoverdiffer-\nent reasoning tasks. We collect datasets of two categories:\nCuration of existing datasetsOur largest source is Nu-\nminaMATH (LI et al., 2024) with 30,660 mathematical\nproblems from online websites. We also include histori-\ncal AIME problems (1983-2021). To enhance diversity,\nwe add OlympicArena (Huang et al., 2024a) with 4,250\nquestions spanning Astronomy, Biology, Chemistry, Com-\nputer Science, Geography, Mathematics, and Physics from\nvarious Olympiads. OmniMath (Gao et al., 2024a) adds\n4,238 competition-level mathematics problems. We also\ninclude 2,385 problems from AGIEval (Zhong et al., 2023),\nwhich features questions from standardized tests like SAT\nand LSAT, covering English, Law, and Logic. We refer to\nTable 7 in C for our other sources.\nNew datasets in quantitative reasoning To comple-\nment these existing datasets, we create two original\ndatasets. s1-prob consists of 182 questions from the prob-\nability section of Stanford Universitys Statistics Depart-\nments PhD Qualifying Exams (https://statistics.\nstanford.edu), accompanied by handwritten solutions\nthat cover difficult proofs. The probability qualifying exam\nis held yearly and requires professional-level mathemati-\ncal problem-solving. s1-teasers comprises 23 challenging\nbrain-teasers commonly used in interview questions for\nquantitative trading positions. Each sample consists of a\nproblem and solution taken from PuzzledQuant (https:\n//www.puzzledquant.com/). Weonlytakeexamples\nwith the highest difficulty level ("Hard").\nFor each question, we generate a reasoning trace and solu-\ntion using the Google Gemini Flash Thinking API (Google,\n2024)extractingitsreasoningtraceandresponse. Thisyields\n59K triplets of a question, generated reasoning trace, and\ngenerated solution. Examples from our dataset are in D.2.\nWe decontaminate all samples against our evaluation ques-\ntions (MATH500, GPQA Diamond, AIME24; C.5) using\n8-grams and deduplicate the data.\n2.2. Final selection of 1K samples\nWe could directly train on our pool of 59K questions, how-\never, our goal is to find thesimplest approach with minimal\nresources. Thus, we go through three stages of filtering to\narriveataminimalsetof1,000samplesrelyingonourthree\nguiding data principles: Quality, Difficulty, and Diversity.\nQuality We first remove any questions where we ran into\nanyAPIerrorsreducingourdatasetto54,116samples. Next,\nwefilteroutlow-qualityexamplesbycheckingiftheycontain\nany string patterns with formatting issues, such as ASCII\nart diagrams, non-existent image references, or inconsistent\nquestionnumberingreducingourdatasetto51,581examples.\n2s1: Simple test-time scaling\nGeometry', '-qualityexamplesbycheckingiftheycontain\nany string patterns with formatting issues, such as ASCII\nart diagrams, non-existent image references, or inconsistent\nquestionnumberingreducingourdatasetto51,581examples.\n2s1: Simple test-time scaling\nGeometry Number\ntheory\nCombin\natorics\nReal\nfunctions\nBiology\nComplex\nfunctions\nQuantum\ntheory\nField\ntheory\nCalculus of\nvariations\nDifference\nequations\nElectro\ndynamics\nGroup\ntheory\nLinear\nalgebra\nProbability\ntheory\nAlgebraic\nsystems\nMechanics\nThermo\ndynamics\nDifferential\nequations\nComputer\nscience\nNumerical\nanalysis\nCalculus\nAlgebraic\nstructures\nAstronomy\nDynamical\nsystems\nStatistical\nmechanics\nOperations\nresearch\nMath\nematics\neducation\nMeasure\ntheory\nConvex\ngeometry\nFluid\nmechanics\nAlgebraic\ngeometry\nStatistics General\ntopology\nEconomics\nAssociative\nrings\nGeneral\nrelativity\nDifferential\ngeometry\nMath\nematical\nlogic\nPartial\ndifferential\nequations\nInformation\ntheory\nSolid\nmech\nanics\nFunctional\nanalysis\nSpecial\nfunctions\nComm\nutative\nalgebra\nIntegral\nequations\nIntegral\ntransform\nApproxi\nmation\ntheory\nPotential\ntheory\nHarmonic\nanalysis\nControl\ntheory\nGeo\nphysics\n1000 17000 800000 N/A\nNumber of Examples\n80\n85\n90\n95\n100MATH500 Accuracy (%)\ns1\nr1-distill\nSky-T1\nQwQ\nBespoke-Stratos\no1-preview\nMost sample-efficient\nFigure 2.s1Kands1-32B. (left)s1Kisadatasetof1,000high-quality,diverse,anddifficultquestionswithreasoningtraces. (right)s1-32B,\na 32B parameter model finetuned ons1K is on the sample-efficiency frontier. See Table 1 for details on other models.\nFrom this pool, we identify 384 samples for our final 1,000\nsamples from datasets that we perceive as high-quality and\nnot in need of further filtering (see C.4 for details).\nDifficulty Fordifficulty, weusetwoindicators: modelper-\nformance and reasoning trace length. We evaluate two mod-\nels on each question: Qwen2.5-7B-Instruct and Qwen2.5-\n32B-Instruct (Qwen et al., 2024), with correctness assessed\nby Claude 3.5 Sonnet comparing each attempt against the\nreference solution (see C.3 for the grading protocol). We\nmeasure the token length of each reasoning trace to indicate\nproblem difficulty using the Qwen2.5 tokenizer. This relies\non the assumption that more difficult problems require more\nthinkingtokens. Basedonthegrading,weremovequestions\nthat either Qwen2.5-7B-Instruct or Qwen2.5-32B-Instruct\ncan solve correctly and thus may be too easy. By using two\nmodels we reduce the likelihood of an easy sample slipping\nthrough our filtering due to a rare mistake on an easy ques-\ntion of one of the models. This brings our total samples\ndown to 24,496, setting the stage for the next round of sub-\nsampling based on diversity. While filtering with these two\nmodels may be optimized for our setup as we will also use\nQwen2.5-32B-Instruct as our model to finetune, the idea of\nmodel-based filtering generalizes to other setups.\nDiversity To quantify diversity, we classify questions into\ndomainsusingClaude3.5SonnetbasedontheMathematics\nSubject Classification (MSC) system (e.g., geometry, com-\nbinatorics, etc.) from the American Mathematical Society.1\nThe taxonomy focuses on topics in mathematics but also\nincludes other sciences such as biology, physics, and eco-\n1https://mathscinet.ams.org/mathscinet/\nmsc/msc2020.html\nnomics. Toselectourfinalexamplesfromthepoolof24,496\nquestions, we first choose one domain uniformly at random.\nThen, we sample one problem from this domain according\ntoadistributionthatfavorslongerreasoningtraces(seeC.4\nfordetails)asmotivatedin Difficulty. Werepeatthisprocess\nuntil we have 1,000 total samples spanning 50 domains.\nIn 5.1, we will show that using our three criteria in combi-\nnation is important, as only relying on quality, diversity, or\ndifficulty in isolation leads to worse datasets. Some distilled\ngenerations are incorrect, which we allow in our data as we\nfocusoncapturingthereasoningprocessratherthanentirely\ncorrect solutions. Our grader (C.3) deems 53.6% correct\nin s1Kand 63.0% in our follow-ups1K-1.1(see A).\n3. Test-time scaling\n3.1. Method\nWe classify test-time scaling methods into1) Sequential,\nwherelatercomputationsdependonearlierones(e.g.,along\nreasoningtrace),and 2)Parallel,wherecomputationsrunin-\ndependently(e.g.,majorityvoting)(Snelletal.,2024;Brown\netal.,2024). Wefocusonsequentialscalingasintuitivelywe\nbelieve it should scale better, since later computations can\nbuild on intermediate results, allowing for deeper reasoning\nanditerativerefinement. Weproposenewsequentialscaling\nmethods and ways to benchmark them.\nBudget forcing We propose a simple decoding-time in-\ntervention by forcing a maximum and/or minimum number\nof thinking tokens. Specifically, we enforce a maximum\ntoken count by simply appending the end-of-thinking token\ndelimiter and optionally Final Answer: to early exit\n3s1: Simple test-time scaling\nHo w many r in r aspberry ?\nLet \' s br eak do wn the pr ocess of counting the letter \' r \' in the \nwor d " r aspberry " ...\n* First letter: \' r \' - This is an \' r \', count = 1.\n* Secon', 'nally Final Answer: to early exit\n3s1: Simple test-time scaling\nHo w many r in r aspberry ?\nLet \' s br eak do wn the pr ocess of counting the letter \' r \' in the \nwor d " r aspberry " ...\n* First letter: \' r \' - This is an \' r \', count = 1.\n* Second letter: \' a \' - Not an \' r \', count r emains 1 ...\n* Sixth letter: \' e \' - Not an \' r \', count r emains 1.\n* Se venth letter: \' r \' - This is an \' r \', count = 2.\n* Eighth letter: \'y \' - Not an \' r \', count r emains 2 ...\nThe number of \' r \' s in " r aspberry " is 2.\n, let \' s r e-r ead the question car efully . It asks "Ho w many r \nin r aspberry ?" ... * r - a - s - p - b - e - r - r - y ... * First \' r \' ... * \nSecond \' r \' ... * Thir d \' r \' ... Count = 3 ...\nW ait\nMy initial answer of 2 was incorr ect due to a quick r eading of \nthe wor d. Final Answer: The final answer is\nQuestion\nReasoning tr ace\nResponse\nFigure 3.Budget forcing with s1-32B.The model tries to stop\nafter ...is 2., but we suppress the end-of-thinking token delimiter\ninstead appending Wait leadings1-32Bto self-correct its answer.\nthe thinking stage and make the model provide its current\nbest answer. To enforce a minimum, we suppress the gener-\nation of the end-of-thinking token delimiter and optionally\nappend the string Wait to the models current reasoning\ntrace to encourage the model to reflect on its current gen-\neration. Figure 3 contains an example of how this simple\napproach can lead the model to arrive at a better answer.\nBaselines We benchmark budget forcing with:(I) Con-\nditional length-control methods, which rely on telling the\nmodel in the prompt how long it should generate for. We\ngroup them by granularity into (a) Token-conditional con-\ntrol: We specify an upper bound of thinking tokens in the\nprompt; (b) Step-conditional control: We specify an upper\nbound of thinking steps, where each step is around 100 to-\nkens; (c) Class-conditional control: We write two generic\npromptsthattellthemodeltoeitherthinkforashortorlong\namount of time (see E.1 for details).(II) Rejection sam-\npling, which samples until a generation fits a predetermined\ncompute budget. This oracle captures the posterior over\nresponses conditioned on its length.\n3.2. Metrics\nWe establish a set of desiderata as evaluation metrics to\nmeasure test-time scaling across methods. Importantly, we\ndo not only care about the accuracy a method can achieve\nbut also its controllability and test-time scaling slope. For\neach method we consider, we run a set of evaluations\n\ue22d varying test-time compute on a fixed benchmark, e.g.\nAIME24. This produces a piece-wise linear function\nwith compute as the x-axis measured in thinking tokens and\naccuracyasthey-axis(seeFigure1,wheretherightmostdot\nfor AIME24 corresponds to(7320)=57% ). We measure\nthree metrics:\nControl = 1\n|\ue22d|\n\n\ue22d\n(min   max) (1)\nwhere min,max refer to a pre-specified minimum and max-\nimum amount of test-time compute; in our case thinking\ntokens. We usually only constrainmax. As tokens gener-\nated correspond to the amount of test-time compute spent,\nthis metric measures the extent to which a method allows\ncontrollability over the use of that test-time compute. We\nreport it as a percentage with 100% being perfect control.\nScaling = 1(|\ue22d|\n2\n)\n\n,\ue22d\n>\n() ()\n (2)\nScalingistheaverageslopeofthepiece-wiselinearfunction.\nIt must be positive for useful methods and larger is better.\nPerformance =max\n\ue22d\n() (3)\nPerformance is simply the maximum performance the\nmethod achieves on the benchmark. A method with mono-\ntonically increasing scaling achieves 100% performance on\nany benchmark in the limit. However, the methods we in-\nvestigate eventually flatten out or further scaling fails due to\ncontrol or context window limitations.\n4. Results\n4.1. Setup\nTraining We perform supervised finetuning on Qwen2.5-\n32B-Instruct usings1Kto obtain our models1-32Busing\nbasic hyperparameters outlined in D. Finetuning took 26\nminutes on 16 NVIDIA H100 GPUs with PyTorch FSDP.\nEvaluation We select three representative reasoning\nbenchmarks widely used in the field:AIME24(of America,\n2024)has30problemsthatwereusedinthe2024American\nInvitational Mathematics Examination (AIME) held from\nJanuary 31  February 1, 2024. AIME tests mathematical\nproblem-solving with arithmetic, algebra, counting, geome-\ntry, number theory, probability, and other secondary school\nmathtopics. High-scoringhighschoolstudentsinthetestare\ninvitedtoparticipateintheUnitedStatesofAmericaMathe-\nmaticsOlympiad(USAMO).AllAIMEanswersareintegers\nranging from000to999, inclusive. Some AIME problems\nrely on figures that we provide to our model using the vec-\ntor graphics language Asymptote as it cannot take image\ninputs. MATH500(Hendrycks et al., 2021) is a benchmark\nof competition math problems of varying difficulty. We\nevaluate on the same 500 samples selected by OpenAI in\nprior work (Lightman et al., 2023).GPQA Diamond(Rein\net al., 2023) consists of 198 PhD-level science questions\n4s1: Simple test-time scaling\nF', ' competition math problems of varying difficulty. We\nevaluate on the same 500 samples selected by OpenAI in\nprior work (Lightman et al., 2023).GPQA Diamond(Rein\net al., 2023) consists of 198 PhD-level science questions\n4s1: Simple test-time scaling\nFor cing 2048/4096 max thinking \ntokens\nIgnoring end-of-thinking 2x/4x/6x and \nappending  W ait  thus for cing it to continue \nr easoning when it tries to stop\n(a) Sequential scaling via budget forcing\nPar allel scaling via Majority \nvoting with base model \n(Qwen2.5-32B-Instruct)\nSequential scaling \nvia For cing with s1\n60% accur acy (b) Parallel scaling via majority voting\nFigure 4.Sequential and parallel test-time scaling.(a): Budget forcing shows clear scaling trends and extrapolates to some extent. For\nthe three rightmost dots, we prevent the model from stopping its thinking 2/4/6 times, each time appending Wait to its current reasoning\ntrace. (b): For Qwen2.5-32B-Instruct we perform 64 evaluations for each sample with a temperature of 1 and visualize the performance\nwhen majority voting across 2, 4, 8, 16, 32, and 64 of these.\nfromBiology,ChemistryandPhysics. ExpertswithPhDsin\nthe corresponding domains only achieved 69.7% on GPQA\nDiamond (OpenAI, 2024). When we write GPQA in the\ncontext of evaluation in this work, we always refer to the\nDiamond subset. We build on the lm-evaluation-harness\nframework (Gao et al., 2021; Biderman et al., 2024). Unless\notherwise specified, we evaluate with a temperature of 0\n(greedy) and measure accuracy (equivalent to pass@1).\nOther models We benchmarks1-32Bagainst: OpenAI\no1 series(OpenAI, 2024), closed-source models that pop-\nularized test-time scaling;DeepSeek r1 series(DeepSeek-\nAI et al., 2025), open-weight reasoning models with up to\no1-level performance; QwensQwQ-32B-preview(Team,\n2024), a 32B open-weight reasoning model without dis-\nclosed methodology;Sky-T1-32B-Preview(Team, 2025)\nandBespoke-32B(Labs, 2025), open models with open rea-\nsoningdatadistilledfromQwQ-32B-previewandr1; Google\nGemini 2.0 Flash Thinking Experimental(Google, 2024),\nthe API that we distill from. As it has no official evaluation\nscores, we use the Gemini API to benchmark it ourselves.\nHowever, the recitation error of the Gemini API makes\nevaluation challenging.2 We circumvent this, by manually\ninsertingall30AIME24questionsinitswebinterfacewhere\ntheerrordoesnotappear. However,weleaveoutMATH500\n(500 questions) and GPQA Diamond (198 questions), thus\nthey are N.A. in Table 1. Our model,s1-32B, is fully open\nincluding weights, reasoning data, and code.\n2https://github.com/google/\ngenerative-ai-docs/issues/257\nTable 1.s1-32B is a strong open reasoning model.We evaluate\ns1-32B, Qwen, and Gemini (some entries are unknown (N.A.), see\n4). Other results are from the respective reports (Qwen et al.,\n2024; Team, 2024; OpenAI, 2024; DeepSeek-AI et al., 2025; Labs,\n2025; Team, 2025). # ex. = number examples used for reasoning\nfinetuning; BF = budget forcing. See A for our betters1.1 model.\nModel # ex. AIME\n2024\nMATH\n500\nGPQA\nDiamond\nAPI only\no1-preview N.A. 44.6 85.5 73.3\no1-mini N.A. 70.0 90.0 60.0\no1 N.A. 74.4 94.8 77.3\nGemini 2.0 N.A. 60.0 N.A. N.A.Flash Think.\nOpen Weights\nQwen2.5- N.A. 26.7 84.0 49.032B-Instruct\nQwQ-32B N.A. 50.0 90.6 54.5\nr1 800K 79.8 97.3 71.5\nr1-distill 800K 72.6 94.3 62.1\nOpen Weights and Open Data\nSky-T1 17K 43.3 82.4 56.8\nBespoke-32B 17K 63.3 93.0 58.1\ns1 w/o BF 1K 50.0 92.6 56.6\ns1-32B 1K 56.7 93.0 59.6\n5s1: Simple test-time scaling\n4.2. Performance\nTest-time scaling Figure 1 shows the performance ofs1-\n32Bwithbudgetforcingscaleswithmoretest-timecompute.\nInFigure4(left),weexpandtheplotfromFigure1(middle)\nshowing that while we can improve AIME24 performance\nusing our budget forcing technique (3) and more test-time\ncomputeitdoeseventuallyflattenoutatsixtimes. Suppress-\ningtheend-of-thinkingtokendelimitertoooftencanleadthe\nmodel into repetitive loops instead of continued reasoning.\nIn Figure 4 (right), we show that after training Qwen2.5-\n32B-Instruct on our 1,000 samples to produces1-32Band\nequipping it with the simple budget forcing technique, it\noperates in a different scaling paradigm. Scaling test-time\ncomputeonthebasemodelviamajorityvotingcannotcatch\nup with the performance ofs1-32B which validates our\nintuition from 3 that sequential scaling is more effective\nthan parallel. We provide example generations ofs1-32B in\nFigure 5.\nSample-efficiency In Figure 2 (right) and Table 1 we\ncompare s1-32B with other models. We find that s1-\n32B is the most sample-efficient open data reasoning\nmodel. It performs significantly better than our base model\n(Qwen2.5-32B-Instruct) despite just training it on an ad-\nditional 1,000 samples. The concurrently released r1-32B\nshows stronger performance thans1-32B while also only\nusingSFT(DeepSeek-AIetal.,2025). However,itistrained\non 800more reasoning samples. It is an open question\nwhether one can achieve their performance with just 1,000\nsamples. Finally, our model nearly matches Gemin', 'rformance thans1-32B while also only\nusingSFT(DeepSeek-AIetal.,2025). However,itistrained\non 800more reasoning samples. It is an open question\nwhether one can achieve their performance with just 1,000\nsamples. Finally, our model nearly matches Gemini 2.0\nThinking on AIME24. As the data fors1-32B is distilled\nfrom Gemini 2.0, this shows our distillation procedure was\nlikely effective.\n5. Ablations\n5.1. Data Quantity, Diversity, and Difficulty\nIn 2 we outlined our three guiding principles in curating\ns1K: Quality, Difficulty, and Diversity. Here we test the\nimportanceofcombiningthemandtheoverallefficacyofour\nselection. Only Quality (1K-random): After obtaining our\nhigh-quality reasoning chains from Gemini, we select 1,000\nsamplesatrandom;notrelyingonourdifficultyanddiversity\nfiltering at all. Table 2 shows this approach performs much\nworse thans1K across all benchmarks. Only Diversity\n(1K-diverse): For this dataset, we sample uniformly across\ndomains to maximize diversity disregarding any notion of\ndifficulty. This approach also leads to poor performance\nsimilar to 1K-random.Only Difficulty (1K-longest): Here\nwerelyononeofourdifficultyindicatorsintroducedin2by\nselectingthe1,000sampleswiththelongestreasoningtraces.\nThis approach significantly boosts GPQA performance but\noverall still falls short of usings1K.Maximize Quantity:\nTable 2.s1K data ablations.We budget force (BF) a maximum\nof around 30,000 thinking tokens for all scores in this table. This\nperforms slightly better than the scores without BF (Table 1) as\nit allows the model to finish with a best guess when stuck in an\ninfinite loop. We report 95% paired bootstrap confidence intervals\nfor differences relative to thes1K model using 10,000 bootstrap\nsamples. E.g., the interval [-13%, 20%] means that, with 95% con-\nfidence, the true difference between 59K-full ands1Kis between\n-13% and +20%. If the entire interval is negative, e.g. [-27%, -3%],\nwe can confidently say that the performance is worse thans1K.\nModel AIME\n2024\nMATH\n500\nGPQA\nDiamond\n1K-random 36.7 90.6 52.0\n[-26.7%, -3.3%] [-4.8%, 0.0%] [-12.6%, 2.5%]\n1K-diverse 26.7 91.2 54.6\n[-40.0%, -10.0%] [-4.0%, 0.2%] [-10.1%, 5.1%]\n1K-longest 33.3 90.4 59.6\n[-36.7%, 0.0%] [-5.0%, -0.2%] [-5.1%, 10.1%]\n59K-full 53.3 92.8 58.1\n[-13.3%, 20.0%] [-2.6%, 2.2%] [-6.6%, 8.6%]\ns1K 50.0 93.0 57.6\nFinally, we compare with just training on all of our 59K\nsamples, a superset of all the 1K-sample versions. This\nleads to a strong model but uses much more resources. To\nfinetune on 59K samples, we use 394 H100 GPU hours\nwhiles1-32Bonly required 7 H100 GPU hours. Moreover,\nrelyingonlyon s1Kisextremelycompetitiveasshownin2.\nOverall, combining all three criteria Quality, Difficulty,\nDiversity  via our methodology in 2 is key for sample-\nefficient reasoning training.\n5.2. Test-time scaling methods\nTable 3.Ablations on methods to scale test-time compute on\nAIME24. |\ue22d| refers to the number of evaluation runs used to\nestimate the properties; thus a higher value indicates more robust-\nness. Bold indicates our chosen method and the best values. BF\n= budget forcing, TCC/SCC/CCC = token/step/class-conditional\ncontrol, RS = rejection sampling.\nMethod Control Scaling Performance |\ue22d|\nBF 100% 15 56.7 5\nTCC 40% -24 40.0 5\nTCC + BF 100% 13 40.0 5\nSCC 60% 3 36.7 5\nSCC + BF 100% 6 36.7 5\nCCC 50% 25 36.7 2\nRS 100% -35 40.0 5\n6s1: Simple test-time scaling\n[AIME24]\nAlice and Bob play the following game. A stack of\ntokens lies before them. The players take turns\nwith Alice going first. On each turn, the player\nremoves either1token or4tokens from the stack.\nWhoever removes the last token wins. Find the\nnumber of positive integersless than or equal to\n2024for which there exists a strategy for Bob that\nguarantees that Bob will win the game regardless\nof Alices play.\nThe problem asks for the number of positive integers\n 2024such that Bob has a winning strategy. This\nis a typical impartial game that can be analyzed using\nthe Sprague-Grundy theorem or by directly finding\nthe winning and losing positions. Let be the set of\nwinning positions (number of tokenssuch that the\nplayer whose turn it is can force a win) andbe the\nset of losing positions. [...]\nLet()be the statement "the player whose turn it is\nwhen there aretokens has a winning strategy.(0)\nis false (the player whose turn it is and faces0tokens\nhas already lost). Consider the small values of:\n = 1: The player whose turn it is can take1token\nand win. So(1)is true.1 .\n = 2: The player whose turn it is can take1token,\nleaving1token for the opponent. Since1 , the\nopponent wins. So(2)is false.2 .\n = 3: The player whose turn it is can take1token,\nleaving2tokens for the opponent. Since2 , the\nplayer whose turn it is wins. So(3)is true. 3 \n.[...]\nWe are looking for the number of positive integers\n 2024for which Bob has a winning strategy. This\nmeans that if the game starts withtokens, and Alice\ngoes first, Bob can win. This happens if and only if\nis a losing pos', 'So(3)is true. 3 \n.[...]\nWe are looking for the number of positive integers\n 2024for which Bob has a winning strategy. This\nmeans that if the game starts withtokens, and Alice\ngoes first, Bob can win. This happens if and only if\nis a losing position (because if, then Alices\nfirst move must lead to a winning position for Bob).\n[...]\nLets try to find a pattern in the losing positions\n={0,2,5,7,10,12,15,17,20,22,...}. Differences\nbetween consecutive terms: 2,3,2,3,2,3,2,3,2,...\nThe pattern of differences is alternating2,3. [...]\nThe losing positions are of the form5or 5+2,\nwhere  0. We need to find the number of positive\nintegers 2024thatareinthesetoflosingpositions\n. [...]\nCase 1:  = 5, where  1. 1  5  2024\n15    20245 = 404.8Since is an integer,\n1  404. The number of values ofis404.\nCase 2: = 5+2, where  0. 5+2  2024\n5  2022  20225 = 404.4Since is an in-\nteger, 0    404. The number of values ofis\n4040+1=405 .\nThe total number of positive integers  2024that\nare losing positions is404+405= 809 .\n[MATH500]\nFind the projection ofonto  =(2\n6\n3\n)\nif=8.\nTheprojectionofvector  ontovector\n is given by the formula:\nproj =  \n2\nWearegiven  =\n(2\n6\n3\n)\nand  =8.\nFirst, we need to find the magnitude\nsquared of, which is2.\n2 = =22+62+32 =4+36+9=49 .\nNow we can plug the values into the\nprojection formula:\nproj = 8\n49\nSubstitute the vector:\nproj = 8\n49\n(2\n6\n3\n)\nMultiply the scalar by each compo-\nnent of the vector:\nproj =\n\n\n\n\n8\n49 2\n8\n49 6\n8\n49 3\n\n\n\n\n=\n\n\n\n\n16\n4948\n4924\n49\n\n\n\n\nFinal Answer: The final answer is\n\n\n\n\n16\n4948\n4924\n49\n\n\n\n\n[GPQA]\nA quantum mechanical particle of\nmassmoves in two dimensions in\nthe following potential, as a function\nof the polar coordinates(,):\n(,)= 1\n22 + 3\n22cos2()\nFind the energy spectrum. Hint:\nWrite the potential in Cartesian co-\nordinates.\nThe problem asks for the energy spec-\ntrum of a particle in a given 2D po-\ntential. The potential is given in polar\ncoordinates(,):\n(,)= 1\n22 + 3\n22cos2().\nWe can rewrite the potential as:\n(,)= 1\n22(1+3cos 2()).\nThe Hamiltonian for a particle in 2D\nis given by:\n = 2\n2\n(\n2\n2 + 2\n2\n)\n+(,).\nIn polar coordinates(,), the Lapla-\ncian operator is:\n2 = 1\n\n\n\n(\n \n\n)\n+ 1\n2\n2\n2.\nSo the Hamiltonian in polar coordi-\nnates is:\n = 2\n2\n[\n1\n\n\n\n(\n \n\n)\n+ 1\n2\n2\n2\n]\n+(,).\nThe time-independent Schrdinger\nequation is = . We can try\nto separate variables by assuming a so-\nlution of the form(,)= ()().\n[...]\nThis confirms the energy spectrum is:\n =\n\n\n\n(\n2 + + 3\n2\n)\n.\nFinal Answer:  =\n\n\n\n\n(\n2 + + 3\n2\n)\n.\nFigure 5.Example model outputs.We pick a question from AIME24 (left), MATH500 (middle), and GPQA (right), where our model\ngenerates the correct answer. The black text is the prompt, the light blue text is the reasoning trace, and the blue text is the answer of\ns1-32B. The gray ellipsis [...] indicates that the text was trimmed to fit this page, but the generated text is actually longer.\n7s1: Simple test-time scaling\nBudget forcing In Table 3 we compare the test-time scal-\ning methods we have introduced in 3. Overall, we find that\nbudget forcingprovides perfect control, good scaling, and\nleads to our best AIME24 score. Thus, this is the method\nwe use fors1-32B in Figure 1 and in 4. In Table 4, we\ncomparedifferentstringsforextrapolatingperformance. We\nfind that Wait generally gives the best performance.\nClass-conditional controlWe provide benchmark scores\nfor this method in E.1 and summarize three findings here:\n(1)Token-conditionalcontrolfailswithoutbudgetforcing,as\nour model cannot reliably count tokens - even when trained\nto do so.(2) Under step-conditional control, the model gen-\nerates a similar total number of tokens when given different\nstep targets, as the model goes from few steps with many\ntokens per step, to many steps with few tokens in each step.\nThus, the model learns to hack its way around the com-\npute constraint making the controllability of this method\nmediocre. (3) Class-conditional control can work - telling a\nmodel to simply think longer can increase its test-time com-\npute and performance, which leads good scaling in Table 3.\nTable 4.Budget forcing extrapolation ablations.We compare\nignoring the end-of-thinking delimiter twice and appending none\nor various strings.\nModel AIME\n2024\nMATH\n500\nGPQA\nDiamond\nNo extrapolation 50.0 93.0 57.6\n2x without string 50.0 90.2 55.1\n2x Alternatively 50.0 92.2 59.6\n2x Hmm 50.0 93.0 59.6\n2x Wait 53.3 93.0 59.6\nRejectionsampling Surprisingly,wefindthatsimplysam-\npling until the generation fits a specific length leads to an\ninverse scaling trend as depicted in Figure 6. In E.2 we in-\n3072 4096 5120\nAverage thinking time (tokens)\n22\n27\n32\n37\n42Accuracy (%)\nFigure 6.Rejection sampling on AIME24 with s1-32B.We sam-\nplewithatemperatureof1untilallgenerationshavelessthan(from\nleft to right) 3500, 4000, 5000,', 'in Figure 6. In E.2 we in-\n3072 4096 5120\nAverage thinking time (tokens)\n22\n27\n32\n37\n42Accuracy (%)\nFigure 6.Rejection sampling on AIME24 with s1-32B.We sam-\nplewithatemperatureof1untilallgenerationshavelessthan(from\nleft to right) 3500, 4000, 5000, 8000, and 16000 thinking tokens\nrequiring an average of 655, 97, 8, 3, 2, and 1 tries per sample.\nspectaquestion,whichwasansweredcorrectlybythemodel\nwhen rejection sampling for 4000, but not for the 8000\ntokensetting. Inthe  4000settingthemodeldirectlyjumps\nto the correct approach, while for the 8000setting it back-\ntracks a lot. We hypothesize that there is a correlation such\nthat shorter generations tend to be the ones where the model\nwas on the right track from the start, whereas longer ones\ntend to be ones where the model made mistakes and thus\nbacktracks or questions itself. This leads to longer samples\noften being wrong when rejection sampling and thus the\ninverse scaling trend.\n6. Discussion and related work\n6.1. Sample-efficient reasoning\nModels There are a number of concurrent efforts to build\nmodelsthatreplicatetheperformanceofo1(OpenAI,2024).\nFor example, DeepSeek-r1 and k1.5 (DeepSeek-AI et al.,\n2025; Team et al., 2025) are built with reinforcement learn-\ning methods, while others rely on SFT using tens of thou-\nsands of distilled examples (Team, 2025; Xu et al., 2025;\nLabs, 2025). We show that SFT on only 1,000 examples\nsuffices to build a competitive reasoning model matching\no1-preview and produces a model that lies on the pareto\nfrontier (Figure 2). Further, we introduce budget forcing\nwhich combined with our reasoning model leads to the first\nreproduction of OpenAIs test-time scaling curves (OpenAI,\n2024). Why does supervised finetuning on just 1,000 sam-\nples lead to such performance gains? We hypothesize that\nthe model is already exposed to large amounts of reason-\ning data during pretraining which spans trillions of tokens.\nThus, the ability to perform reasoning is already present in\nour model. Our sample-efficient finetuning stage just acti-\nvatesitandwescaleitfurtherattesttimewithbudgetforcing.\nThis is similar to the "Superficial Alignment Hypothesis"\npresented in LIMA (Zhou et al., 2023), where the authors\nfind that 1,000 examples can be sufficient to align a model\nto adhere to user preferences.\nBenchmarks and methods To evaluate and push the lim-\nits of these models, increasingly challenging benchmarks\nhave been introduced, such as Olympiad-level science com-\npetitions (He et al., 2024; Jain et al., 2024; Zhong et al.,\n2023)andothers(Srivastavaetal.,2023;Glazeretal.,2024;\nSu et al., 2024; Kim et al., 2024; Phan et al., 2025). To\nenhancemodelsperformanceonreasoning-relatedtasks,re-\nsearchers have pursued several strategies: Prior works have\nexplored continuing training language models on special-\nized corpora related to mathematics and science (Azerbayev\netal.,2023;Yangetal.,2024),sometimesevensynthetically\ngenerated data (Yu et al., 2024). Others have developed\ntraining methodologies specifically aimed at reasoning per-\nformance (Zelikman et al., 2022; 2024; Luo et al., 2025;\n8s1: Simple test-time scaling\nYuanetal.,2025;Wuetal.,2024a). Anothersignificantline\nof work focuses on prompting-based methods to elicit and\nimprove reasoning abilities, including methods like Chain-\nof-Thought prompting (Wei et al., 2023; Yao et al., 2023a;b;\nBi et al., 2023; Fu et al., 2023; Zhang et al., 2024b; Xi-\nang et al., 2025; Hu et al., 2024; Diao et al., 2024). These\ncombined efforts aim to advance the reasoning ability of\nlanguage models, enabling them to handle more complex\nand abstract tasks effectively.\n6.2. Test-time scaling\nMethods Asweintroducein3,wedifferentiatetwometh-\nodstoscaletest-timecompute: parallelandsequential. The\nformer relies on multiple solution attempts generated in par-\nallel and selecting the best outcome via specific criteria.\nThese criteria include choosing the most frequent response\nfor majority voting or the best response based on an exter-\nnal reward for Best-of-N (Brown et al., 2024; Irvine et al.,\n2023; Levi, 2024). Unlike repeated sampling, previous se-\nquential scaling methods let the model generate solution\nattempts sequentially based on previous attempts, allowing\nit to refine each attempt based on previous outcomes (Snell\net al., 2024; Hou et al., 2025; Lee et al., 2025). Tree-based\nsearch methods (Gandhi et al., 2024; Wu et al., 2024b) offer\na hybrid approach between sequential and parallel scaling,\nsuch as Monte-Carlo Tree Search (MCTS) (Liu et al., 2024;\nZhang et al., 2023; Zhou et al., 2024; Choi et al., 2023) and\nguided beam search (Xie et al., 2023).REBASE (Wu et al.,\n2024b) employs a process reward model to balance exploita-\ntion and pruning during tree search. Empirically,REBASE\nhas been shown to outperform sampling-based methods and\nMCTS (Wu et al., 2024b). Reward models (Lightman et al.,\n2023; Wang et al., 2024b;c) play a key role in these meth-\nods. They come in two variants: outcome reward models\nand p', 'Empirically,REBASE\nhas been shown to outperform sampling-based methods and\nMCTS (Wu et al., 2024b). Reward models (Lightman et al.,\n2023; Wang et al., 2024b;c) play a key role in these meth-\nods. They come in two variants: outcome reward models\nand process reward models. Outcome reward models (Xin\net al., 2024; Ankner et al., 2024) assign a score to complete\nsolutions and are particularly useful in Best-of-N selection,\nwhile process reward models (Lightman et al., 2023; Wang\net al., 2024b; Wu et al., 2024b) assess individual reasoning\nsteps and are effective in guiding tree-based search methods.\nLimits to further test-time scalingWe have shown that\nbudget forcing allows extrapolating test-time compute in\n4, e.g., improving AIME24 performance from 50% to 57%.\nHowever, it has two key limitations when scaling further: it\neventuallyflattens out(Figure 4), and thecontext window\nof the underlying language model constrains it. Despite\nthese constraints, our work shows test-time scaling across a\nwide range of accuracies (Figure 1), partly because scaling\ndown test-time compute behaves predictably and does not\nsuffer from these constraints.\nContinuing test-time scaling will require approaches that\ncan further extrapolate test-time compute. How can we get\n2048 8192 32768 131072\nAverage thinking time (tokens)\n30\n40\n50\n60Accuracy (%)\nREBASE\nMajority\nSequential\nscaling\nFigure 7.Scalingfurtherwithparallelscalingmethods. Allmet-\nrics averaged over the 30 questions in AIME24. Average thinking\ntokensfor REBASE donotaccountfortheadditionalcomputefrom\nthe reward model. For sequential scaling, we prompt the model\nto use up to (from left to right) 32, 64, 256, and 512 steps. For\nREBASE and majority voting we generate 16 parallel trajectories\nto aggregate across.\nsuch extrapolation? There may be improvements to budget\nforcing such as rotating through different strings, not only\nWait, or combining it with frequency penalties or higher\ntemperature to avoid repetitive loops. An exciting direc-\ntion for future work is also researching whether applying\nbudget forcing to a reasoning model trained with reinforce-\nmentlearningyieldsbetterextrapolation; orifRLallowsfor\nnew ways of test-time scaling beyond budget forcing. Our\nwork definestheright metrics (3.2) Control, Scaling, and\nPerformance  to enable future research and progress on\nextrapolating test-time compute.\nParallel scaling as a solutionParallel scaling offers one\nsolution to the limits of sequential scaling, thus we aug-\nment our sequentially scaled model with two methods:(I)\nMajority voting: After generating solutions, the final\nsolution is the most frequent one across generations;(II)\nTree search viaREBASE:We use theREBASE process\nreward model, which is initialized from LLaMA-34B and\nfurther finetuned on a synthetic process reward modeling\ndataset (Wu et al., 2024b). We then aggregate the solutions\ngenerated byREBASE via majority voting. As shown in\nFigure7,augmentingourmodelwith REBASE scalesbetter\nthan majority voting, and even sequential scaling in this sce-\nnario. However,REBASE requires an additional forward\npassateachstepfortherewardmodeladdingsomecomputa-\ntionoverhead. Forsequentialscaling,whenpromptedtouse\nup to 512 steps, for 12 out of the 30 evaluation questions the\nmodelgeneratesaresponsethatexceedsthecontextwindow\nleading to a large performance drop. Overall, we find that\nthese parallel scaling methods complement sequential scal-\ning thus they offer an avenue for scaling test-time compute\neven further; beyond fixed context windows.\n9s1: Simple test-time scaling\nImpact Statement\nLanguage models with strong reasoning capabilities have\nthe potential to greatly enhance human productivity, from\nassisting in complex decision-making to driving scientific\nbreakthroughs. However,recentadvancesinreasoning,such\nas OpenAIs o1 and DeepSeeks r1, lack transparency, limit-\ning broader research progress. Our work aims to push the\nfrontier of reasoning in a fully open manner, fostering in-\nnovation and collaboration to accelerate advancements that\nultimately benefit society.\nAcknowledgements\nWethankRyanMartenforgeneratingtracesfromDeepSeek\nr1 for s1.1 using Bespoke Curator (Marten et al., 2025).\nThisworkwaspartlyconductedusingtheStanfordMarlowe\nGPUcluster(Kapferetal.,2025),madepossiblebyfinancial\nsupport from Stanford University. We thank Alexander M.\nRush, Andrew Ilyas, Banghua Zhu, Chenglei Si, Chunting\nZhou, John Yang, Ludwig Schmidt, Samy Jelassi, Suhas\nKotha,TengyuMa,XuechenLi,YuSun,andYueZhangfor\nvery constructive discussions.\nReferences\nAnkner, Z., Paul, M., Cui, B., Chang, J. D., and Am-\nmanabrolu, P. Critique-out-loud reward models, 2024.\nURL https://arxiv.org/abs/2408.11791.\nArora, D., Singh, H. G., and Mausam. Have llms advanced\nenough? a challenging problem solving benchmark for\nlarge language models, 2023. URLhttps://arxiv.\norg/abs/2305.15074.\nAzerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D.,\nMcAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and\nWell', 'llms advanced\nenough? a challenging problem solving benchmark for\nlarge language models, 2023. URLhttps://arxiv.\norg/abs/2305.15074.\nAzerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D.,\nMcAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and\nWelleck, S. Llemma: An open language model for math-\nematics, 2023.\nBi, Z., Zhang, N., Jiang, Y., Deng, S., Zheng, G., and\nChen, H. When do program-of-thoughts work for reason-\ning?,2023. URLhttps://arxiv.org/abs/2308.\n15452.\nBiderman, S., Schoelkopf, H., Sutawika, L., Gao, L., Tow,\nJ., Abbasi, B., Aji, A. F., Ammanamanchi, P. S., Black,\nS., Clive, J., DiPofi, A., Etxaniz, J., Fattori, B., Forde,\nJ. Z., Foster, C., Hsu, J., Jaiswal, M., Lee, W. Y., Li, H.,\nLovering, C., Muennighoff, N., Pavlick, E., Phang, J.,\nSkowron, A., Tan, S., Tang, X., Wang, K. A., Winata,\nG. I., Yvon, F., and Zou, A. Lessons from the trenches on\nreproducible evaluation of language models, 2024.\nBrown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., R,\nC., and Mirhoseini, A. Large language monkeys: Scaling\ninference compute with repeated sampling, 2024. URL\nhttps://arxiv.org/abs/2407.21787.\nCesista, F.L. Multimodalstructuredgeneration: Cvprs2nd\nmmfm challenge technical report, 2024. URLhttps:\n//arxiv.org/abs/2406.11403.\nChen, W., Yin, M., Ku, M., Lu, P., Wan, Y., Ma, X.,\nXu, J., Wang, X., and Xia, T. Theoremqa: A theorem-\ndriven question answering dataset, 2023. URLhttps:\n//arxiv.org/abs/2305.12524.\nChoi,S.,Fang,T.,Wang,Z.,andSong,Y. Kcts: Knowledge-\nconstrained tree search decoding with token-level halluci-\nnation detection, 2023. URLhttps://arxiv.org/\nabs/2310.09044.\nDeepSeek-AI,Guo,D.,Yang,D.,Zhang,H.,Song,J.,Zhang,\nR., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X.,\nYu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao,\nZ., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu,\nC., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D.,\nChen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao,\nG., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang,\nH., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J.,\nLi, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai,\nJ. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao,\nK., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L.,\nZhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang,\nM., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian,\nN., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q.,\nGe, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin,\nR. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang,\nS., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu,\nS., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W.,\nZhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang,\nW., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X.,\nNie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X.,\nLi, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen,\nX., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X.,\nShan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y.,\nXu,Y., Li,Y., Zhao,Y., Sun,Y., Wang, Y.,Yu, Y.,Zhang,\nY., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan,\nY., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong,\nY., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y.,\nZhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng,\nY., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z.,\nRen, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao,\nZ., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li,\nZ., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang,\nZ., and Zhang, Z. Deepseek-r1: Incentivizing reasoning\ncapability in llms via reinforcement learning, 2025. URL\nhttps://arxiv.org/abs/2501.12948.\nDiao, S., Wang, P., Lin, Y., Pan, R., Liu, X., and Zhang,\nT. Active prompting with chain-of-thought for large lan-\n10s1: Simple test-time scaling\nguage models, 2024. URL https://arxiv.org/\nabs/2302.12246.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A.,\nLetman, A., Mathur, A., Schelten, A., Yang, A., Fan, A.,\nGoyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravanku-\nmar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A.,\nRodriguez, A., Gregerson, A., et al. The llama 3 herd\nof models, 2024. URLhttps://arxiv.org/abs/\n2407.21783.\nFu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot,\nT. Complexity-based prompting for multi-step reason-\ning, 2023. URLhttps://arxiv.org/abs/2210.\n00720.\nGandhi,K.,Lee,D.,Grand,G.,Liu,M.,Cheng,W.,Sharma,\nA.,andGoodman,N.D. Streamofsearch(sos): Learning\nto search in language, 2024. URLhttps://arxiv.\norg/abs/2404.03683.\nGao, B., Song, F., Yang, Z., Cai, Z., Miao, Y., Dong, Q., Li,\nL., Ma, C., Chen, L., Xu, R., Tang, Z., Wang, B., Zan, D.,\nQuan, S., Zhang, G., Sha, L., Zhang, Y., Ren, X., Liu, T.,\nand Chang, B. Omni-math: A universal olympiad level\nmathematic benchmark for large language models, 2024a.\nURL https://arxiv.org/abs/2410.07985.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,\nC., Golding, L., Hsu, J., McDonell, K., Muennighoff, N.,\nPhang, J., Reynolds, L., Tang, E., Thite, A., Wang, B.,\n', 'k for large language models, 2024a.\nURL https://arxiv.org/abs/2410.07985.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,\nC., Golding, L., Hsu, J., McDonell, K., Muennighoff, N.,\nPhang, J., Reynolds, L., Tang, E., Thite, A., Wang, B.,\nWang,K.,andZou,A.Aframeworkforfew-shotlanguage\nmodel evaluation, September 2021. URLhttps://\ndoi.org/10.5281/zenodo.5371628.\nGao,Z.,Niu,B.,He,X.,Xu,H.,Liu,H.,Liu,A.,Hu,X.,and\nWen, L. Interpretable contrastive monte carlo tree search\nreasoning,2024b. URLhttps://arxiv.org/abs/\n2410.01707.\nGlazer, E., Erdil, E., Besiroglu, T., Chicharro, D., Chen,\nE., Gunning, A., Olsson, C. F., Denain, J.-S., Ho, A.,\nde Oliveira Santos, E., Jrviniemi, O., Barnett, M., San-\ndler, R., Vrzala, M., Sevilla, J., Ren, Q., Pratt, E., Levine,\nL., Barkley, G., Stewart, N., Grechuk, B., Grechuk, T.,\nEnugandla,S.V.,andWildon,M. Frontiermath: Abench-\nmark for evaluating advanced mathematical reasoning in\nai, 2024. URL https://arxiv.org/abs/2411.\n04872.\nGoogle. Gemini 2.0 flash thinking mode (gemini-2.0-\nflash-thinking-exp-1219), December 2024. URL\nhttps://cloud.google.com/vertex-ai/\ngenerative-ai/docs/thinking-mode.\nGroeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney,\nR., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I.,\nWang, Y., Arora, S., Atkinson, D., Authur, R., Chandu,\nK.R., Cohan,A., Dumas,J., Elazar, Y., Gu,Y., Hessel,J.,\nKhot,T.,Merrill,W.,Morrison,J.,Muennighoff,N.,Naik,\nA., Nam, C., Peters, M. E., Pyatkin, V., Ravichander, A.,\nSchwenk,D.,Shah,S.,Smith,W.,Strubell,E.,Subramani,\nN., Wortsman, M., Dasigi, P., Lambert, N., Richardson,\nK.,Zettlemoyer,L.,Dodge,J.,Lo,K.,Soldaini,L.,Smith,\nN. A., and Hajishirzi, H. Olmo: Accelerating the science\nof language models, 2024.\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J.,\nHu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L.,\nLiu, Z., and Sun, M. Olympiadbench: A challeng-\ning benchmark for promoting agi with olympiad-level\nbilingual multimodal scientific problems, 2024. URL\nhttps://arxiv.org/abs/2402.14008.\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS.,Tang,E.,Song,D.,andSteinhardt,J. Measuringmath-\nematical problem solving with the math dataset, 2021.\nURL https://arxiv.org/abs/2103.03874.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai,T.,Rutherford,E.,deLasCasas,D.,Hendricks,L.A.,\nWelbl, J., Clark, A., Hennigan, T., Noland, E., Millican,\nK., van den Driessche, G., Damoc, B., Guy, A., Osindero,\nS., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and\nSifre, L. Training compute-optimal large language mod-\nels, 2022. URLhttps://arxiv.org/abs/2203.\n15556.\nHou, Z., Lv, X., Lu, R., Zhang, J., Li, Y., Yao, Z., Li, J.,\nTang, J., and Dong, Y. Advancing language model rea-\nsoning through reinforcement learning and inference scal-\ning, 2025. URLhttps://arxiv.org/abs/2501.\n11651.\nHu, Y., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettle-\nmoyer, L., Smith, N. A., and Krishna, R. Visual sketch-\npad: Sketchingasavisualchainofthoughtformultimodal\nlanguage models, 2024. URLhttps://arxiv.org/\nabs/2406.09403.\nHuang, Z., Wang, Z., Xia, S., Li, X., Zou, H., Xu, R., Fan,\nR.-Z., Ye, L., Chern, E., Ye, Y., Zhang, Y., Yang, Y., Wu,\nT.,Wang,B.,Sun,S.,Xiao,Y.,Li,Y.,Zhou,F.,Chern,S.,\nQin, Y., Ma, Y., Su, J., Liu, Y., Zheng, Y., Zhang, S., Lin,\nD., Qiao, Y., and Liu, P. Olympicarena: Benchmarking\nmulti-discipline cognitive reasoning for superintelligent\nai, 2024a. URLhttps://arxiv.org/abs/2406.\n12753.\nHuang, Z., Zou, H., Li, X., Liu, Y., Zheng, Y., Chern, E.,\nXia, S., Qin, Y., Yuan, W., and Liu, P. O1 replication\njourney  part 2: Surpassing o1-preview through simple\ndistillation, big progress or bitter lesson?, 2024b. URL\nhttps://arxiv.org/abs/2411.16489.\n11s1: Simple test-time scaling\nHuang, Z., Geng, G., Hua, S., Huang, Z., Zou, H., Zhang,\nS., Liu, P., and Zhang, X. O1 replication journey  part 3:\nInference-time scaling for medical reasoning, 2025. URL\nhttps://arxiv.org/abs/2501.06458.\nIrvine, R., Boubert, D., Raina, V., Liusie, A., Zhu, Z.,\nMudupalli, V., Korshuk, A., Liu, Z., Cremer, F., As-\nsassi, V., Beauchamp, C.-C., Lu, X., Rialan, T., and\nBeauchamp, W. Rewarding chatbots for real-world en-\ngagement with millions of users, 2023. URLhttps:\n//arxiv.org/abs/2303.06135.\nJain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang,\nT., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I.\nLivecodebench: Holistic and contamination free evalu-\nation of large language models for code, 2024. URL\nhttps://arxiv.org/abs/2403.07974.\nKapfer, C., Stine, K., Narasimhan, B., Mentzel, C., and\nCandes,E. Marlowe: Stanfordsgpu-basedcomputational\ninstrument, January 2025. URLhttps://doi.org/\n10.5281/zenodo.14751899.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J.,\nand Amodei, D. Scaling laws for neural language mod-\nels, 2020. URLhttps://arxiv.org/abs/2001.\n08361.\nKim,E.,Suk,J.,Kim,S.,Muennighoff,N.,Kim,D.,andOh,\nA. Llm-as-an-interviewer: Beyond static testing through\ndynamic llm ev', 'y, S., Radford, A., Wu, J.,\nand Amodei, D. Scaling laws for neural language mod-\nels, 2020. URLhttps://arxiv.org/abs/2001.\n08361.\nKim,E.,Suk,J.,Kim,S.,Muennighoff,N.,Kim,D.,andOh,\nA. Llm-as-an-interviewer: Beyond static testing through\ndynamic llm evaluation, 2024. URLhttps://arxiv.\norg/abs/2412.10424.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Effi-\ncientmemorymanagementforlargelanguagemodelserv-\ning with pagedattention, 2023. URLhttps://arxiv.\norg/abs/2309.06180.\nLabs, B. Bespoke-stratos: The unreasonable effectiveness\nof reasoning distillation, 2025. URL https://hf.\nco/bespokelabs/Bespoke-Stratos-32B. Ac-\ncessed: 2025-01-22.\nLee, K.-H., Fischer, I., Wu, Y.-H., Marwood, D., Baluja,\nS., Schuurmans, D., and Chen, X. Evolving deeper llm\nthinking, 2025. URL https://arxiv.org/abs/\n2501.09891.\nLevi, N. A simple model of inference scaling laws, 2024.\nURL https://arxiv.org/abs/2410.16377.\nLI, J., Beeching, E., Tunstall, L., Lipkin, B., Solet-\nskyi, R., Huang, S. C., Rasul, K., Yu, L., Jiang, A.,\nShen, Z., Qin, Z., Dong, B., Zhou, L., Fleureau, Y.,\nLample, G., and Polu, S. Numinamath, 2024. URL\nhttps://github.com/project-numina/\naimo-progress-prize/blob/main/report/\nnumina_dataset.pdf.\nLightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker,\nB., Lee, T., Leike, J., Schulman, J., Sutskever, I., and\nCobbe, K. Lets verify step by step, 2023. URLhttps:\n//arxiv.org/abs/2305.20050.\nLing,W.,Yogatama,D.,Dyer,C.,andBlunsom,P. Program\ninduction by rationale generation : Learning to solve and\nexplain algebraic word problems, 2017. URLhttps:\n//arxiv.org/abs/1705.04146.\nLiu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang,\nY. Logiqa: A challenge dataset for machine reading com-\nprehension with logical reasoning, 2020. URLhttps:\n//arxiv.org/abs/2007.08124.\nLiu, J., Cohen, A., Pasunuru, R., Choi, Y., Hajishirzi,\nH., and Celikyilmaz, A. Dont throw away your value\nmodel! generatingmorepreferabletextwithvalue-guided\nmonte-carlo tree search decoding, 2024. URLhttps:\n//arxiv.org/abs/2309.15028.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization, 2019.\nLuo,H.,Sun,Q.,Xu,C.,Zhao,P.,Lou,J.,Tao,C.,Geng,X.,\nLin, Q., Chen, S., Tang, Y., and Zhang, D. Wizardmath:\nEmpowering mathematical reasoning for large language\nmodels via reinforced evol-instruct, 2025. URLhttps:\n//arxiv.org/abs/2308.09583.\nMarten, R., Vu, T., Ji, C. C.-J., Sharma, K., Pimpalgaonkar,\nS., Dimakis, A., and Sathiamoorthy, M. Curator: A tool\nfor synthetic data creation.https://github.com/\nbespokelabsai/curator, January 2025.\nMuennighoff, N., Soldaini, L., Groeneveld, D., Lo, K., Mor-\nrison,J.,Min,S.,Shi,W.,Walsh,P.,Tafjord,O.,Lambert,\nN., Gu, Y., Arora, S., Bhagia, A., Schwenk, D., Wadden,\nD., Wettig, A., Hui, B., Dettmers, T., Kiela, D., Farhadi,\nA., Smith, N. A., Koh, P. W., Singh, A., and Hajishirzi, H.\nOlmoe: Open mixture-of-experts language models, 2024.\nURL https://arxiv.org/abs/2409.02060.\nof America, M. A. Aime, February 2024. URLhttps:\n//artofproblemsolving.com/wiki/index.\nphp/AIME_Problems_and_Solutions/.\nOpenAI. Learning to reason with llms, Septem-\nber 2024. URL https://openai.com/index/\nlearning-to-reason-with-llms/ .\nOpenAI. Openaio3-mini,2025. URLhttps://openai.\ncom/index/openai-o3-mini/. Accessed: 2025-\n02-24.\n12s1: Simple test-time scaling\nPhan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Shi,\nS., Choi, M., Agrawal, A., Chopra, A., et al. Humanitys\nlast exam, 2025. URLhttps://arxiv.org/abs/\n2501.14249.\nQin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye,\nY., Yuan, W., Liu, H., Li, Y., and Liu, P. O1 replication\njourney: A strategic progress report  part 1, 2024. URL\nhttps://arxiv.org/abs/2410.18982.\nQwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng,\nB., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H.,\nYang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J.,\nLin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li,\nM., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li,\nT., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y.,\nWan,Y.,Liu,Y.,Cui,Z.,Zhang,Z.,andQiu,Z. Qwen2.5\ntechnical report, 2024. URLhttps://arxiv.org/\nabs/2412.15115.\nRein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y.,\nDirani, J., Michael, J., and Bowman, S. R. Gpqa: A\ngraduate-level google-proof q&a benchmark, 2023. URL\nhttps://arxiv.org/abs/2311.12022.\nShi, Q., Tang, M., Narasimhan, K., and Yao, S. Can lan-\nguagemodels solveolympiad programming?, 2024. URL\nhttps://arxiv.org/abs/2404.10952.\nSnell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-\ntime compute optimally can be more effective than scal-\ning model parameters, 2024. URLhttps://arxiv.\norg/abs/2408.03314.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., et al. Beyond the imitation game:\nQuantifyingandextrapolatingthecapabilitiesoflanguage\nmodels, 2023.\nSu,H.,Yen,H.,Xia,M.,Shi,W.,Muennighoff,N.,yuWang,\nH.,Liu,H.,Shi,Q', ' M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., et al. Beyond the imitation game:\nQuantifyingandextrapolatingthecapabilitiesoflanguage\nmodels, 2023.\nSu,H.,Yen,H.,Xia,M.,Shi,W.,Muennighoff,N.,yuWang,\nH.,Liu,H.,Shi,Q.,Siegel,Z.S.,Tang,M.,Sun,R.,Yoon,\nJ., Arik, S. O., Chen, D., and Yu, T. Bright: A realis-\ntic and challenging benchmark for reasoning-intensive\nretrieval, 2024. URL https://arxiv.org/abs/\n2407.12883.\nSun,L.,Han,Y.,Zhao,Z.,Ma,D.,Shen,Z.,Chen,B.,Chen,\nL., and Yu, K. Scieval: A multi-level large language\nmodel evaluation benchmark for scientific research, 2024.\nURL https://arxiv.org/abs/2308.13149.\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li,\nC., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang,\nD., Yuan, E., Lu, E., Tang, F., Sung, F., Wei, G., Lai, G.,\nGuo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H.,\nYao, H., Zhao, H., Lu, H., Li, H., Yu, H., Gao, H., Zheng,\nH., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J.,\nZhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S.,\nMa, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W.,\nXiong, W., He, W., Huang, W., Wu, W., He, W., Wei, X.,\nJia,X.,Wu,X.,Xu,X.,Zu,X.,Zhou,X.,Pan,X.,Charles,\nY.,Li,Y.,Hu,Y.,Liu,Y.,Chen,Y.,Wang,Y.,Liu,Y.,Qin,\nY., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y.,\nZhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang, Z., Wang, Z.,\nYang,Z.,Huang,Z.,Huang,Z.,Xu,Z.,andYang,Z. Kimi\nk1.5: Scaling reinforcement learning with llms, 2025.\nURL https://arxiv.org/abs/2501.12599.\nTeam, N. Sky-t1: Fully open-source reasoning\nmodel with o1-preview performance in $450 budget,\n2025. URL https://novasky-ai.github.io/\nposts/sky-t1. Accessed: 2025-01-09.\nTeam, Q. Qwq: Reflect deeply on the boundaries of the\nunknown, November 2024. URLhttps://qwenlm.\ngithub.io/blog/qwq-32b-preview/.\nWang, J., Meng, F., Liang, Y., and Zhou, J. Drt-o1: Op-\ntimized deep reasoning translation via long chain-of-\nthought, 2024a. URLhttps://arxiv.org/abs/\n2412.17498.\nWang, P., Li, L., Shao, Z., Xu, R. X., Dai, D., Li, Y., Chen,\nD., Wu, Y., and Sui, Z. Math-shepherd: Verify and rein-\nforcellmsstep-by-stepwithouthumanannotations,2024b.\nURL https://arxiv.org/abs/2312.08935.\nWang, S., Liu, Z., Zhong, W., Zhou, M., Wei, Z., Chen, Z.,\nand Duan, N. From lsat: The progress and challenges\nof complex reasoning, 2021. URLhttps://arxiv.\norg/abs/2108.00648.\nWang, Z., Dong, Y., Delalleau, O., Zeng, J., Shen, G., Egert,\nD., Zhang, J. J., Sreedhar, M. N., and Kuchaiev, O. Help-\nsteer2: Open-source dataset for training top-performing\nreward models, 2024c. URLhttps://arxiv.org/\nabs/2406.08673.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter,\nB., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-\nthought prompting elicits reasoning in large language\nmodels, 2023. URL https://arxiv.org/abs/\n2201.11903.\nWelleck,S.,Bertsch,A.,Finlayson,M.,Schoelkopf,H.,Xie,\nA., Neubig, G., Kulikov, I., and Harchaoui, Z. From de-\ncoding to meta-generation: Inference-time algorithms for\nlarge language models, 2024. URLhttps://arxiv.\norg/abs/2406.16838.\n13s1: Simple test-time scaling\nWu, T., Lan, J., Yuan, W., Jiao, J., Weston, J., and\nSukhbaatar, S. Thinking llms: General instruction fol-\nlowing with thought generation, 2024a. URLhttps:\n//arxiv.org/abs/2410.10630.\nWu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y. Infer-\nence scaling laws: An empirical analysis of compute-\noptimalinferenceforproblem-solvingwithlanguagemod-\nels,2024b. URLhttps://arxiv.org/abs/2408.\n00724.\nXiang, V., Snell, C., Gandhi, K., Albalak, A., Singh, A.,\nBlagden, C., Phung, D., Rafailov, R., Lile, N., Mahan,\nD., Castricato, L., Franken, J.-P., Haber, N., and Finn, C.\nTowards system 2 reasoning in llms: Learning how to\nthink with meta chain-of-thought, 2025. URLhttps:\n//arxiv.org/abs/2501.04682.\nXie, Y., Kawaguchi, K., Zhao, Y., Zhao, X., Kan, M.-Y., He,\nJ., and Xie, Q. Self-evaluation guided beam search for\nreasoning, 2023. URLhttps://arxiv.org/abs/\n2305.00633.\nXin, H., Guo, D., Shao, Z., Ren, Z., Zhu, Q., Liu, B., Ruan,\nC., Li, W., and Liang, X. Deepseek-prover: Advancing\ntheorem proving in llms through large-scale synthetic\ndata,2024. URLhttps://arxiv.org/abs/2405.\n14333.\nXu, H., Wu, X., Wang, W., Li, Z., Zheng, D., Chen, B.,\nHu, Y., Kang, S., Ji, J., Zhang, Y., Guo, Z., Yang, Y.,\nZhang,M.,andZhang,D. Redstar: Doesscalinglong-cot\ndata unlock better slow-reasoning systems?, 2025. URL\nhttps://arxiv.org/abs/2501.11284.\nYang, Z., Band, N., Li, S., Cands, E., and Hashimoto, T.\nSynthetic continued pretraining, 2024. URLhttps:\n//arxiv.org/abs/2409.07431.\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao,\nY., and Narasimhan, K. Tree of thoughts: Deliberate\nproblemsolvingwithlargelanguagemodels,2023a. URL\nhttps://arxiv.org/abs/2305.10601.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK., and Cao, Y. React: Synergizing reasoning and acting\nin language models, 2023b. URLhttps://arxiv.\norg/abs/2210', 'msolvingwithlargelanguagemodels,2023a. URL\nhttps://arxiv.org/abs/2305.10601.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK., and Cao, Y. React: Synergizing reasoning and acting\nin language models, 2023b. URLhttps://arxiv.\norg/abs/2210.03629.\nYe, Y., Huang, Z., Xiao, Y., Chern, E., Xia, S., and Liu, P.\nLimo: Less is more for reasoning, 2025a. URLhttps:\n//arxiv.org/abs/2502.03387.\nYe, Y., Xiao, Y., Mi, T., and Liu, P. Aime-preview: A\nrigorous and immediate evaluation framework for ad-\nvanced mathematical reasoning. https://github.\ncom/GAIR-NLP/AIME-Preview, 2025b. GitHub\nrepository.\nYu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok,\nJ.T.,Li,Z.,Weller,A.,andLiu,W. Metamath: Bootstrap\nyour own mathematical questions for large language mod-\nels, 2024. URLhttps://arxiv.org/abs/2309.\n12284.\nYuan, S., Chen, Z., Xi, Z., Ye, J., Du, Z., and Chen, J.\nAgent-r: Training language model agents to reflect via\niterative self-training, 2025. URLhttps://arxiv.\norg/abs/2501.11425.\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. D. Star:\nBootstrapping reasoning with reasoning, 2022. URL\nhttps://arxiv.org/abs/2203.14465.\nZelikman, E., Harik, G., Shao, Y., Jayasiri, V., Haber, N.,\nand Goodman, N. D. Quiet-star: Language models can\nteach themselves to think before speaking, 2024. URL\nhttps://arxiv.org/abs/2403.09629.\nZhang, H. and Chen, C. Test-time compute scaling laws,\n2024. URLhttps://github.com/hughbzhang/\no1_inference_scaling_laws.\nZhang, S., Chen, Z., Shen, Y., Ding, M., Tenenbaum, J. B.,\nand Gan, C. Planning with large language models for\ncode generation, 2023. URLhttps://arxiv.org/\nabs/2303.05510.\nZhang, Y., Wu, S., Yang, Y., Shu, J., Xiao, J., Kong, C., and\nSang, J. o1-coder: an o1 replication for coding, 2024a.\nURL https://arxiv.org/abs/2412.00154.\nZhang, Y., Yang, J., Yuan, Y., and Yao, A. C.-C. Cumula-\ntive reasoning with large language models, 2024b. URL\nhttps://arxiv.org/abs/2308.04371.\nZhong, H., Xiao, C., Tu, C., Zhang, T., Liu, Z., and Sun, M.\nJec-qa: A legal-domain question answering dataset, 2019.\nURL https://arxiv.org/abs/1911.12011.\nZhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang,\nY., Saied, A., Chen, W., and Duan, N. Agieval: A\nhuman-centric benchmark for evaluating foundation mod-\nels, 2023. URLhttps://arxiv.org/abs/2304.\n06364.\nZhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H.,\nand Wang, Y.-X. Language agent tree search unifies rea-\nsoning acting and planning in language models, 2024.\nURL https://arxiv.org/abs/2310.04406.\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M.,\nZettlemoyer, L., and Levy, O. Lima: Less is more for\nalignment, 2023. URLhttps://arxiv.org/abs/\n2305.11206.\n14s1: Simple test-time scaling\nContents\n1 Introduction 1\n2 Reasoning data curation to create s1K 2\n2.1 Initial collection of 59K samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n2.2 Final selection of 1K samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n3 Test-time scaling 3\n3.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n3.2 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n4 Results 4\n4.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n4.2 Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n5 Ablations 6\n5.1 Data Quantity, Diversity, and Difficulty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n5.2 Test-time scaling methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n6 Discussion and related work 8\n6.1 Sample-efficient reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n6.2 Test-time scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\nA s1.1 16\nB Evaluation determinism 16\nC s1K details 17\nC.1 s1K summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nC.2 Dataset composition for full 59K questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nC.3 s1K grading prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC.4 s1K diversity selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC.5 Decontamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nD Training details 21\nD.1 Training Ablations: Sequence length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nD.2 Training Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nE Test-time sca', 'D.1 Training Ablations: Sequence length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nD.2 Training Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nE Test-time scaling details 35\nE.1 Sequential scaling ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\nE.2 Examples for rejection sampling ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nF Version Control 46\n15s1: Simple test-time scaling\nA. s1.1\nSeven days after our release of s1, we released s1.1. We regenerated traces for our 1,000 samples ins1Kusing DeepSeek\nr1 (DeepSeek-AI et al., 2025) to creates1K-1.1. We use the same training procedure to train our models1.1. Other updates\nsince our launch include the release of o3 (OpenAI, 2025), LIMO (Ye et al., 2025a), and AIME 2025. We consider all these\nnew developments in Table 5. We find that s1.1 performs significantly better than s1. We also tried distilling from Claude\n3.7, which led to worse performance than from r1 (not reported).\nTable 5.s1-32B is an open and sample-efficient reasoning model.We evaluates1-32B, Qwen, and Gemini (some entries are unknown\n(N.A.), see 4). Other results are from the respective reports (Qwen et al., 2024; Team, 2024; OpenAI, 2024; 2025; DeepSeek-AI et al.,\n2025; Labs, 2025; Team, 2025) except for AIME 2025 (Ye et al., 2025b). # ex. = number examples used for reasoning finetuning; BF =\nbudget forcing.\nModel # Examples MATH500 GPQA AIME 2024 AIME 2025\nAPI only\no3-mini-low N/A 95.8 70.6 56.3 42.1\no3-mini-medium N/A 97.3 76.8 75.8 70.4\no3-mini-high N/A 97.9 79.7 83.8 80.9\nOpen Weights\nQwQ-32B N.A. 90.6 54.5 46.7 32.7\nr1 800K 07.3 71.5 79.8 70.0\nr1-distill-Llama-70B 800K 94.5 65.2 57.1 56.3\nr1-distill-Qwen-14B 800K 93.9 59.1 61.7 48.0\nr1-distill-Qwen-32B 800K 94.3 62.1 58.3 49.6\nOpen Weights and Open Data\nLIMO 817 94.8 66.7 56.3 44.6\ns1 w/o BF 1K 92.6 56.6 50.0 26.7\ns1 with Budget Forcing Wait 1x 1K 92.8 59.6 53.3 30.0\ns1 with Budget Forcing Wait 2x 1K 93.0 59.6 53.3 33.3\ns1 with Budget Forcing Wait 4x 1K 92.2 58.6 56.7 36.7\ns1.1 w/o BF 1K 94.4 60.6 56.7 50.0\ns1.1 with Budget Forcing Wait 1x 1K 95.4 62.6 56.7 50.0\ns1.1 with Budget Forcing Wait 2x 1K 95.4 63.6 56.7 50.0\nB. Evaluation determinism\nWe run our evaluations using vLLM (Kwon et al., 2023) as it is faster than the alternatives we tried. However, we find that\neven when using the same random seeds and greedy sampling, evaluation scores can change significantly across runs:\n Different batch sizes causing different results seehttps://github.com/vllm-project/vllm/issues/\n5898\n Continuing generations causing different results seehttps://github.com/vllm-project/vllm/issues/\n11783\n Changes in tensor parallelism causing different results\nAs our model generates long reasoning traces prior to its answer, small numeric changes can snowball into large differences.\nWe encounter many generations that are exactly the same for thousands of tokens and then suddenly differ in one token\neventually ending up with an entirely different answer. To partly counter this issue we generally run our final evaluations\nusing full precision unless otherwise indicated.\n16s1: Simple test-time scaling\nC. s1K details\nC.1. s1K summary\nTable 6.Summary of our dataset s1K. Token count measured by the Qwen-2.5 tokenizer. We prompt Claude to produce keywords given\nseveral questions from the domain.\nDomain #questions Total token count Keywords\nGeometry 109 560.2K Area, Triangle, Distance\nNumber theory 98 522.5K Sequences, Divisibility\nCombinatorics 75 384.7K Permutations, Counting\nReal functions 43 234.8K Trigonometry, Calculus\nBiology 41 120.9K Organic reactions\nComplex functions 32 170.2K Complex roots\nQuantum theory 32 127.9K Particles, Wave functions\nField theory 28 150.1K Polynomials, Roots\nCalculus of variations 28 155.5K Optimization, Control\nDifference equations 24 132.5K Recurrence, Recursion\nElectromagnetic theory 23 95.8K Optics, Waves, Diffraction\nGroup theory 22 100.0K Groups, Automorphisms\nLinear algebra 22 128.3K Matrices, Determinants\nProbability theory 20 114.6K Random walk, Expectation\nAlgebraic systems 19 109.9K Functional equations\nMechanics 19 103.6K Forces, Motion, Energy\nThermodynamics 19 74.2K Heat engines, Entropy\nDifferential equations 18 89.6K Substitution, Existence\nComputer science 18 34.2K Complexity theory, Algorithms\nNumerical analysis 18 76.5K Error analysis, Stability\nCalculus 17 96.3K Convergence, Summation\nAlgebraic structures 17 90.4K Inequalities, Sets\nAstronomy 16 37.7K Stellar populations, Orbits\nRemaining 27 domains 242 982.2K Domains with  16 questions\nAll domains (51) 1000 4.7M s1K\nC.2. Dataset composition for full 59K questions\n17s1: Simple test-time scaling\nTable 7.Composition of full 59K questions.Thinking and response lengths are measured in tokens using the Qwen2.5-32B-Instruct\ntokenizer (Qwen et al., 2024). In addition to excluding our eval', 'ition for full 59K questions\n17s1: Simple test-time scaling\nTable 7.Composition of full 59K questions.Thinking and response lengths are measured in tokens using the Qwen2.5-32B-Instruct\ntokenizer (Qwen et al., 2024). In addition to excluding our evaluation benchmark, AIME24, we also exclude AIME questions from\n2022-2023 as we use these 90 questions during our development stage ofs1-32B.\nSource Description #Samples Avg.\nthinking\nlength\nNuminaMATH (LI et al., 2024) Math problems from online websites 30660 4.1K\nMATH (Hendrycks et al., 2021) Math problems from competitions 11999 2.9K\nOlympicArena (Huang et al.,\n2024a)\nAstronomy, Biology, Chemistry, Com-\nputer Science, Geography, Math, and\nPhysics olympiad questions\n4250 3.2K\nOmniMath (Gao et al., 2024a) Math problems from competitions 4238 4.4K\nAGIEval (Zhong et al., 2023;\nLingetal.,2017;Hendrycksetal.,\n2021; Liu et al., 2020; Zhong\net al., 2019; Wang et al., 2021)\nEnglish, Law, Logic and Math prob-\nlems from the SAT, LSAT and other\nexams\n2385 1.2K\nxword Crossword puzzles 999 0.7K\nOlympiadBench (He et al., 2024) Math and Physics olympiad questions 896 3.9K\nAIME (1983-2021) AmericanInvitationalMathematicsEx-\namination\n890 4.7K\nTheoremQA (Chen et al., 2023) Computer Science, Finance, Math, and\nPhysics university-level questions relat-\ning to theorems\n747 2.1K\nUSACO (Shi et al., 2024) Code problems from the USA Comput-\ning Olympiad\n519 3.6K\nJEEBench (Arora et al., 2023) Chemistry,Math,andPhysicsproblems\nused in the university entrance exami-\nnation of the Indian Institute of Tech-\nnology\n515 2.9K\nGPQA (Rein et al., 2023) PhD-Level Science Questions 348 2.9K\nSciEval (Sun et al., 2024) Biology, Chemistry, and Physics prob-\nlems from various sources\n227 0.7K\ns1-prob Stanford statistics qualifying exams 182 4.0K\nLiveCodeBench (Jain et al., 2024)Code problems from coding websites\n(LeetCode, AtCoder, and CodeForces)\n151 3.5K\ns1-teasers Math brain-teasers crawled from the In-\nternet\n23 4.1K\nAll 59K questions Composite of the above datasets with\nreasoning traces and solutions\n59029 3.6K\n18s1: Simple test-time scaling\nC.3. s1K grading prompt\nTogradewhetheranexampleiscorrectforourdatasetselectionin2,weusethepromptinFigure8. WegradeusingClaude\n3.5 except for the correctness among the final 1,000 samples, which we graded with Claude 3.7.\nYou are an AI assistant for grading a science problem. The user will provide you with the question itself, an attempt made by\na student and the correct answer to the problem. Your job is to judge whether the attempt is correct by comparing it with the\ncorrect answer. If the expected solution concludes with a number or choice, there should be no ambiguity. If the expected\nsolution involves going through the entire reasoning process, you should judge the attempt based on whether the reasoning\nprocess is correct with correct answer if helpful.\nThe user will provide the attempt and the correct answer in the following format:\n# Problem\n{problem}\n## Attempt\n{attempt}\n## Correct answer\n{solution}\nExplain your reasoning, and end your response on a new line with only "Yes" or "No" (without quotes).\nFigure 8.Grading prompt.\nC.4. s1K diversity selection\nAlgorithm 1 provides our algorithm for selecting data in our diversity selection stage. As mentioned in 2, we also include\nsamples from some specific benchmarks we perceive as high-quality. None of the samples overlap with our final evaluation.\nC.5. Decontamination\nWe filter all samples by checking for an 8-gram overlap between the selected examples and the evaluation benchmarks:\nMATH500, GPTQA Diamond, and AIME24. We exclude questions with more than an 8-gram overlap.\n19s1: Simple test-time scaling\nAlgorithm 1Two-stage sampling fors1K\n1: Input: \ue23d := Set of 24,496 questions with features\n2: Output: \ue23f := Set of 1,000 selected questions\n3: \ue23f   Initialize the output set (only tracks unique elements)\n4: for\ue23d do\n5: if IsGeminiCorrect()and (IsAIME()or IsGPQA()) then\n6: \ue23f  \ue23f {}\n7: Select all correct AIME/GPQA solutions\n8: else ifIsGeminiCorrect()andIsMATH() andThinkingLength() > 5600then\n9: \ue23f  \ue23f {}\n10: Select correct MATH500 solutions with long chains\n11: end if\n12: end for\n13: \ue230  All available domains\n14: Initialize domain pool\n15: while|\ue23f| <1000do\n16:   RandomChoice(\ue230)\n17: Randomly select a domain\n18:   Questions in domain\n19: Get questions from this domain\n20: ranks  RankByThinkingLength()\n21: Rank by thinking length\n22: weights 2ranks\n23: Apply power-law weighting\n24:  WeightedSample(, weights)\n25: Sample favoring longer chains\n26: \ue23f  \ue23f {}\n27: Add selected question\n28:     {}\n29: if  = then\n30: \ue230  \ue230  {}\n31: Remove exhausted domains\n32: end if\n33: end while\n20s1: Simple test-time scaling\nD. Training details\nWe take a model that has already been pretrained and instruction tuned and further finetune it for reasoning. Specifically, we\nuse Qwen2.5-32B-Instruct (Qwen et al., 2024), which on math tasks generally matches or outperforms the larger Qwen2.5-\n72B-Instru'], 'dimensions': 1536}'
[0m

2025-12-08 14:33:31,801 - DEBUG - Using AiohttpTransport...
2025-12-08 14:33:31,801 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:33:31,801 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:33:31,801 - DEBUG - RAW RESPONSE:
The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


2025-12-08 14:33:31,801 - DEBUG - Logging Details: logger_fn - None | callable(logger_fn) - False
2025-12-08 14:33:31,802 - DEBUG - Logging Details LiteLLM-Failure Call: [<bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x106c13850>>, <bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x109a16d50>>]
2025-12-08 14:33:31,804 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:33:31,804 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:33:31,804 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:33:31,804 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:33:31,806 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:33:31,807 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:33:31,807 - ERROR - Error adding /Users/gabriele/Documents/GitHub/research-agent-cli/library/4603a874ea67c1b582ccff73ff5f82f2/2501.19393v3.pdf: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable LiteLLM Retried: 3 times
2025-12-08 14:33:31,807 - DEBUG - Adding PDF: /Users/gabriele/Documents/GitHub/research-agent-cli/library/04351a7a49b36885fcc89a38088d75c4/327-440-1-sm.pdf
2025-12-08 14:33:31,807 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:33:31,932 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'custom_llm_provider': None}
2025-12-08 14:33:31,932 - DEBUG - model_info: {'key': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': None, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': None, 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:31,932 - DEBUG - added/updated model=8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be in litellm.model_cost: 8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be
2025-12-08 14:33:31,932 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:31,932 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:31,932 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:31,932 - DEBUG - added/updated model=gemini/gemini-2.0-flash-exp in litellm.model_cost: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:31,932 - DEBUG - 
Initialized Model List ['gemini/gemini-2.0-flash-exp']
2025-12-08 14:33:31,932 - INFO - Routing strategy: simple-shuffle
2025-12-08 14:33:31,932 - DEBUG - Inside async function with retries.
2025-12-08 14:33:31,932 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x10a04b790>>, num_retries - 3
2025-12-08 14:33:31,932 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:31,932 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:31,932 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:31,932 - DEBUG - retrieve cooldown models: []
2025-12-08 14:33:31,932 - DEBUG - async cooldown deployments: []
2025-12-08 14:33:31,932 - DEBUG - cooldown_deployments: []
2025-12-08 14:33:31,932 - DEBUG - cooldown deployments: []
2025-12-08 14:33:31,933 - DEBUG - 

2025-12-08 14:33:31,933 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:33:31,933 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\n\xa02 (26) 2024\n87\nURL: https://cyberspace.pgu.ru\n 004.8 + 004.93\nDOI 10.17726/philIT.2024.2.6\n   ,  ?\n . ,\n   \n- , \nalexander.xom@gmail.com\n. \xa0\xa0\xa0\xa0\xa0-\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0-\xa0Google\xa0Gemini\xa0\n\xa0\xa0\xa0,\xa0\xa0\xa0-,\xa0\xa0\xa0\xa0\xa0\n.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0LLM,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0.,\xa0\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0-\n\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\n\xa0\xa0\xa0(inference).\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\n\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0,\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa01950-\xa0\xa0\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0.\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0(\xa0)\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0.\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0.\n :\xa0LLM;\xa0;\xa0;\xa0;\xa0-\n\xa0;\xa0\xa0;\xa0.\nWhy dont transformers think like humans?\nAlexander B. Khomyakov,\nMaster of Physical Sciences \nSaint- Petersburg, Russia\nalexander.xom@gmail.com88\n     \n \nAbstract. Large\xa0language\xa0models\xa0in\xa0the\xa0form\xa0of\xa0chatbots\xa0very\xa0realistical-\nly\xa0imitate\xa0a\xa0dialogue\xa0as\xa0an\xa0omniscient\xa0interlocutor\xa0and\xa0therefore\xa0have\xa0become\xa0\nwidespread.\xa0But\xa0even\xa0Google\xa0in\xa0its\xa0Gemini\xa0chatbot\xa0does\xa0not\xa0recommend\xa0trust-\ning\xa0what\xa0the\xa0chatbot\xa0will\xa0write\xa0and\xa0asks\xa0to\xa0check\xa0its\xa0answers.\xa0In\xa0this\xa0review,\xa0\nvarious\xa0types\xa0of\xa0LLM\xa0errors\xa0such\xa0as\xa0the\xa0curse\xa0of\xa0inversion,\xa0number\xa0processing,\xa0\netc.\xa0will\xa0be\xa0analyzed\xa0to\xa0identify\xa0their\xa0causes.\xa0Such\xa0an\xa0analysis\xa0led\xa0to\xa0the\xa0con-\nclusion\xa0about\xa0the\xa0common\xa0causes\xa0of\xa0all\xa0errors,\xa0which\xa0is\xa0that\xa0transformers\xa0do\xa0\nnot\xa0have\xa0deep\xa0analogy,\xa0hierarchy\xa0of\xa0schemes\xa0and\xa0selectivity\xa0of\xa0content\xa0taken\xa0\ninto\xa0account\xa0in\xa0the\xa0inference.\xa0But\xa0the\xa0most\xa0important\xa0conclusion\xa0is\xa0that\xa0trans-\nformers,\xa0like\xa0other\xa0neural\xa0networks,\xa0are\xa0built\xa0on\xa0the\xa0concept\xa0of\xa0processing\xa0\nthe\xa0input\xa0signal,\xa0which\xa0creates\xa0a\xa0strong\xa0dependence\xa0on\xa0superficial\xa0noise\xa0and\xa0\nirrelevant\xa0information\xa0that\xa0the\xa0transformers\xa0attention\xa0layer\xa0cannot\xa0compensate\xa0\nfor.\xa0The\xa0concept\xa0of\xa0neural\xa0networks\xa0was\xa0laid\xa0down\xa0in\xa0the\xa01950s\xa0by\xa0the\xa0idea\xa0of\xa0\nF.\xa0Rosenblatts\xa0perceptron\xa0and\xa0did\xa0not\xa0take\xa0into\xa0account\xa0the\xa0achievements\xa0of\xa0\ncognitive\xa0psychology\xa0that\xa0appeared\xa0later.\xa0According\xa0to\xa0the\xa0constructivist\xa0par-\nadigm,\xa0the\xa0input\xa0word\xa0(or\xa0perception)\xa0is\xa0only\xa0a\xa0way\xa0to\xa0check\xa0the\xa0correctness\xa0\nof\xa0the\xa0constructed\xa0predictive\xa0model\xa0for\xa0possible\xa0situations.\xa0This\xa0is\xa0the\xa0cause\xa0of\xa0\nthe\xa0biggest\xa0problem\xa0of\xa0transformers,\xa0called\xa0hallucinations.\xa0And\xa0its\xa0elimination\xa0\nis\xa0possible\xa0only\xa0by\xa0changing\xa0the\xa0architecture\xa0of\xa0the\xa0neural\xa0network,\xa0but\xa0not\xa0by\xa0\nincreasing\xa0the\xa0amount\xa0of\xa0data\xa0in\xa0training.\nKeywords:\xa0LLM;\xa0transformers;\xa0thinking;\xa0analogy;\xa0cognitive\xa0psycholo-\ngy;\xa0perceptual\xa0cycle.\n\n\xa0 \xa0 \xa0 \xa0 -\n\xa0.\xa0 \xa0\xa0 \xa0\n\xa0\xa0\xa0\xa0\xa0\xa0-\n.\xa0\xa0,\xa0\xa0\xa0,\xa0\xa0\n\xa0(\xa0\xa0\xa0),\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0.\xa0\xa0\n,\xa0,\xa0\xa0\xa0 \xa0\xa0\xa0\n,\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\n\xa0\xa0,\xa0\xa0\xa0\xa0\xa0-\n\xa0\xa0,\xa0\xa0,\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0.\xa0\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n.\xa02 (26) 2024\n89\nURL: https://cyberspace.pgu.ru\n\xa0,\xa0\xa0\xa0\xa0 \xa02024\xa0 .\xa0\xa0\nNeurlPS\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0(\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n).\xa0\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0,\xa0\n\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\n\xa0,\xa0\xa0\xa0[1].\xa0\xa0\xa0\xa0\xa0,\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0,\xa0\n\nCitation:"}], caching=False, client=None, n=1, stream=False, litellm_trace_id='080a8fd5-6904-4c9d-a9cb-eefb2c9c504d', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:33:31,933 - DEBUG - 

2025-12-08 14:33:31,933 - DEBUG - self.optional_params: {}
2025-12-08 14:33:31,933 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:33:31,933 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:33:31,933 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:31,933 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:33:31,933 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\n\xa02 (26) 2024\n87\nURL: https://cyberspace.pgu.ru\n 004.8 + 004.93\nDOI 10.17726/philIT.2024.2.6\n   ,  ?\n . ,\n   \n- , \nalexander.xom@gmail.com\n. \xa0\xa0\xa0\xa0\xa0-\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0-\xa0Google\xa0Gemini\xa0\n\xa0\xa0\xa0,\xa0\xa0\xa0-,\xa0\xa0\xa0\xa0\xa0\n.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0LLM,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0.,\xa0\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0-\n\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\n\xa0\xa0\xa0(inference).\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\n\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0,\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa01950-\xa0\xa0\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0.\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0(\xa0)\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0.\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0.\n :\xa0LLM;\xa0;\xa0;\xa0;\xa0-\n\xa0;\xa0\xa0;\xa0.\nWhy dont transformers think like humans?\nAlexander B. Khomyakov,\nMaster of Physical Sciences \nSaint- Petersburg, Russia\nalexander.xom@gmail.com88\n     \n \nAbstract. Large\xa0language\xa0models\xa0in\xa0the\xa0form\xa0of\xa0chatbots\xa0very\xa0realistical-\nly\xa0imitate\xa0a\xa0dialogue\xa0as\xa0an\xa0omniscient\xa0interlocutor\xa0and\xa0therefore\xa0have\xa0become\xa0\nwidespread.\xa0But\xa0even\xa0Google\xa0in\xa0its\xa0Gemini\xa0chatbot\xa0does\xa0not\xa0recommend\xa0trust-\ning\xa0what\xa0the\xa0chatbot\xa0will\xa0write\xa0and\xa0asks\xa0to\xa0check\xa0its\xa0answers.\xa0In\xa0this\xa0review,\xa0\nvarious\xa0types\xa0of\xa0LLM\xa0errors\xa0such\xa0as\xa0the\xa0curse\xa0of\xa0inversion,\xa0number\xa0processing,\xa0\netc.\xa0will\xa0be\xa0analyzed\xa0to\xa0identify\xa0their\xa0causes.\xa0Such\xa0an\xa0analysis\xa0led\xa0to\xa0the\xa0con-\nclusion\xa0about\xa0the\xa0common\xa0causes\xa0of\xa0all\xa0errors,\xa0which\xa0is\xa0that\xa0transformers\xa0do\xa0\nnot\xa0have\xa0deep\xa0analogy,\xa0hierarchy\xa0of\xa0schemes\xa0and\xa0selectivity\xa0of\xa0content\xa0taken\xa0\ninto\xa0account\xa0in\xa0the\xa0inference.\xa0But\xa0the\xa0most\xa0important\xa0conclusion\xa0is\xa0that\xa0trans-\nformers,\xa0like\xa0other\xa0neural\xa0networks,\xa0are\xa0built\xa0on\xa0the\xa0concept\xa0of\xa0processing\xa0\nthe\xa0input\xa0signal,\xa0which\xa0creates\xa0a\xa0strong\xa0dependence\xa0on\xa0superficial\xa0noise\xa0and\xa0\nirrelevant\xa0information\xa0that\xa0the\xa0transformers\xa0attention\xa0layer\xa0cannot\xa0compensate\xa0\nfor.\xa0The\xa0concept\xa0of\xa0neural\xa0networks\xa0was\xa0laid\xa0down\xa0in\xa0the\xa01950s\xa0by\xa0the\xa0idea\xa0of\xa0\nF.\xa0Rosenblatts\xa0perceptron\xa0and\xa0did\xa0not\xa0take\xa0into\xa0account\xa0the\xa0achievements\xa0of\xa0\ncognitive\xa0psychology\xa0that\xa0appeared\xa0later.\xa0According\xa0to\xa0the\xa0constructivist\xa0par-\nadigm,\xa0the\xa0input\xa0word\xa0(or\xa0perception)\xa0is\xa0only\xa0a\xa0way\xa0to\xa0check\xa0the\xa0correctness\xa0\nof\xa0the\xa0constructed\xa0predictive\xa0model\xa0for\xa0possible\xa0situations.\xa0This\xa0is\xa0the\xa0cause\xa0of\xa0\nthe\xa0biggest\xa0problem\xa0of\xa0transformers,\xa0called\xa0hallucinations.\xa0And\xa0its\xa0elimination\xa0\nis\xa0possible\xa0only\xa0by\xa0changing\xa0the\xa0architecture\xa0of\xa0the\xa0neural\xa0network,\xa0but\xa0not\xa0by\xa0\nincreasing\xa0the\xa0amount\xa0of\xa0data\xa0in\xa0training.\nKeywords:\xa0LLM;\xa0transformers;\xa0thinking;\xa0analogy;\xa0cognitive\xa0psycholo-\ngy;\xa0perceptual\xa0cycle.\n\n\xa0 \xa0 \xa0 \xa0 -\n\xa0.\xa0 \xa0\xa0 \xa0\n\xa0\xa0\xa0\xa0\xa0\xa0-\n.\xa0\xa0,\xa0\xa0\xa0,\xa0\xa0\n\xa0(\xa0\xa0\xa0),\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0.\xa0\xa0\n,\xa0,\xa0\xa0\xa0 \xa0\xa0\xa0\n,\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\n\xa0\xa0,\xa0\xa0\xa0\xa0\xa0-\n\xa0\xa0,\xa0\xa0,\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0.\xa0\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n.\xa02 (26) 2024\n89\nURL: https://cyberspace.pgu.ru\n\xa0,\xa0\xa0\xa0\xa0 \xa02024\xa0 .\xa0\xa0\nNeurlPS\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0(\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n).\xa0\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0,\xa0\n\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\n\xa0,\xa0\xa0\xa0[1].\xa0\xa0\xa0\xa0\xa0,\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0,\xa0\n\nCitation:"}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:33:31,934 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:31,934 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:33:31,934 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:31,934 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:31,934 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:31,934 - DEBUG - Using AiohttpTransport...
2025-12-08 14:33:31,934 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:33:31,934 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:33:31,934 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\n\xa02 (26) 2024\n87\nURL: https://cyberspace.pgu.ru\n 004.8 + 004.93\nDOI 10.17726/philIT.2024.2.6\n   ,  ?\n . ,\n   \n- , \nalexander.xom@gmail.com\n. \xa0\xa0\xa0\xa0\xa0-\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0-\xa0Google\xa0Gemini\xa0\n\xa0\xa0\xa0,\xa0\xa0\xa0-,\xa0\xa0\xa0\xa0\xa0\n.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0LLM,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0.,\xa0\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0-\n\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\n\xa0\xa0\xa0(inference).\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\n\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0,\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa01950-\xa0\xa0\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0.\xa0\xa0\xa0\xa0\n,\xa0\xa0\xa0(\xa0)\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n,\xa0\xa0.\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0.\n :\xa0LLM;\xa0;\xa0;\xa0;\xa0-\n\xa0;\xa0\xa0;\xa0.\nWhy dont transformers think like humans?\nAlexander B. Khomyakov,\nMaster of Physical Sciences \nSaint- Petersburg, Russia\nalexander.xom@gmail.com88\n     \n \nAbstract. Large\xa0language\xa0models\xa0in\xa0the\xa0form\xa0of\xa0chatbots\xa0very\xa0realistical-\nly\xa0imitate\xa0a\xa0dialogue\xa0as\xa0an\xa0omniscient\xa0interlocutor\xa0and\xa0therefore\xa0have\xa0become\xa0\nwidespread.\xa0But\xa0even\xa0Google\xa0in\xa0its\xa0Gemini\xa0chatbot\xa0does\xa0not\xa0recommend\xa0trust-\ning\xa0what\xa0the\xa0chatbot\xa0will\xa0write\xa0and\xa0asks\xa0to\xa0check\xa0its\xa0answers.\xa0In\xa0this\xa0review,\xa0\nvarious\xa0types\xa0of\xa0LLM\xa0errors\xa0such\xa0as\xa0the\xa0curse\xa0of\xa0inversion,\xa0number\xa0processing,\xa0\netc.\xa0will\xa0be\xa0analyzed\xa0to\xa0identify\xa0their\xa0causes.\xa0Such\xa0an\xa0analysis\xa0led\xa0to\xa0the\xa0con-\nclusion\xa0about\xa0the\xa0common\xa0causes\xa0of\xa0all\xa0errors,\xa0which\xa0is\xa0that\xa0transformers\xa0do\xa0\nnot\xa0have\xa0deep\xa0analogy,\xa0hierarchy\xa0of\xa0schemes\xa0and\xa0selectivity\xa0of\xa0content\xa0taken\xa0\ninto\xa0account\xa0in\xa0the\xa0inference.\xa0But\xa0the\xa0most\xa0important\xa0conclusion\xa0is\xa0that\xa0trans-\nformers,\xa0like\xa0other\xa0neural\xa0networks,\xa0are\xa0built\xa0on\xa0the\xa0concept\xa0of\xa0processing\xa0\nthe\xa0input\xa0signal,\xa0which\xa0creates\xa0a\xa0strong\xa0dependence\xa0on\xa0superficial\xa0noise\xa0and\xa0\nirrelevant\xa0information\xa0that\xa0the\xa0transformers\xa0attention\xa0layer\xa0cannot\xa0compensate\xa0\nfor.\xa0The\xa0concept\xa0of\xa0neural\xa0networks\xa0was\xa0laid\xa0down\xa0in\xa0the\xa01950s\xa0by\xa0the\xa0idea\xa0of\xa0\nF.\xa0Rosenblatts\xa0perceptron\xa0and\xa0did\xa0not\xa0take\xa0into\xa0account\xa0the\xa0achievements\xa0of\xa0\ncognitive\xa0psychology\xa0that\xa0appeared\xa0later.\xa0According\xa0to\xa0the\xa0constructivist\xa0par-\nadigm,\xa0the\xa0input\xa0word\xa0(or\xa0perception)\xa0is\xa0only\xa0a\xa0way\xa0to\xa0check\xa0the\xa0correctness\xa0\nof\xa0the\xa0constructed\xa0predictive\xa0model\xa0for\xa0possible\xa0situations.\xa0This\xa0is\xa0the\xa0cause\xa0of\xa0\nthe\xa0biggest\xa0problem\xa0of\xa0transformers,\xa0called\xa0hallucinations.\xa0And\xa0its\xa0elimination\xa0\nis\xa0possible\xa0only\xa0by\xa0changing\xa0the\xa0architecture\xa0of\xa0the\xa0neural\xa0network,\xa0but\xa0not\xa0by\xa0\nincreasing\xa0the\xa0amount\xa0of\xa0data\xa0in\xa0training.\nKeywords:\xa0LLM;\xa0transformers;\xa0thinking;\xa0analogy;\xa0cognitive\xa0psycholo-\ngy;\xa0perceptual\xa0cycle.\n\n\xa0 \xa0 \xa0 \xa0 -\n\xa0.\xa0 \xa0\xa0 \xa0\n\xa0\xa0\xa0\xa0\xa0\xa0-\n.\xa0\xa0,\xa0\xa0\xa0,\xa0\xa0\n\xa0(\xa0\xa0\xa0),\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0.\xa0\xa0\n,\xa0,\xa0\xa0\xa0 \xa0\xa0\xa0\n,\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\n\xa0\xa0,\xa0\xa0\xa0\xa0\xa0-\n\xa0\xa0,\xa0\xa0,\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0.\xa0\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0-\n.\xa02 (26) 2024\n89\nURL: https://cyberspace.pgu.ru\n\xa0,\xa0\xa0\xa0\xa0 \xa02024\xa0 .\xa0\xa0\nNeurlPS\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0\xa0,\xa0-\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0(\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n).\xa0\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\xa0\xa0,\xa0\n\xa0,\xa0\xa0\xa0\xa0,\xa0\xa0\xa0\n\xa0,\xa0\xa0\xa0[1].\xa0\xa0\xa0\xa0\xa0,\xa0\n\xa0\xa0.\xa0\xa0\xa0\xa0,\xa0\n\nCitation:"}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:33:33,963 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Khomyakov, Alexander B. Why Dont Transformers Think Like Humans? *     *, no. 2 (26), 2024, pp. 87-89. DOI 10.17726/philIT.2024.2.6. Accessed 14 May 2025.\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.056632812206561751
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 2771,
    "candidatesTokenCount": 91,
    "totalTokenCount": 2862,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 2771
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 91
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "rNM2aaH8EtOfvdIPjP7X4Qs"
}



2025-12-08 14:33:33,964 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:33,965 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:33,965 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:33,965 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:33,965 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:33,965 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:33,966 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:33,966 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:33,966 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:33,966 - DEBUG - response_cost: 0.0
2025-12-08 14:33:33,966 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:33,966 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:33:33,967 - DEBUG - response_cost: 0.0
2025-12-08 14:33:33,967 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:33,967 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:33,968 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:33,968 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:33,968 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:33,968 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:33,968 - DEBUG - Async Response: ModelResponse(id='rNM2aaH8EtOfvdIPjP7X4Qs', created=1765200811, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Khomyakov, Alexander B. Why Dont Transformers Think Like Humans? *     *, no. 2 (26), 2024, pp. 87-89. DOI 10.17726/philIT.2024.2.6. Accessed 14 May 2025.\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=91, prompt_tokens=2771, total_tokens=2862, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=91, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=2771, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:33:33,969 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:33:33,969 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:33:33,969 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:33,969 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:33,970 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:33,970 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:33,970 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:33,970 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:33,971 - DEBUG - Inside async function with retries.
2025-12-08 14:33:33,971 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x10a04b790>>, num_retries - 3
2025-12-08 14:33:33,971 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:33,971 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:33,971 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:33,971 - DEBUG - retrieve cooldown models: []
2025-12-08 14:33:33,971 - DEBUG - async cooldown deployments: []
2025-12-08 14:33:33,971 - DEBUG - cooldown_deployments: []
2025-12-08 14:33:33,971 - DEBUG - cooldown deployments: []
2025-12-08 14:33:33,971 - DEBUG - 

2025-12-08 14:33:33,971 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:33:33,971 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': "Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author's value should be a list of authors. Khomyakov, Alexander B. Why Dont Transformers Think Like Humans? *     *, no. 2 (26), 2024, pp. 87-89. DOI 10.17726/philIT.2024.2.6. Accessed 14 May 2025.\n\n\nCitation JSON:"}], caching=False, client=None, n=1, stream=False, litellm_trace_id='fe26687f-b1b2-44ef-b6db-71065f25198c', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:33:33,972 - DEBUG - 

2025-12-08 14:33:33,972 - DEBUG - self.optional_params: {}
2025-12-08 14:33:33,972 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:33:33,972 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:33:33,973 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:33,973 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x109abb6d0>>
2025-12-08 14:33:33,973 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:33:33,973 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:33,973 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': "Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author's value should be a list of authors. Khomyakov, Alexander B. Why Dont Transformers Think Like Humans? *     *, no. 2 (26), 2024, pp. 87-89. DOI 10.17726/philIT.2024.2.6. Accessed 14 May 2025.\n\n\nCitation JSON:"}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:33:33,973 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:33,973 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:33,973 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:33:33,974 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:33,974 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:33,974 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:33,975 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': "Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author's value should be a list of authors. Khomyakov, Alexander B. Why Dont Transformers Think Like Humans? *     *, no. 2 (26), 2024, pp. 87-89. DOI 10.17726/philIT.2024.2.6. Accessed 14 May 2025.\n\n\nCitation JSON:"}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:33:35,092 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"title\": \"Why Dont Transformers Think Like Humans?\",\n  \"authors\": [\n    \"Alexander B. Khomyakov\"\n  ],\n  \"doi\": \"10.17726/philIT.2024.2.6\"\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.00023690117118151292
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 144,
    "candidatesTokenCount": 69,
    "totalTokenCount": 213,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 144
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 69
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "rtM2adGKBumyxN8PwM_XmQ8"
}



2025-12-08 14:33:35,094 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:35,094 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:35,094 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:35,095 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:35,095 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:35,095 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:35,095 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:35,095 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:35,095 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:35,096 - DEBUG - response_cost: 0.0
2025-12-08 14:33:35,096 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:35,096 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:33:35,096 - DEBUG - response_cost: 0.0
2025-12-08 14:33:35,096 - DEBUG - Async Response: ModelResponse(id='rtM2adGKBumyxN8PwM_XmQ8', created=1765200813, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='```json\n{\n  "title": "Why Dont Transformers Think Like Humans?",\n  "authors": [\n    "Alexander B. Khomyakov"\n  ],\n  "doi": "10.17726/philIT.2024.2.6"\n}\n```', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=69, prompt_tokens=144, total_tokens=213, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=69, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=144, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:33:35,097 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:35,097 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:33:35,097 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:35,097 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:35,097 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:35,097 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:35,098 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:33:35,098 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:35,098 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:35,099 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:35,099 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:35,129 - DEBUG - Attempting to populate metadata query: {'doi': '10.17726/philIT.2024.2.6', 'authors': ['Alexander B. Khomyakov'], 'title': 'Why Dont Transformers Think Like Humans?', 'session': <aiohttp.client.ClientSession object at 0x1089eafd0>} via providers=[<paperqa.clients.semantic_scholar.SemanticScholarProvider object at 0x1095f3d50>, <paperqa.clients.crossref.CrossrefProvider object at 0x1095f1ed0>] processors=[<paperqa.clients.journal_quality.JournalQualityPostProcessor object at 0x1095f0350>]
2025-12-08 14:33:35,129 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x109aba550>>
2025-12-08 14:33:35,129 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:35,129 - WARNING - SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.
2025-12-08 14:33:35,129 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:35,651 - WARNING - Metadata not found for 10.17726/philIT.2024.2.6 in CrossrefProvider.
2025-12-08 14:33:35,968 - ERROR - Error adding /Users/gabriele/Documents/GitHub/research-agent-cli/library/04351a7a49b36885fcc89a38088d75c4/327-440-1-sm.pdf: This does not look like a text document: /Users/gabriele/Documents/GitHub/research-agent-cli/library/04351a7a49b36885fcc89a38088d75c4/327-440-1-sm.pdf. Pass disable_check to ignore this error.
2025-12-08 14:33:35,968 - DEBUG - Adding PDF: /Users/gabriele/Documents/GitHub/research-agent-cli/library/38a0b433fee8c9dea59792d623fe7285/var-folders-yk-4-ysm4397c313kwrc2q92l0w0000gn-t-tmpax15kxg7.pdf
2025-12-08 14:33:35,969 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:33:36,037 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'combined_stripped_model_name': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'custom_llm_provider': None}
2025-12-08 14:33:36,037 - DEBUG - model_info: {'key': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': None, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': None, 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:36,037 - DEBUG - added/updated model=8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be in litellm.model_cost: 8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be
2025-12-08 14:33:36,037 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:36,037 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:36,037 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:36,037 - DEBUG - added/updated model=gemini/gemini-2.0-flash-exp in litellm.model_cost: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:36,037 - DEBUG - 
Initialized Model List ['gemini/gemini-2.0-flash-exp']
2025-12-08 14:33:36,037 - INFO - Routing strategy: simple-shuffle
2025-12-08 14:33:36,038 - DEBUG - Inside async function with retries.
2025-12-08 14:33:36,038 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x109ebe250>>, num_retries - 3
2025-12-08 14:33:36,038 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:36,038 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:36,038 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:36,038 - DEBUG - retrieve cooldown models: []
2025-12-08 14:33:36,038 - DEBUG - async cooldown deployments: []
2025-12-08 14:33:36,038 - DEBUG - cooldown_deployments: []
2025-12-08 14:33:36,038 - DEBUG - cooldown deployments: []
2025-12-08 14:33:36,038 - DEBUG - 

2025-12-08 14:33:36,038 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:33:36,038 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\nATTENTION IS ALL YOU NEED IN SPEECH SEPARATION\nCem Subakan1, Mirco Ravanelli1, Samuele Cornell2, Mirko Bronzi1, Jianyuan Zhong3\n1Mila-Quebec AI Institute, Canada,\n2Universit`a Politecnica delle Marche, Italy\n3University of Rochester, USA\nABSTRACT\nRecurrent Neural Networks (RNNs) have long been the dominant\narchitecture in sequence-to-sequence learning. RNNs, however, are\ninherently sequential models that do not allow parallelization of their\ncomputations. Transformers are emerging as a natural alternative to\nstandard RNNs, replacing recurrent computations with a multi-head\nattention mechanism.\nIn this paper, we propose the SepFormer, a novel RNN-free\nTransformer-based neural network for speech separation. The Sep-\nFormer learns short and long-term dependencies with a multi-scale\napproach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix\ndatasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an\nSI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the\nparallelization advantages of Transformers and achieves a compet-\nitive performance even when downsampling the encoded represen-\ntation by a factor of 8. It is thus signicantly faster and it is less\nmemory-demanding than the latest speech separation systems with\ncomparable performance.\nIndex Terms speech separation, source separation, trans-\nformer, attention, deep learning.\n1. INTRODUCTION\nRNNs are a crucial component of modern audio processing sys-\ntems and they are used in many different domains, including speech\nrecognition, synthesis, enhancement, and separation, just to name a\nfew. Especially when coupled with multiplicative gate mechanisms\n(like LSTM [1] and GRU [2, 3]), their recurrent connections are es-\nsential to learn long-term dependencies and properly manage speech\ncontexts. Nevertheless, the inherently sequential nature of RNNs\nimpairs an effective parallelization of the computations. This bot-\ntleneck is particularly evident when processing large datasets with\nlong sequences. On the other hand, Transformers [4] completely\navoid this bottleneck by eliminating recurrence and replacing it with\na fully attention-based mechanism. By attending to the whole se-\nquence at once, a direct connection can be established between dis-\ntant elements allowing Transformers to learn long-term dependen-\ncies more easily [5]. For this reason, Transformers are gaining con-\nsiderable popularity for speech processing and recently showed com-\npetitive performance in speech recognition [6], synthesis [7], en-\nhancement [8], diarization [9], as well as speaker recognition [10].\nLittle research has been done so far on Transformer-based mod-\nels for monaural audio source separation. The eld has been revo-\nlutionized by the adoption of deep learning techniques [1116], and\nwith recent works [1723] achieving impressive results by adopt-\ning an end-to-end approach. Most of the current speech separation\ntechniques [14, 15, 1722] require effective modeling of long input\nx Encoder h Masking Net Decoder\ns1\ns2\nm1\nm2\nFig. 1. The high-level description of our system: The encoder block\nestimates a learned-representation for the input signal, while the\nmasking network estimates optimal masks to separate the sources\npresent in the mixtures. The decoder nally reconstructs the esti-\nmated sources in the time domain using the masks provided by the\nmasking network.\nsequences to perform well. Current systems rely, in large part, on the\nlearned-domain masking strategy popularized by Conv-TasNet [15].\nIn this framework, an overcomplete set of analysis and synthesis l-\nters is learned directly from the data, and separation is performed\nby estimating a mask for each source in this learned-domain. Build-\ning on this, Dual-Path RNN (DPRNN) [17] has demonstrated that\nbetter long-term modeling is crucial to improve the separation per-\nformance. This is achieved by splitting the input sequence into mul-\ntiple chunks that are processed locally and globally with different\nRNNs. Nevertheless, due to the use of RNNs, DPRNN still suffers\nfrom the aforementioned limitations of recurrent connections, espe-\ncially regarding the global processing step. An attempt to integrate\ntransformers into the speech separation pipeline has been recently\ndone in [22] where the proposed Dual-Path Transformer Network\n(DPTNet) is shown to outperform the standard DPRNN. Such an ar-\nchitecture, however, still embeds an RNN, effectively negating the\nparallelization capability of pure-attention models.\nIn this paper, we propose a novel model called SepFormer (Sep-\naration Transformer), which is mainly composed of multi-head at-\ntention and feed-forward layers. We adopt the dual-path frame-\nwork introduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the\n\nCitation:"}], caching=False, client=None, n=1, stream=False, litellm_trace_id='8000d780-1aa9-41cd-af2b-91a6f30bfab2', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:33:36,038 - DEBUG - 

2025-12-08 14:33:36,038 - DEBUG - self.optional_params: {}
2025-12-08 14:33:36,038 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:33:36,038 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:33:36,038 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:36,038 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:33:36,039 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\nATTENTION IS ALL YOU NEED IN SPEECH SEPARATION\nCem Subakan1, Mirco Ravanelli1, Samuele Cornell2, Mirko Bronzi1, Jianyuan Zhong3\n1Mila-Quebec AI Institute, Canada,\n2Universit`a Politecnica delle Marche, Italy\n3University of Rochester, USA\nABSTRACT\nRecurrent Neural Networks (RNNs) have long been the dominant\narchitecture in sequence-to-sequence learning. RNNs, however, are\ninherently sequential models that do not allow parallelization of their\ncomputations. Transformers are emerging as a natural alternative to\nstandard RNNs, replacing recurrent computations with a multi-head\nattention mechanism.\nIn this paper, we propose the SepFormer, a novel RNN-free\nTransformer-based neural network for speech separation. The Sep-\nFormer learns short and long-term dependencies with a multi-scale\napproach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix\ndatasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an\nSI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the\nparallelization advantages of Transformers and achieves a compet-\nitive performance even when downsampling the encoded represen-\ntation by a factor of 8. It is thus signicantly faster and it is less\nmemory-demanding than the latest speech separation systems with\ncomparable performance.\nIndex Terms speech separation, source separation, trans-\nformer, attention, deep learning.\n1. INTRODUCTION\nRNNs are a crucial component of modern audio processing sys-\ntems and they are used in many different domains, including speech\nrecognition, synthesis, enhancement, and separation, just to name a\nfew. Especially when coupled with multiplicative gate mechanisms\n(like LSTM [1] and GRU [2, 3]), their recurrent connections are es-\nsential to learn long-term dependencies and properly manage speech\ncontexts. Nevertheless, the inherently sequential nature of RNNs\nimpairs an effective parallelization of the computations. This bot-\ntleneck is particularly evident when processing large datasets with\nlong sequences. On the other hand, Transformers [4] completely\navoid this bottleneck by eliminating recurrence and replacing it with\na fully attention-based mechanism. By attending to the whole se-\nquence at once, a direct connection can be established between dis-\ntant elements allowing Transformers to learn long-term dependen-\ncies more easily [5]. For this reason, Transformers are gaining con-\nsiderable popularity for speech processing and recently showed com-\npetitive performance in speech recognition [6], synthesis [7], en-\nhancement [8], diarization [9], as well as speaker recognition [10].\nLittle research has been done so far on Transformer-based mod-\nels for monaural audio source separation. The eld has been revo-\nlutionized by the adoption of deep learning techniques [1116], and\nwith recent works [1723] achieving impressive results by adopt-\ning an end-to-end approach. Most of the current speech separation\ntechniques [14, 15, 1722] require effective modeling of long input\nx Encoder h Masking Net Decoder\ns1\ns2\nm1\nm2\nFig. 1. The high-level description of our system: The encoder block\nestimates a learned-representation for the input signal, while the\nmasking network estimates optimal masks to separate the sources\npresent in the mixtures. The decoder nally reconstructs the esti-\nmated sources in the time domain using the masks provided by the\nmasking network.\nsequences to perform well. Current systems rely, in large part, on the\nlearned-domain masking strategy popularized by Conv-TasNet [15].\nIn this framework, an overcomplete set of analysis and synthesis l-\nters is learned directly from the data, and separation is performed\nby estimating a mask for each source in this learned-domain. Build-\ning on this, Dual-Path RNN (DPRNN) [17] has demonstrated that\nbetter long-term modeling is crucial to improve the separation per-\nformance. This is achieved by splitting the input sequence into mul-\ntiple chunks that are processed locally and globally with different\nRNNs. Nevertheless, due to the use of RNNs, DPRNN still suffers\nfrom the aforementioned limitations of recurrent connections, espe-\ncially regarding the global processing step. An attempt to integrate\ntransformers into the speech separation pipeline has been recently\ndone in [22] where the proposed Dual-Path Transformer Network\n(DPTNet) is shown to outperform the standard DPRNN. Such an ar-\nchitecture, however, still embeds an RNN, effectively negating the\nparallelization capability of pure-attention models.\nIn this paper, we propose a novel model called SepFormer (Sep-\naration Transformer), which is mainly composed of multi-head at-\ntention and feed-forward layers. We adopt the dual-path frame-\nwork introduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the\n\nCitation:"}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:33:36,039 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:36,039 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:33:36,039 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:36,039 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:36,039 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:36,039 - DEBUG - Using AiohttpTransport...
2025-12-08 14:33:36,039 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:33:36,039 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:33:36,039 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': "Provide the citation for the following text in MLA Format. Do not write an introductory sentence. Do not fabricate a DOI such as '10.xxxx' if one cannot be found, just leave it out of the citation. If reporting date accessed, the current year is 2025\n\nATTENTION IS ALL YOU NEED IN SPEECH SEPARATION\nCem Subakan1, Mirco Ravanelli1, Samuele Cornell2, Mirko Bronzi1, Jianyuan Zhong3\n1Mila-Quebec AI Institute, Canada,\n2Universit`a Politecnica delle Marche, Italy\n3University of Rochester, USA\nABSTRACT\nRecurrent Neural Networks (RNNs) have long been the dominant\narchitecture in sequence-to-sequence learning. RNNs, however, are\ninherently sequential models that do not allow parallelization of their\ncomputations. Transformers are emerging as a natural alternative to\nstandard RNNs, replacing recurrent computations with a multi-head\nattention mechanism.\nIn this paper, we propose the SepFormer, a novel RNN-free\nTransformer-based neural network for speech separation. The Sep-\nFormer learns short and long-term dependencies with a multi-scale\napproach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix\ndatasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an\nSI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the\nparallelization advantages of Transformers and achieves a compet-\nitive performance even when downsampling the encoded represen-\ntation by a factor of 8. It is thus signicantly faster and it is less\nmemory-demanding than the latest speech separation systems with\ncomparable performance.\nIndex Terms speech separation, source separation, trans-\nformer, attention, deep learning.\n1. INTRODUCTION\nRNNs are a crucial component of modern audio processing sys-\ntems and they are used in many different domains, including speech\nrecognition, synthesis, enhancement, and separation, just to name a\nfew. Especially when coupled with multiplicative gate mechanisms\n(like LSTM [1] and GRU [2, 3]), their recurrent connections are es-\nsential to learn long-term dependencies and properly manage speech\ncontexts. Nevertheless, the inherently sequential nature of RNNs\nimpairs an effective parallelization of the computations. This bot-\ntleneck is particularly evident when processing large datasets with\nlong sequences. On the other hand, Transformers [4] completely\navoid this bottleneck by eliminating recurrence and replacing it with\na fully attention-based mechanism. By attending to the whole se-\nquence at once, a direct connection can be established between dis-\ntant elements allowing Transformers to learn long-term dependen-\ncies more easily [5]. For this reason, Transformers are gaining con-\nsiderable popularity for speech processing and recently showed com-\npetitive performance in speech recognition [6], synthesis [7], en-\nhancement [8], diarization [9], as well as speaker recognition [10].\nLittle research has been done so far on Transformer-based mod-\nels for monaural audio source separation. The eld has been revo-\nlutionized by the adoption of deep learning techniques [1116], and\nwith recent works [1723] achieving impressive results by adopt-\ning an end-to-end approach. Most of the current speech separation\ntechniques [14, 15, 1722] require effective modeling of long input\nx Encoder h Masking Net Decoder\ns1\ns2\nm1\nm2\nFig. 1. The high-level description of our system: The encoder block\nestimates a learned-representation for the input signal, while the\nmasking network estimates optimal masks to separate the sources\npresent in the mixtures. The decoder nally reconstructs the esti-\nmated sources in the time domain using the masks provided by the\nmasking network.\nsequences to perform well. Current systems rely, in large part, on the\nlearned-domain masking strategy popularized by Conv-TasNet [15].\nIn this framework, an overcomplete set of analysis and synthesis l-\nters is learned directly from the data, and separation is performed\nby estimating a mask for each source in this learned-domain. Build-\ning on this, Dual-Path RNN (DPRNN) [17] has demonstrated that\nbetter long-term modeling is crucial to improve the separation per-\nformance. This is achieved by splitting the input sequence into mul-\ntiple chunks that are processed locally and globally with different\nRNNs. Nevertheless, due to the use of RNNs, DPRNN still suffers\nfrom the aforementioned limitations of recurrent connections, espe-\ncially regarding the global processing step. An attempt to integrate\ntransformers into the speech separation pipeline has been recently\ndone in [22] where the proposed Dual-Path Transformer Network\n(DPTNet) is shown to outperform the standard DPRNN. Such an ar-\nchitecture, however, still embeds an RNN, effectively negating the\nparallelization capability of pure-attention models.\nIn this paper, we propose a novel model called SepFormer (Sep-\naration Transformer), which is mainly composed of multi-head at-\ntention and feed-forward layers. We adopt the dual-path frame-\nwork introduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the\n\nCitation:"}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:33:37,177 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Subakan, Cem, et al. \"Attention Is All You Need in Speech Separation.\" *Mila-Quebec AI Institute*, 2020.\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.098100514972911168
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 1235,
    "candidatesTokenCount": 34,
    "totalTokenCount": 1269,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 1235
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 34
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "sNM2afH2GIXuxN8P-cDL4A4"
}



2025-12-08 14:33:37,178 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:37,179 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:37,179 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:37,179 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:37,179 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:37,179 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:37,179 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:37,180 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:37,180 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:37,180 - DEBUG - response_cost: 0.0
2025-12-08 14:33:37,180 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:37,180 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:33:37,180 - DEBUG - response_cost: 0.0
2025-12-08 14:33:37,181 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:37,181 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:37,181 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:37,181 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:37,181 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:37,182 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:37,182 - DEBUG - Async Response: ModelResponse(id='sNM2afH2GIXuxN8P-cDL4A4', created=1765200816, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Subakan, Cem, et al. "Attention Is All You Need in Speech Separation." *Mila-Quebec AI Institute*, 2020.\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=34, prompt_tokens=1235, total_tokens=1269, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=34, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=1235, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:33:37,182 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:33:37,183 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:33:37,183 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:37,183 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:37,183 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:37,183 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:37,183 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:37,184 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:37,184 - DEBUG - Inside async function with retries.
2025-12-08 14:33:37,184 - DEBUG - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x109ebe250>>, num_retries - 3
2025-12-08 14:33:37,184 - DEBUG - initial list of deployments: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:37,185 - DEBUG - healthy_deployments after team filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:37,185 - DEBUG - healthy_deployments after web search filter: [{'model_name': 'gemini/gemini-2.0-flash-exp', 'litellm_params': {'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'gemini/gemini-2.0-flash-exp', 'temperature': 0.0}, 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}}]
2025-12-08 14:33:37,185 - DEBUG - retrieve cooldown models: []
2025-12-08 14:33:37,185 - DEBUG - async cooldown deployments: []
2025-12-08 14:33:37,185 - DEBUG - cooldown_deployments: []
2025-12-08 14:33:37,185 - DEBUG - cooldown deployments: []
2025-12-08 14:33:37,185 - DEBUG - 

2025-12-08 14:33:37,185 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:33:37,185 - DEBUG - [92mlitellm.acompletion(use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='gemini/gemini-2.0-flash-exp', temperature=0.0, messages=[{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Subakan, Cem, et al. "Attention Is All You Need in Speech Separation." *Mila-Quebec AI Institute*, 2020.\n\n\nCitation JSON:'}], caching=False, client=None, n=1, stream=False, litellm_trace_id='0902602f-b550-4412-b9b0-36b2b89c5596', metadata={'model_group': 'gemini/gemini-2.0-flash-exp', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'gemini/gemini-2.0-flash-exp', 'model_info': {'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, 'api_base': None, 'deployment_model_name': 'gemini/gemini-2.0-flash-exp', 'caching_groups': None}, model_info={'id': '8d16764488e854b1b8d7ac02d1aaf2580c7b449df711871b4a21d1515c0c25be', 'db_model': False}, timeout=60.0, max_retries=0)[0m
2025-12-08 14:33:37,185 - DEBUG - 

2025-12-08 14:33:37,185 - DEBUG - self.optional_params: {}
2025-12-08 14:33:37,185 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:33:37,185 - DEBUG -  NO SHARED SESSION: acompletion called without shared_session
2025-12-08 14:33:37,186 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:37,186 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x109e743d0>>
2025-12-08 14:33:37,186 - INFO - 
LiteLLM completion() model= gemini-2.0-flash-exp; provider = gemini
2025-12-08 14:33:37,186 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:37,187 - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-exp', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': None, 'n': 1, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Subakan, Cem, et al. "Attention Is All You Need in Speech Separation." *Mila-Quebec AI Institute*, 2020.\n\n\nCitation JSON:'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}
2025-12-08 14:33:37,187 - DEBUG - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:37,187 - DEBUG - Final returned optional params: {'temperature': 0.0, 'candidate_count': 1}
2025-12-08 14:33:37,187 - DEBUG - self.optional_params: {'temperature': 0.0, 'n': 1, 'stream': False, 'max_retries': 0}
2025-12-08 14:33:37,187 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:37,187 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:37,187 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:37,188 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=*****FXM4 \
-H 'Content-Type: application/json' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': 'Extract the title, authors, and doi as a JSON from this MLA citation. If any field can not be found, return it as null. Use title, authors, and doi as keys, author\'s value should be a list of authors. Subakan, Cem, et al. "Attention Is All You Need in Speech Separation." *Mila-Quebec AI Institute*, 2020.\n\n\nCitation JSON:'}]}], 'generationConfig': {'temperature': 0.0, 'candidate_count': 1}}'
[0m

2025-12-08 14:33:38,054 - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```json\n{\n  \"title\": \"Attention Is All You Need in Speech Separation\",\n  \"authors\": [\n    \"Cem Subakan\",\n    \"et al.\"\n  ],\n  \"doi\": null\n}\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.00034868022283682454
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 87,
    "candidatesTokenCount": 52,
    "totalTokenCount": 139,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 87
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 52
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash-exp",
  "responseId": "sdM2aY_MEp2dvdIP2qvdgA8"
}



2025-12-08 14:33:38,055 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:38,055 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:38,056 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:38,056 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:38,056 - DEBUG - selected model name for cost calculation: gemini/gemini-2.0-flash-exp
2025-12-08 14:33:38,056 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:38,056 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:38,056 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:38,056 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:38,057 - DEBUG - response_cost: 0.0
2025-12-08 14:33:38,057 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:38,057 - INFO - litellm.acompletion(model=gemini/gemini-2.0-flash-exp)[32m 200 OK[0m
2025-12-08 14:33:38,057 - DEBUG - response_cost: 0.0
2025-12-08 14:33:38,057 - DEBUG - Async Response: ModelResponse(id='sdM2aY_MEp2dvdIP2qvdgA8', created=1765200817, model='gemini-2.0-flash-exp', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='```json\n{\n  "title": "Attention Is All You Need in Speech Separation",\n  "authors": [\n    "Cem Subakan",\n    "et al."\n  ],\n  "doi": null\n}\n```', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=52, prompt_tokens=87, total_tokens=139, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=52, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=87, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])
2025-12-08 14:33:38,058 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:38,058 - DEBUG - selected model name for cost calculation: vertex_ai/gemini-2.0-flash-exp
2025-12-08 14:33:38,059 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:38,059 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:38,059 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:38,059 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:38,060 - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-12-08 14:33:38,060 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:38,060 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:38,061 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:38,061 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:38,090 - DEBUG - Attempting to populate metadata query: {'authors': ['Cem Subakan', 'et al.'], 'title': 'Attention Is All You Need in Speech Separation', 'session': <aiohttp.client.ClientSession object at 0x109480650>} via providers=[<paperqa.clients.semantic_scholar.SemanticScholarProvider object at 0x1095dfc90>, <paperqa.clients.crossref.CrossrefProvider object at 0x1095ddd90>] processors=[<paperqa.clients.journal_quality.JournalQualityPostProcessor object at 0x1095dfd90>]
2025-12-08 14:33:38,090 - DEBUG - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1099dcd90>>
2025-12-08 14:33:38,090 - DEBUG - Filtered callbacks: ['cache']
2025-12-08 14:33:38,090 - WARNING - SEMANTIC_SCHOLAR_API_KEY environment variable not set. Semantic Scholar API rate limits may apply.
2025-12-08 14:33:38,091 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-12-08 14:33:39,686 - DEBUG - connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2025-12-08 14:33:39,706 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10a16b490>
2025-12-08 14:33:39,706 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10a3f3920> server_hostname='raw.githubusercontent.com' timeout=5
2025-12-08 14:33:39,733 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10a16a890>
2025-12-08 14:33:39,733 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:33:39,733 - DEBUG - send_request_headers.complete
2025-12-08 14:33:39,733 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:33:39,733 - DEBUG - send_request_body.complete
2025-12-08 14:33:39,733 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:33:39,755 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'52540'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3cd138251572ce0662c39b6cee3c3fb9dc040f283d3f3d4afe7b0952ce409f78"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'8389:10C5B:28F09C4:2ED595D:6936D36F'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 08 Dec 2025 13:33:39 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-lin1730049-LIN'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'2'), (b'X-Timer', b'S1765200820.817387,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'8c1fc19285ca50a6fe485ad8b1a5ca049d54a9d2'), (b'Expires', b'Mon, 08 Dec 2025 13:38:39 GMT'), (b'Source-Age', b'68')])
2025-12-08 14:33:39,755 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:33:39,775 - DEBUG - receive_response_body.complete
2025-12-08 14:33:39,775 - DEBUG - response_closed.started
2025-12-08 14:33:39,775 - DEBUG - response_closed.complete
2025-12-08 14:33:39,776 - DEBUG - close.started
2025-12-08 14:33:39,776 - DEBUG - close.complete
2025-12-08 14:33:39,780 - DEBUG - 

2025-12-08 14:33:39,780 - DEBUG - [92mRequest to litellm:[0m
2025-12-08 14:33:39,780 - DEBUG - [92mlitellm.aembedding(model='text-embedding-3-small', input=['ATTENTION IS ALL YOU NEED IN SPEECH SEPARATION\nCem Subakan1, Mirco Ravanelli1, Samuele Cornell2, Mirko Bronzi1, Jianyuan Zhong3\n1Mila-Quebec AI Institute, Canada,\n2Universit`a Politecnica delle Marche, Italy\n3University of Rochester, USA\nABSTRACT\nRecurrent Neural Networks (RNNs) have long been the dominant\narchitecture in sequence-to-sequence learning. RNNs, however, are\ninherently sequential models that do not allow parallelization of their\ncomputations. Transformers are emerging as a natural alternative to\nstandard RNNs, replacing recurrent computations with a multi-head\nattention mechanism.\nIn this paper, we propose the SepFormer, a novel RNN-free\nTransformer-based neural network for speech separation. The Sep-\nFormer learns short and long-term dependencies with a multi-scale\napproach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix\ndatasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an\nSI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the\nparallelization advantages of Transformers and achieves a compet-\nitive performance even when downsampling the encoded represen-\ntation by a factor of 8. It is thus signicantly faster and it is less\nmemory-demanding than the latest speech separation systems with\ncomparable performance.\nIndex Terms speech separation, source separation, trans-\nformer, attention, deep learning.\n1. INTRODUCTION\nRNNs are a crucial component of modern audio processing sys-\ntems and they are used in many different domains, including speech\nrecognition, synthesis, enhancement, and separation, just to name a\nfew. Especially when coupled with multiplicative gate mechanisms\n(like LSTM [1] and GRU [2, 3]), their recurrent connections are es-\nsential to learn long-term dependencies and properly manage speech\ncontexts. Nevertheless, the inherently sequential nature of RNNs\nimpairs an effective parallelization of the computations. This bot-\ntleneck is particularly evident when processing large datasets with\nlong sequences. On the other hand, Transformers [4] completely\navoid this bottleneck by eliminating recurrence and replacing it with\na fully attention-based mechanism. By attending to the whole se-\nquence at once, a direct connection can be established between dis-\ntant elements allowing Transformers to learn long-term dependen-\ncies more easily [5]. For this reason, Transformers are gaining con-\nsiderable popularity for speech processing and recently showed com-\npetitive performance in speech recognition [6], synthesis [7], en-\nhancement [8], diarization [9], as well as speaker recognition [10].\nLittle research has been done so far on Transformer-based mod-\nels for monaural audio source separation. The eld has been revo-\nlutionized by the adoption of deep learning techniques [1116], and\nwith recent works [1723] achieving impressive results by adopt-\ning an end-to-end approach. Most of the current speech separation\ntechniques [14, 15, 1722] require effective modeling of long input\nx Encoder h Masking Net Decoder\ns1\ns2\nm1\nm2\nFig. 1. The high-level description of our system: The encoder block\nestimates a learned-representation for the input signal, while the\nmasking network estimates optimal masks to separate the sources\npresent in the mixtures. The decoder nally reconstructs the esti-\nmated sources in the time domain using the masks provided by the\nmasking network.\nsequences to perform well. Current systems rely, in large part, on the\nlearned-domain masking strategy popularized by Conv-TasNet [15].\nIn this framework, an overcomplete set of analysis and synthesis l-\nters is learned directly from the data, and separation is performed\nby estimating a mask for each source in this learned-domain. Build-\ning on this, Dual-Path RNN (DPRNN) [17] has demonstrated that\nbetter long-term modeling is crucial to improve the separation per-\nformance. This is achieved by splitting the input sequence into mul-\ntiple chunks that are processed locally and globally with different\nRNNs. Nevertheless, due to the use of RNNs, DPRNN still suffers\nfrom the aforementioned limitations of recurrent connections, espe-\ncially regarding the global processing step. An attempt to integrate\ntransformers into the speech separation pipeline has been recently\ndone in [22] where the proposed Dual-Path Transformer Network\n(DPTNet) is shown to outperform the standard DPRNN. Such an ar-\nchitecture, however, still embeds an RNN, effectively negating the\nparallelization capability of pure-attention models.\nIn this paper, we propose a novel model called SepFormer (Sep-\naration Transformer), which is mainly composed of multi-head at-\ntention and feed-forward layers. We adopt the dual-path frame-\nwork introduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the', 'oduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the\ndual-path framework process smaller chunks.\nTo the best of our knowledge, this is the rst work showing\nthat we can obtain state-of-the-art performance in separation with an\nRNN-free Transformer-based architecture. The SepFormer achieves\nan SI-SNRi of 22.3 dB on the standard WSJ0-2mix dataset. It also\nachieves the SOTA performance of 19.5 dB SI-SNRi on the WSJ0-\n3mix dataset. The SepFormer not only processes all the time steps in\nparallel but also achieves competitive performance when downsam-\npling the encoded representation by a factor of 8. This makes the\nproposed architecture signicantly faster and less memory demand-\narXiv:2010.13154v2  [eess.AS]  8 Mar 2021ing than the latest RNN-based separation models.\n2. THE MODEL\nThe proposed model is based on the learned-domain masking ap-\nproach [14, 15, 1722] and employs an encoder, a decoder, and a\nmasking network, as shown in Figure 1. The encoder is fully con-\nvolutional, while the masking network employs two Transformers\nembedded inside the dual-path processing block proposed in [17].\nThe decoder nally reconstructs the separated signals in the time\ndomain by using the masks predicted by the masking network. To\nfoster reproducibility, the SepFormer will be made available within\nthe SpeechBrain toolkit1.\n2.1. Encoder\nThe encoder takes in the time-domain mixture-signal x  RT as\ninput, which contains audio from multiple speakers. It learns an\nSTFT-like representation h RFT\nusing a single convolutional\nlayer:\nh = ReLU(conv1d(x)). (1)\nAs we will describe in Sec. 4, the stride factor of this convolution\nimpacts signicantly on the performance, speed, and memory of the\nmodel.\n2.2. Masking Network\nFigure 2 (top) shows the detailed architecture of the masking net-\nwork (Masking Net). The masking network is fed by the encoded\nrepresentations h RFT\nand estimates a mask {m1, . . . , mNs }\nfor each of the Ns speakers in the mixture.\nAs in [15], the encoded inputh is normalized with layer normal-\nization [24] and processed by a linear layer (with dimensionalityF).\nWe then create overlapping chunks of size C by chopping up h on\nthe time axis with an overlap factor of 50%. We denote the output of\nthe chunking operation with hRFCNc , where C is the length\nof each chunk, and Nc is the resulting number of chunks.\nThe representation hfeeds the SepFormer block, which is the\nmain component of the masking network. This block, which will be\ndescribed in detail in Sec. 2.3, employs a pipeline composed of two\ntransformers able to learn short and long-term dependencies.\nThe output of the SepFormer h RFCNc is processed by\nPReLU activations followed by a linear layer. We denote the output\nof this module hR(FNs)CNc , where Ns is the number of\nspeakers. Afterwards we apply the overlap-add scheme described\nin [17] and obtain h RFNsT\n. We pass this representation\nthrough two feed-forward layers and a ReLU activation at the end to\nnally obtain the mask mk for each of the speakers.\n2.3. SepFormer Block\nFigure 2 (Middle) shows the architecture of the SepFormer block.\nThe SepFormer block is designed to model both short and long-\nterm dependencies with the dual-scale approach of DPRNNs [17].\nIn our model, the transformer block which models the short-term\ndependencies is named IntraTransformer (IntraT), and the block for\nlonger-term dependencies is named InterTransformer (InterT). In-\ntraT processes the second dimension of h, and thus acts on each\nchunk independently, modeling the short-term dependencies within\n1speechbrain.github.io/\neach chunk. Next, we permute the last two dimensions (which we\ndenote with P), and the InterT is applied to model the transitions\nacross chunks. This scheme enables effective modelling of long-\nterm dependencies across the chunks. The overall transformation of\nthe SepFormer is therefore dened as follows:\nh= finter(P(fintra(h))), (2)\nwhere we denote the IntraT and InterT with finter(.), and fintra(.),\nrespectively. The overall SepFormer block is repeated N times.\n2.3.1. Intra and Inter Transformers\nFigure 2 (Bottom) shows the architecture of the Transformers used\nfor both the IntraT and InterT blocks. It closely resembles the orig-\ninal one dened in [4]. We use the variable z to denote the input\nto the Transformer. First of all, sinusoidal positional encoding e is\nadded to the input z, such that,\nz= z + e. (3)\nPositional encoding injects information on the order of the various\nelements composing the sequence, thus improving the separation\nperformance. We follow the positional encoding denition in [4].\nWe then apply multiple Transformer layers. Inside each Trans-\nformer layer g(.), we rst apply layer normalization, followed by\nmulti-head attention (MHA):\nz= MultiHeadAttention', 'ation\nperformance. We follow the positional encoding denition in [4].\nWe then apply multiple Transformer layers. Inside each Trans-\nformer layer g(.), we rst apply layer normalization, followed by\nmulti-head attention (MHA):\nz= MultiHeadAttention(LayerNorm(z)). (4)\nAs proposed in [4], each attention head computes the scaled dot-\nproduct attention between all the elements of the sequence. The\nTransformer nally employs a feed-forward network (FFW), which\nis applied to each position independently:\nz= FeedForward(LayerNorm(z+ z)) +z+ z. (5)\nThe overall transformer block is therefore dened as follows:\nf(z) =gK(z + e) +z, (6)\nwhere gK(.) denotes K layers of transformer layer g(.). We use\nK = Nintra layers for the IntraT, and K = Ninter layers for the\nInterT. As shown in Figure 2 (Bottom) and Eq. (6), we add residual\nconnections across the transformer layers, and across the transformer\narchitecture to improve gradient backpropagation.\n2.4. Decoder\nThe decoder simply uses a transposed convolution layer, with the\nsame stride and kernel size of the encoder. The input to the de-\ncoder is the element-wise multiplication between the mask mk of\nthe source k and the output of the encoder h. The transformation of\nthe decoder can therefore be expressed as follows:\nsk = conv1d-transpose(mk h), (7)\nwhere sk RT denotes the separated source k.\n3. EXPERIMENTAL SETUP\n3.1. Dataset\nWe use the popular WSJ0-2mix and WSJ0-3mix datasets [11] for\nsource separation, where mixtures of two speakers and three speak-\ners are created by randomly mixing utterances in the WSJ0 corpus.\nThe relative levels for the sources are sampled uniformly between 0\ndB to 5 dB. Respectively, 30, 10, 5 hours of speech is used for train-\ning, validation, and test. The training and test sets are created with\ndifferent sets of speakers. The waveforms are sampled at 8 kHz.h Norm+Linear Chunking SepFormer PReLU+Linear OverlapAdd FFW+ReLU\nm1\nm2\nh h h h\nRepeat N times\nh IntraTransformer Permute InterTransformer h\nRepeat K times\nz\ne\nLayerNorm MHA LayerNorm FFW z f(z)z z\nFig. 2. (Top) The overall architecture proposed for the masking network. (Middle) The SepFormer Block. (Bottom) The transformer\narchitecture f(.) that is used both in the IntraTransformer block and in the InterTransformer block.\n3.2. Architecture and Training Details\nThe encoder is based on 256 convolutional lters with a kernel size\nof 16 samples and a stride factor of 8 samples. The decoder uses the\nsame kernel size and the stride factors of the encoder.\nIn our best models, the SepFormer masking network processes\nchunks of size C = 250 with a 50 % overlap between them and\nemploys 8 layers of transformers in both IntraT and InterT. The\nIntraT-InterT dual-path processing pipeline is repeatedN = 2times.\nWe used 8 parallel attention heads, and 1024-dimensional positional\nfeed-forward networks within each Transformer layer. The model\nhas a total of 26 million parameters.\nWe explored the use of dynamic mixing (DM) data augmenta-\ntion [23] which consists in on-the-y creation of new mixtures from\nsingle speaker sources. In this work we expanded this powerful tech-\nnique by applying also speed perturbation on the sources before mix-\ning them. The speed randomly changes between 95 % slow-down\nand 105 % speed-up.\nWe used the Adam algorithm [25] as optimizer, with a learn-\ning rate of 15e5. After epoch 65 (after epoch 100 with DM), the\nlearning rate is annealed by halving it if we do not observe any im-\nprovement of the validation performance for 3 successive epochs\n(5 epoch for DM). Gradient clipping is employed to limit the L2\nnorm of the gradients to 5. During training, we used a batch size of\n1, and used the scale-invariant signal-to-noise Ratio (SI-SNR) [26]\nvia utterance-level permutation invariant loss [13], with clipping at\n30dB [23]. We used automatic mixed-precision to speed up training.\nThe system is trained for a maximum of 200 epochs. Each epoch\ntakes approximately 1.5 hours on a single NVIDIA V100 GPU with\n32 GB of memory.\n4. RESULTS\n4.1. Results on WSJ0-2mix\nTable 1 compares the performance achieved by the proposed Sep-\nFormer with the best results reported in the literature on the WSJ0-\n2mix dataset. The SepFormer achieves an SI-SNR improvement (SI-\nSNRi) of 22.3 dB and a Signal-to-Distortion Ratio [30] (SDRi) im-\nprovement of 22.4 dB on the test-set with dynamic mixing. When\nusing dynamic mixing, the proposed architecture achieves state-of-\nthe-art performance. The SepFormer outperforms previous systems\nwithout using dynamic mixing except Wavesplit, which uses speaker\nidentity as additional information.\nTable 1. Best results on the WSJ0-2mix dataset (test-set). DM\nstands for dynamic mixing.\nModel SI-SNRi SDRi # Param Stride\nTasnet [27] 10.8 11.1 n.a 20\nSignPredictionNet [28] 15.3 15.6 55.2M 8\nConvTasnet [15] 15.3 15.6 5.1M 10\nTwo-Step CTN [29] 16.1 n.a. 8.6M 10\nDeepCASA [18] 17.7 18.0 12.8M 1\nFurcaNeXt [19] n.a. 18.4 51.4M n.a.\nDualPathRNN [17] 18.8 19.0 ', '# Param Stride\nTasnet [27] 10.8 11.1 n.a 20\nSignPredictionNet [28] 15.3 15.6 55.2M 8\nConvTasnet [15] 15.3 15.6 5.1M 10\nTwo-Step CTN [29] 16.1 n.a. 8.6M 10\nDeepCASA [18] 17.7 18.0 12.8M 1\nFurcaNeXt [19] n.a. 18.4 51.4M n.a.\nDualPathRNN [17] 18.8 19.0 2.6M 1\nsudo rm -rf [21] 18.9 n.a. 2.6M 10\nVSUNOS [20] 20.1 20.4 7.5M 2\nDPTNet* [22] 20.2 20.6 2.6M 1\nWavesplit** [23] 21.0 21.2 29M 1\nWavesplit** + DM [23] 22.2 22.3 29M 1\nSepFormer 20.4 20.5 26M 8\nSepFormer + DM 22.3 22.4 26M 8\n*only SI-SNR and SDR (without improvement) are reported.\n**uses speaker-ids as additional info.\nTable 2. Ablation of the SepFormer on WSJ0-2Mix (validation set).\nSI-SNRi N Nintra Ninter # Heads DFF PosEnc DM\n22.3 2 8 8 8 1024 Yes Yes\n20.5 2 8 8 8 1024 Yes No\n20.4 2 4 4 16 2048 Yes No\n20.2 2 4 4 8 2048 Yes No\n19.9 2 4 4 8 2048 Yes No\n19.8 3 4 4 8 2048 Yes No\n19.4 2 4 4 8 2048 No No\n19.2 2 4 1 8 2048 Yes No\n19.1 2 3 3 8 2048 Yes No\n19.0 2 3 3 8 2048 No No\n4.2. Ablation Study\nHereafter we study the effect of various hyperparameters and data\naugmentation on the performance of the SepFormer using WSJ0-\n2mix dataset. The results are summarized in Table 2. The reported\nperformance in this table is calculated on the validation set.\nWe observe that the number of InterT and IntraT blocks has an\nimportant impact on the performance. The best results are achieved\nwith 8 layers for both blocks replicated two times. We also would\nlike to point out that a respectable performance of 19.2 dB is ob-\ntained even when we use a single layer transformer for the Inter-0 10 20 30 40\nTraining time (hours)\n8\n10\n12\n14\n16\n18\n20SI-SNRi (dB)\nTraining Speed on WSJ0-2Mix\nSepFormer\nDP-RNN\nDPTNet\n1.0 2.0 3.0 4.0 5.0\nInput sequence length in seconds\n32.0\n55.0\n95.0\n166.0\n288.0\n501.0miliseconds\nAverage Forward-Pass Time\nSepFormer\nDP-RNN\nDPTNet\nWavesplit\n1.0 2.0 3.0 4.0 5.0\nInput sequence length in seconds\n2.0\n3.0\n6.0\n11.0\n21.0\n40.0GBytes\nMemory Usage\nSepFormer\nDP-RNN\nDPTNet\nWavesplit\nFig. 3. ( Left) The traning curves of SepFormer, DPRNN, and DPTNeT on the WSJ0-2mix dataset. (Middle & Right)The comparison of\nforward-pass speed and memory usage in the GPU on inputs ranging 1-5 seconds long sampled at 8kHz.\nTable 3. Best results on the WSJ0-3mix dataset.\nModel SI-SNRi SDRi # Param\nConvTasnet [15] 12.7 13.1 5.1M\nDualPathRNN [17] 14.7 n.a 2.6M\nVSUNOS [20] 16.9 n.a 7.5M\nWavesplit [23] 17.3 17.6 29M\nWavesplit [23] + DM 17.8 18.1 29M\nSepformer 17.6 17.9 26M\nSepformer + DM 19.5 19.7 26M\nTransformer. This suggests that the IntraTransformer, and thus lo-\ncal processing, has a greater inuence on the performance. It also\nemerges that positional encoding is helpful (e.g. see lines 3 and 5 of\nTable 2). A similar outcome has been observed in [31] for speech\nenhancement. As for the number of attention heads, we observe a\nslight performance difference between 8 and 16 heads. Finally, it\ncan be observed that dynamic mixing helps the performance signi-\ncantly.\n4.3. Results on WSJ0-3mix\nTable 3 showcases the best performing models on the WSJ0-3mix\ndataset. SepFormer obtains the state-of-the-art performance with an\nSI-SNRi of 19.5 dB and an SDRi of 19.7 dB. We used here the best\narchitecture found for the WSJ0-2mix dataset. The only difference is\nthat the decoder has now three outputs. It is worth noting that on this\ncorpus the SepFormer outperforms all previously proposed systems.\nOur results on WSJ0-2mix and WSJ0-3mix show that it is pos-\nsible to achieve state-of-the-art performance in separation with an\nRNN-free Transformer-based model. The big advantage of Sep-\nFormer over RNN-based systems like [17,20,22] is the possibility to\nparallelize the computations over different time steps. This leads to\nfaster training and inference, as described in the following section.\n4.4. Speed and Memory Comparison\nWe now compare the training and inference speed of our model with\nDPRNN [17] and DPTNet [22]. Figure 3 (left) shows the training\ncurves of the aforementioned models on the WSJ0-2mix dataset.\nWe plot the performance achieved on the validation set in the rst\n48 hours of training versus the wall-clock time. For a fair com-\nparison, we used the same machine with the same GPU (a single\nNVIDIA V100-32GB) for all the models. Moreover, all the systems\nare trained with a batch size of 1 and employ automatic mixed pre-\ncision. We observe that the SepFormer is faster than DPRNN and\nDPTNeT. Figure 3 (left), highlights that SepFormer reaches above\n17dB levels only after a full day of training, whereas the DPRNN\nmodel requires two days of training to achieve the same level of per-\nformance.\nFigure 3 (middle&right) compares the average computation time\n(in ms) and the total memory allocation (in GB) during inference\nwhen single precision is used. We analyze the speed of our best\nmodel for both WSJ0-2Mix and WSJ0-3Mix datasets. We compare\nour models against DP-RNN, DPTNeT, and Wavesplit. All the mod-\nels are stored in the same NVIDIA RTX8000-48GB GPU and we\nperformed this analysis using the PyTorch proler [32', 'eed of our best\nmodel for both WSJ0-2Mix and WSJ0-3Mix datasets. We compare\nour models against DP-RNN, DPTNeT, and Wavesplit. All the mod-\nels are stored in the same NVIDIA RTX8000-48GB GPU and we\nperformed this analysis using the PyTorch proler [32]. For Waves-\nplit we used the implementation in [33].\nFrom this analysis, it emerges that the SepFormer is not only\nfaster but also less memory demanding than DPTNet, DPRNN, and\nWavesplit. We observed the same behavior using the CPU for infer-\nence also. Such a level of computational efciency is achieved even\nthough the proposed SepFormer employs more parameters than the\nother RNN-based methods (see Table 1). This is not only due to the\nsuperior parallelization capabilities of the proposed model, but also\nbecause the best performance is achieved with a stride factor of 8\nsamples, against a stride of 1 for DPRNN and DPTNet. Increasing\nthe stride of the encoder results in downsampling the input sequence,\nand therefore the model processes less data. In [17], the authors\nshowed that the DPRNN performance degrades when increasing the\nstride factor. The SepFormer, instead, reaches competitive results\neven with a relatively large stride, leading to the aforementioned\nspeed and memory advantages.\n5. CONCLUSIONS\nIn this paper, we proposed a novel neural model for speech sepa-\nration called SepFormer (Separation Transformer). The SepFormer\nis an RNN-free architecture that employs a masking network com-\nposed of transformers only. The masking network learns both short\nand long-term dependencies using a multi-scale approach. Our re-\nsults, reported on the WSJ0-2mix and WSJ0-3mix datasets, high-\nlight that we can reach state-of-the-art performances in source sep-\naration without using RNNs in the network design. This way, com-\nputations over different time-steps can be parallelized. Moreover,\nour model achieves a competitive performance even when subsam-\npling the encoded representation by a factor of 8. These two prop-\nerties lead to a signicant speed-up at training/inference time and\na drastic reduction of memory usage, especially when compared to\nrecent models such as DPRNN, DPTNet, and Wavesplit. As future\nwork, we would like to explore different transformer architectures\nthat could potentially further improve performance, speed, and mem-\nory usage.6. REFERENCES\n[1] S. Hochreiter and J. Schmidhuber, Long short-term memory,\nNeural Computation, vol. 9, no. 8, pp. 17351780, Nov. 1997.\n[2] K. Cho, B. van Merri enboer, D. Bahdanau, and Y . Bengio, On\nthe properties of neural machine translation: Encoderdecoder\napproaches, in Proc. of SSST, 2014, pp. 103111.\n[3] M. Ravanelli, P. Brakel, M. Omologo, and Y . Bengio, Light\ngated recurrent units for speech recognition, IEEE Transac-\ntions on Emerging Topics in Computational Intelligence, vol.\n2, no. 2, pp. 92102, April 2018.\n[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all\nyou need, CoRR, vol. abs/1706.03762, 2017.\n[5] G. Kerg, B. Kanuparthi, A. Goyal, K. Goyette, Y . Bengio, and\nG. Lajoie, Untangling tradeoffs between recurrence and self-\nattention in neural networks, CoRR, vol. abs/2006.09471,\n2020.\n[6] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y . Soplin, R. Yamamoto, X. Wang, S. Watan-\nabe, T. Yoshimura, and W. Zhang, A comparative study on\ntransformer vs rnn in speech applications, in Proc. of ASRU,\n2019, pp. 449456.\n[7] N. Li, S. Liu, Y . Liu, S. Zhao, and M. Liu, Neural speech\nsynthesis with transformer network, in Proc. of AAAI, 2019,\npp. 67066713.\n[8] J. Kim, M. El-Khamy, and J. Lee, T-gsa: Transformer with\ngaussian-weighted self-attention for speech enhancement, in\nProc. of ICASSP, 2020, pp. 66496653.\n[9] Q. Li, F. L. Kreyssig, C. Zhang, and P. C. Woodland, Dis-\ncriminative neural clustering for speaker diarisation, CoRR,\nvol. abs/1910.09703, 2019.\n[10] X. Chang, W. Zhang, Y . Qian, J. Le Roux, and S. Watan-\nabe, End-to-end multi-speaker speech recognition with trans-\nformer, in Proc. of ICASSP, 2020, pp. 61346138.\n[11] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, Deep\nclustering: Discriminative embeddings for segmentation and\nseparation, in Proc. of ICASSP, 2016, pp. 3135.\n[12] D. Yu, M. Kolbk, Z. Tan, and J. Jensen, Permutation in-\nvariant training of deep models for speaker-independent multi-\ntalker speech separation, in Proc. of ICASSP, 2017, pp. 241\n245.\n[13] M. Kolbk, D. Yu, Z.-H. Tan, and J. Jensen, Multitalker\nspeech separation with utterance-level permutation invariant\ntraining of deep recurrent neural networks,IEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing, vol. 25,\nno. 10, pp. 19011913, 2017.\n[14] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis,\nEnd-to-end source separation with adaptive front-ends, in\nProc. of ACSSC, 2018, pp. 684688.\n[15] Y . Luo and N. Mesgarani, Conv-TasNet: Surpassing Ideal\nTimeFrequency Magni', ' 2017.\n[14] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis,\nEnd-to-end source separation with adaptive front-ends, in\nProc. of ACSSC, 2018, pp. 684688.\n[15] Y . Luo and N. Mesgarani, Conv-TasNet: Surpassing Ideal\nTimeFrequency Magnitude Masking for Speech Separation,\nvol. 27, no. 8, pp. 12561266, Aug. 2019.\n[16] P. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,\nDeep learning for monoaural source separation, in Proc. of\nICASSP, 2014, pp. 15621566.\n[17] Y . Luo, Z. Chen, and T. Yoshioka, Dual-path rnn: ef-\ncient long sequence modeling for time-domain single-channel\nspeech separation, in Proc. of ICASSP, 2020, pp. 4650.\n[18] Y . Liu and D. Wang, Divide and conquer: A deep casa\napproach to talker-independent monaural speaker separation,\nIEEE/ACM Transactions on audio, speech, and language pro-\ncessing, vol. 27, no. 12, 2019.\n[19] Z. Shi, H. Lin, L. Liu, R. Liu, J. Han, and A. Shi, Furcanext:\nEnd-to-end monaural speech separation with dynamic gated di-\nlated temporal convolutional networks, in MultiMedia Mod-\neling, 2020, pp. 653665.\n[20] E. Nachmani, Y . Adi, and L. Wolf, V oice separation with\nan unknown number of multiple speakers, ICML, pp. 7164\n7175, 2020.\n[21] E. Tzinis, Z. Wang, and P. Smaragdis, Sudo rm -rf: Efcient\nnetworks for universal audio source separation, in MLSP,\n2020, pp. 16.\n[22] J. Chen, Q. Mao, and D. Liu, Dual-Path Transformer\nNetwork: Direct Context-Aware Modeling for End-to-End\nMonaural Speech Separation, in Proc. of Interspeech 2020,\n2020, pp. 26422646.\n[23] N. Zeghidour and D. Grangier, Wavesplit: End-to-end\nspeech separation by speaker clustering, arXiv preprint\narXiv:2002.08933, 2020.\n[24] L. J. Ba, J. R. Kiros, and G. E. Hinton, Layer normalization,\nCoRR, vol. abs/1607.06450, 2016.\n[25] D. P. Kingma and J. Ba, Adam: A method for stochastic opti-\nmization, arXiv preprint arXiv:1412.6980, 2014.\n[26] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, Sdr\nhalf-baked or well done?, in Proc. of ICASSP. IEEE, 2019,\npp. 626630.\n[27] Y . Luo and N. Mesgarani, TasNet: time-domain audio separa-\ntion network for real-time, single-channel speech separation,\nCoRR, vol. abs/1711.00541, 2017.\n[28] Zhong-Qiu Wang, Ke Tan, and DeLiang Wang, Deep learning\nbased phase reconstruction for speaker separation: A trigono-\nmetric perspective, in Proc. of ICASSP, 2019, pp. 7175.\n[29] E. Tzinis, S. Venkataramani, Z. Wang, C. Subakan, and\nP. Smaragdis, Two-step sound source separation: Training on\nlearned latent targets, in Proc. of ICASSP, 2020, pp. 3135.\n[30] E. Vincent, R. Gribonval, and C F evotte, Performance mea-\nsurement in blind audio source separation, IEEE transactions\non audio, speech, and language processing, vol. 14, no. 4, pp.\n14621469, 2006.\n[31] J. Kim, M. El-Khamy, and J. Lee, T-gsa: Transformer with\ngaussian-weighted self-attention for speech enhancement, in\nProc. of ICASSP, 2020, pp. 66496653.\n[32] Pytorch, Proler, https://pytorch.org/\ntutorials/recipes/recipes/profiler.html,\n2020, Accessed: 2020-10-21.\n[33] M. Pariente, S. Cornell, J. Cosentino, S. Sivasankaran, E. Tzi-\nnis, J. Heitkaemper, M. Olvera, F.-R. St oter, M. Hu, J. M.\nMartn-Donas, D. Ditter, A. Frank, A. Deleforge, and E. Vin-\ncent, Asteroid: the PyTorch-based audio source separation\ntoolkit for researchers, in Proc. of Interspeech, 2020, pp.\n26372641.'], dimensions=1536, num_retries=3, timeout=120)[0m
2025-12-08 14:33:39,780 - DEBUG - 

2025-12-08 14:33:39,780 - DEBUG - self.optional_params: {}
2025-12-08 14:33:39,780 - DEBUG - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
2025-12-08 14:33:39,780 - DEBUG - self.optional_params: {'dimensions': 1536}
2025-12-08 14:33:39,781 - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://api.openai.com/v1 \
-d '{'model': 'text-embedding-3-small', 'input': ['ATTENTION IS ALL YOU NEED IN SPEECH SEPARATION\nCem Subakan1, Mirco Ravanelli1, Samuele Cornell2, Mirko Bronzi1, Jianyuan Zhong3\n1Mila-Quebec AI Institute, Canada,\n2Universit`a Politecnica delle Marche, Italy\n3University of Rochester, USA\nABSTRACT\nRecurrent Neural Networks (RNNs) have long been the dominant\narchitecture in sequence-to-sequence learning. RNNs, however, are\ninherently sequential models that do not allow parallelization of their\ncomputations. Transformers are emerging as a natural alternative to\nstandard RNNs, replacing recurrent computations with a multi-head\nattention mechanism.\nIn this paper, we propose the SepFormer, a novel RNN-free\nTransformer-based neural network for speech separation. The Sep-\nFormer learns short and long-term dependencies with a multi-scale\napproach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix\ndatasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an\nSI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the\nparallelization advantages of Transformers and achieves a compet-\nitive performance even when downsampling the encoded represen-\ntation by a factor of 8. It is thus signicantly faster and it is less\nmemory-demanding than the latest speech separation systems with\ncomparable performance.\nIndex Terms speech separation, source separation, trans-\nformer, attention, deep learning.\n1. INTRODUCTION\nRNNs are a crucial component of modern audio processing sys-\ntems and they are used in many different domains, including speech\nrecognition, synthesis, enhancement, and separation, just to name a\nfew. Especially when coupled with multiplicative gate mechanisms\n(like LSTM [1] and GRU [2, 3]), their recurrent connections are es-\nsential to learn long-term dependencies and properly manage speech\ncontexts. Nevertheless, the inherently sequential nature of RNNs\nimpairs an effective parallelization of the computations. This bot-\ntleneck is particularly evident when processing large datasets with\nlong sequences. On the other hand, Transformers [4] completely\navoid this bottleneck by eliminating recurrence and replacing it with\na fully attention-based mechanism. By attending to the whole se-\nquence at once, a direct connection can be established between dis-\ntant elements allowing Transformers to learn long-term dependen-\ncies more easily [5]. For this reason, Transformers are gaining con-\nsiderable popularity for speech processing and recently showed com-\npetitive performance in speech recognition [6], synthesis [7], en-\nhancement [8], diarization [9], as well as speaker recognition [10].\nLittle research has been done so far on Transformer-based mod-\nels for monaural audio source separation. The eld has been revo-\nlutionized by the adoption of deep learning techniques [1116], and\nwith recent works [1723] achieving impressive results by adopt-\ning an end-to-end approach. Most of the current speech separation\ntechniques [14, 15, 1722] require effective modeling of long input\nx Encoder h Masking Net Decoder\ns1\ns2\nm1\nm2\nFig. 1. The high-level description of our system: The encoder block\nestimates a learned-representation for the input signal, while the\nmasking network estimates optimal masks to separate the sources\npresent in the mixtures. The decoder nally reconstructs the esti-\nmated sources in the time domain using the masks provided by the\nmasking network.\nsequences to perform well. Current systems rely, in large part, on the\nlearned-domain masking strategy popularized by Conv-TasNet [15].\nIn this framework, an overcomplete set of analysis and synthesis l-\nters is learned directly from the data, and separation is performed\nby estimating a mask for each source in this learned-domain. Build-\ning on this, Dual-Path RNN (DPRNN) [17] has demonstrated that\nbetter long-term modeling is crucial to improve the separation per-\nformance. This is achieved by splitting the input sequence into mul-\ntiple chunks that are processed locally and globally with different\nRNNs. Nevertheless, due to the use of RNNs, DPRNN still suffers\nfrom the aforementioned limitations of recurrent connections, espe-\ncially regarding the global processing step. An attempt to integrate\ntransformers into the speech separation pipeline has been recently\ndone in [22] where the proposed Dual-Path Transformer Network\n(DPTNet) is shown to outperform the standard DPRNN. Such an ar-\nchitecture, however, still embeds an RNN, effectively negating the\nparallelization capability of pure-attention models.\nIn this paper, we propose a novel model called SepFormer (Sep-\naration Transformer), which is mainly composed of multi-head at-\ntention and feed-forward layers. We adopt the dual-path frame-\nwork introduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the', 'oduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the\ndual-path framework process smaller chunks.\nTo the best of our knowledge, this is the rst work showing\nthat we can obtain state-of-the-art performance in separation with an\nRNN-free Transformer-based architecture. The SepFormer achieves\nan SI-SNRi of 22.3 dB on the standard WSJ0-2mix dataset. It also\nachieves the SOTA performance of 19.5 dB SI-SNRi on the WSJ0-\n3mix dataset. The SepFormer not only processes all the time steps in\nparallel but also achieves competitive performance when downsam-\npling the encoded representation by a factor of 8. This makes the\nproposed architecture signicantly faster and less memory demand-\narXiv:2010.13154v2  [eess.AS]  8 Mar 2021ing than the latest RNN-based separation models.\n2. THE MODEL\nThe proposed model is based on the learned-domain masking ap-\nproach [14, 15, 1722] and employs an encoder, a decoder, and a\nmasking network, as shown in Figure 1. The encoder is fully con-\nvolutional, while the masking network employs two Transformers\nembedded inside the dual-path processing block proposed in [17].\nThe decoder nally reconstructs the separated signals in the time\ndomain by using the masks predicted by the masking network. To\nfoster reproducibility, the SepFormer will be made available within\nthe SpeechBrain toolkit1.\n2.1. Encoder\nThe encoder takes in the time-domain mixture-signal x  RT as\ninput, which contains audio from multiple speakers. It learns an\nSTFT-like representation h RFT\nusing a single convolutional\nlayer:\nh = ReLU(conv1d(x)). (1)\nAs we will describe in Sec. 4, the stride factor of this convolution\nimpacts signicantly on the performance, speed, and memory of the\nmodel.\n2.2. Masking Network\nFigure 2 (top) shows the detailed architecture of the masking net-\nwork (Masking Net). The masking network is fed by the encoded\nrepresentations h RFT\nand estimates a mask {m1, . . . , mNs }\nfor each of the Ns speakers in the mixture.\nAs in [15], the encoded inputh is normalized with layer normal-\nization [24] and processed by a linear layer (with dimensionalityF).\nWe then create overlapping chunks of size C by chopping up h on\nthe time axis with an overlap factor of 50%. We denote the output of\nthe chunking operation with hRFCNc , where C is the length\nof each chunk, and Nc is the resulting number of chunks.\nThe representation hfeeds the SepFormer block, which is the\nmain component of the masking network. This block, which will be\ndescribed in detail in Sec. 2.3, employs a pipeline composed of two\ntransformers able to learn short and long-term dependencies.\nThe output of the SepFormer h RFCNc is processed by\nPReLU activations followed by a linear layer. We denote the output\nof this module hR(FNs)CNc , where Ns is the number of\nspeakers. Afterwards we apply the overlap-add scheme described\nin [17] and obtain h RFNsT\n. We pass this representation\nthrough two feed-forward layers and a ReLU activation at the end to\nnally obtain the mask mk for each of the speakers.\n2.3. SepFormer Block\nFigure 2 (Middle) shows the architecture of the SepFormer block.\nThe SepFormer block is designed to model both short and long-\nterm dependencies with the dual-scale approach of DPRNNs [17].\nIn our model, the transformer block which models the short-term\ndependencies is named IntraTransformer (IntraT), and the block for\nlonger-term dependencies is named InterTransformer (InterT). In-\ntraT processes the second dimension of h, and thus acts on each\nchunk independently, modeling the short-term dependencies within\n1speechbrain.github.io/\neach chunk. Next, we permute the last two dimensions (which we\ndenote with P), and the InterT is applied to model the transitions\nacross chunks. This scheme enables effective modelling of long-\nterm dependencies across the chunks. The overall transformation of\nthe SepFormer is therefore dened as follows:\nh= finter(P(fintra(h))), (2)\nwhere we denote the IntraT and InterT with finter(.), and fintra(.),\nrespectively. The overall SepFormer block is repeated N times.\n2.3.1. Intra and Inter Transformers\nFigure 2 (Bottom) shows the architecture of the Transformers used\nfor both the IntraT and InterT blocks. It closely resembles the orig-\ninal one dened in [4]. We use the variable z to denote the input\nto the Transformer. First of all, sinusoidal positional encoding e is\nadded to the input z, such that,\nz= z + e. (3)\nPositional encoding injects information on the order of the various\nelements composing the sequence, thus improving the separation\nperformance. We follow the positional encoding denition in [4].\nWe then apply multiple Transformer layers. Inside each Trans-\nformer layer g(.), we rst apply layer normalization, followed by\nmulti-head attention (MHA):\nz= MultiHeadAttention', 'ation\nperformance. We follow the positional encoding denition in [4].\nWe then apply multiple Transformer layers. Inside each Trans-\nformer layer g(.), we rst apply layer normalization, followed by\nmulti-head attention (MHA):\nz= MultiHeadAttention(LayerNorm(z)). (4)\nAs proposed in [4], each attention head computes the scaled dot-\nproduct attention between all the elements of the sequence. The\nTransformer nally employs a feed-forward network (FFW), which\nis applied to each position independently:\nz= FeedForward(LayerNorm(z+ z)) +z+ z. (5)\nThe overall transformer block is therefore dened as follows:\nf(z) =gK(z + e) +z, (6)\nwhere gK(.) denotes K layers of transformer layer g(.). We use\nK = Nintra layers for the IntraT, and K = Ninter layers for the\nInterT. As shown in Figure 2 (Bottom) and Eq. (6), we add residual\nconnections across the transformer layers, and across the transformer\narchitecture to improve gradient backpropagation.\n2.4. Decoder\nThe decoder simply uses a transposed convolution layer, with the\nsame stride and kernel size of the encoder. The input to the de-\ncoder is the element-wise multiplication between the mask mk of\nthe source k and the output of the encoder h. The transformation of\nthe decoder can therefore be expressed as follows:\nsk = conv1d-transpose(mk h), (7)\nwhere sk RT denotes the separated source k.\n3. EXPERIMENTAL SETUP\n3.1. Dataset\nWe use the popular WSJ0-2mix and WSJ0-3mix datasets [11] for\nsource separation, where mixtures of two speakers and three speak-\ners are created by randomly mixing utterances in the WSJ0 corpus.\nThe relative levels for the sources are sampled uniformly between 0\ndB to 5 dB. Respectively, 30, 10, 5 hours of speech is used for train-\ning, validation, and test. The training and test sets are created with\ndifferent sets of speakers. The waveforms are sampled at 8 kHz.h Norm+Linear Chunking SepFormer PReLU+Linear OverlapAdd FFW+ReLU\nm1\nm2\nh h h h\nRepeat N times\nh IntraTransformer Permute InterTransformer h\nRepeat K times\nz\ne\nLayerNorm MHA LayerNorm FFW z f(z)z z\nFig. 2. (Top) The overall architecture proposed for the masking network. (Middle) The SepFormer Block. (Bottom) The transformer\narchitecture f(.) that is used both in the IntraTransformer block and in the InterTransformer block.\n3.2. Architecture and Training Details\nThe encoder is based on 256 convolutional lters with a kernel size\nof 16 samples and a stride factor of 8 samples. The decoder uses the\nsame kernel size and the stride factors of the encoder.\nIn our best models, the SepFormer masking network processes\nchunks of size C = 250 with a 50 % overlap between them and\nemploys 8 layers of transformers in both IntraT and InterT. The\nIntraT-InterT dual-path processing pipeline is repeatedN = 2times.\nWe used 8 parallel attention heads, and 1024-dimensional positional\nfeed-forward networks within each Transformer layer. The model\nhas a total of 26 million parameters.\nWe explored the use of dynamic mixing (DM) data augmenta-\ntion [23] which consists in on-the-y creation of new mixtures from\nsingle speaker sources. In this work we expanded this powerful tech-\nnique by applying also speed perturbation on the sources before mix-\ning them. The speed randomly changes between 95 % slow-down\nand 105 % speed-up.\nWe used the Adam algorithm [25] as optimizer, with a learn-\ning rate of 15e5. After epoch 65 (after epoch 100 with DM), the\nlearning rate is annealed by halving it if we do not observe any im-\nprovement of the validation performance for 3 successive epochs\n(5 epoch for DM). Gradient clipping is employed to limit the L2\nnorm of the gradients to 5. During training, we used a batch size of\n1, and used the scale-invariant signal-to-noise Ratio (SI-SNR) [26]\nvia utterance-level permutation invariant loss [13], with clipping at\n30dB [23]. We used automatic mixed-precision to speed up training.\nThe system is trained for a maximum of 200 epochs. Each epoch\ntakes approximately 1.5 hours on a single NVIDIA V100 GPU with\n32 GB of memory.\n4. RESULTS\n4.1. Results on WSJ0-2mix\nTable 1 compares the performance achieved by the proposed Sep-\nFormer with the best results reported in the literature on the WSJ0-\n2mix dataset. The SepFormer achieves an SI-SNR improvement (SI-\nSNRi) of 22.3 dB and a Signal-to-Distortion Ratio [30] (SDRi) im-\nprovement of 22.4 dB on the test-set with dynamic mixing. When\nusing dynamic mixing, the proposed architecture achieves state-of-\nthe-art performance. The SepFormer outperforms previous systems\nwithout using dynamic mixing except Wavesplit, which uses speaker\nidentity as additional information.\nTable 1. Best results on the WSJ0-2mix dataset (test-set). DM\nstands for dynamic mixing.\nModel SI-SNRi SDRi # Param Stride\nTasnet [27] 10.8 11.1 n.a 20\nSignPredictionNet [28] 15.3 15.6 55.2M 8\nConvTasnet [15] 15.3 15.6 5.1M 10\nTwo-Step CTN [29] 16.1 n.a. 8.6M 10\nDeepCASA [18] 17.7 18.0 12.8M 1\nFurcaNeXt [19] n.a. 18.4 51.4M n.a.\nDualPathRNN [17] 18.8 19.0 ', '# Param Stride\nTasnet [27] 10.8 11.1 n.a 20\nSignPredictionNet [28] 15.3 15.6 55.2M 8\nConvTasnet [15] 15.3 15.6 5.1M 10\nTwo-Step CTN [29] 16.1 n.a. 8.6M 10\nDeepCASA [18] 17.7 18.0 12.8M 1\nFurcaNeXt [19] n.a. 18.4 51.4M n.a.\nDualPathRNN [17] 18.8 19.0 2.6M 1\nsudo rm -rf [21] 18.9 n.a. 2.6M 10\nVSUNOS [20] 20.1 20.4 7.5M 2\nDPTNet* [22] 20.2 20.6 2.6M 1\nWavesplit** [23] 21.0 21.2 29M 1\nWavesplit** + DM [23] 22.2 22.3 29M 1\nSepFormer 20.4 20.5 26M 8\nSepFormer + DM 22.3 22.4 26M 8\n*only SI-SNR and SDR (without improvement) are reported.\n**uses speaker-ids as additional info.\nTable 2. Ablation of the SepFormer on WSJ0-2Mix (validation set).\nSI-SNRi N Nintra Ninter # Heads DFF PosEnc DM\n22.3 2 8 8 8 1024 Yes Yes\n20.5 2 8 8 8 1024 Yes No\n20.4 2 4 4 16 2048 Yes No\n20.2 2 4 4 8 2048 Yes No\n19.9 2 4 4 8 2048 Yes No\n19.8 3 4 4 8 2048 Yes No\n19.4 2 4 4 8 2048 No No\n19.2 2 4 1 8 2048 Yes No\n19.1 2 3 3 8 2048 Yes No\n19.0 2 3 3 8 2048 No No\n4.2. Ablation Study\nHereafter we study the effect of various hyperparameters and data\naugmentation on the performance of the SepFormer using WSJ0-\n2mix dataset. The results are summarized in Table 2. The reported\nperformance in this table is calculated on the validation set.\nWe observe that the number of InterT and IntraT blocks has an\nimportant impact on the performance. The best results are achieved\nwith 8 layers for both blocks replicated two times. We also would\nlike to point out that a respectable performance of 19.2 dB is ob-\ntained even when we use a single layer transformer for the Inter-0 10 20 30 40\nTraining time (hours)\n8\n10\n12\n14\n16\n18\n20SI-SNRi (dB)\nTraining Speed on WSJ0-2Mix\nSepFormer\nDP-RNN\nDPTNet\n1.0 2.0 3.0 4.0 5.0\nInput sequence length in seconds\n32.0\n55.0\n95.0\n166.0\n288.0\n501.0miliseconds\nAverage Forward-Pass Time\nSepFormer\nDP-RNN\nDPTNet\nWavesplit\n1.0 2.0 3.0 4.0 5.0\nInput sequence length in seconds\n2.0\n3.0\n6.0\n11.0\n21.0\n40.0GBytes\nMemory Usage\nSepFormer\nDP-RNN\nDPTNet\nWavesplit\nFig. 3. ( Left) The traning curves of SepFormer, DPRNN, and DPTNeT on the WSJ0-2mix dataset. (Middle & Right)The comparison of\nforward-pass speed and memory usage in the GPU on inputs ranging 1-5 seconds long sampled at 8kHz.\nTable 3. Best results on the WSJ0-3mix dataset.\nModel SI-SNRi SDRi # Param\nConvTasnet [15] 12.7 13.1 5.1M\nDualPathRNN [17] 14.7 n.a 2.6M\nVSUNOS [20] 16.9 n.a 7.5M\nWavesplit [23] 17.3 17.6 29M\nWavesplit [23] + DM 17.8 18.1 29M\nSepformer 17.6 17.9 26M\nSepformer + DM 19.5 19.7 26M\nTransformer. This suggests that the IntraTransformer, and thus lo-\ncal processing, has a greater inuence on the performance. It also\nemerges that positional encoding is helpful (e.g. see lines 3 and 5 of\nTable 2). A similar outcome has been observed in [31] for speech\nenhancement. As for the number of attention heads, we observe a\nslight performance difference between 8 and 16 heads. Finally, it\ncan be observed that dynamic mixing helps the performance signi-\ncantly.\n4.3. Results on WSJ0-3mix\nTable 3 showcases the best performing models on the WSJ0-3mix\ndataset. SepFormer obtains the state-of-the-art performance with an\nSI-SNRi of 19.5 dB and an SDRi of 19.7 dB. We used here the best\narchitecture found for the WSJ0-2mix dataset. The only difference is\nthat the decoder has now three outputs. It is worth noting that on this\ncorpus the SepFormer outperforms all previously proposed systems.\nOur results on WSJ0-2mix and WSJ0-3mix show that it is pos-\nsible to achieve state-of-the-art performance in separation with an\nRNN-free Transformer-based model. The big advantage of Sep-\nFormer over RNN-based systems like [17,20,22] is the possibility to\nparallelize the computations over different time steps. This leads to\nfaster training and inference, as described in the following section.\n4.4. Speed and Memory Comparison\nWe now compare the training and inference speed of our model with\nDPRNN [17] and DPTNet [22]. Figure 3 (left) shows the training\ncurves of the aforementioned models on the WSJ0-2mix dataset.\nWe plot the performance achieved on the validation set in the rst\n48 hours of training versus the wall-clock time. For a fair com-\nparison, we used the same machine with the same GPU (a single\nNVIDIA V100-32GB) for all the models. Moreover, all the systems\nare trained with a batch size of 1 and employ automatic mixed pre-\ncision. We observe that the SepFormer is faster than DPRNN and\nDPTNeT. Figure 3 (left), highlights that SepFormer reaches above\n17dB levels only after a full day of training, whereas the DPRNN\nmodel requires two days of training to achieve the same level of per-\nformance.\nFigure 3 (middle&right) compares the average computation time\n(in ms) and the total memory allocation (in GB) during inference\nwhen single precision is used. We analyze the speed of our best\nmodel for both WSJ0-2Mix and WSJ0-3Mix datasets. We compare\nour models against DP-RNN, DPTNeT, and Wavesplit. All the mod-\nels are stored in the same NVIDIA RTX8000-48GB GPU and we\nperformed this analysis using the PyTorch proler [32', 'eed of our best\nmodel for both WSJ0-2Mix and WSJ0-3Mix datasets. We compare\nour models against DP-RNN, DPTNeT, and Wavesplit. All the mod-\nels are stored in the same NVIDIA RTX8000-48GB GPU and we\nperformed this analysis using the PyTorch proler [32]. For Waves-\nplit we used the implementation in [33].\nFrom this analysis, it emerges that the SepFormer is not only\nfaster but also less memory demanding than DPTNet, DPRNN, and\nWavesplit. We observed the same behavior using the CPU for infer-\nence also. Such a level of computational efciency is achieved even\nthough the proposed SepFormer employs more parameters than the\nother RNN-based methods (see Table 1). This is not only due to the\nsuperior parallelization capabilities of the proposed model, but also\nbecause the best performance is achieved with a stride factor of 8\nsamples, against a stride of 1 for DPRNN and DPTNet. Increasing\nthe stride of the encoder results in downsampling the input sequence,\nand therefore the model processes less data. In [17], the authors\nshowed that the DPRNN performance degrades when increasing the\nstride factor. The SepFormer, instead, reaches competitive results\neven with a relatively large stride, leading to the aforementioned\nspeed and memory advantages.\n5. CONCLUSIONS\nIn this paper, we proposed a novel neural model for speech sepa-\nration called SepFormer (Separation Transformer). The SepFormer\nis an RNN-free architecture that employs a masking network com-\nposed of transformers only. The masking network learns both short\nand long-term dependencies using a multi-scale approach. Our re-\nsults, reported on the WSJ0-2mix and WSJ0-3mix datasets, high-\nlight that we can reach state-of-the-art performances in source sep-\naration without using RNNs in the network design. This way, com-\nputations over different time-steps can be parallelized. Moreover,\nour model achieves a competitive performance even when subsam-\npling the encoded representation by a factor of 8. These two prop-\nerties lead to a signicant speed-up at training/inference time and\na drastic reduction of memory usage, especially when compared to\nrecent models such as DPRNN, DPTNet, and Wavesplit. As future\nwork, we would like to explore different transformer architectures\nthat could potentially further improve performance, speed, and mem-\nory usage.6. REFERENCES\n[1] S. Hochreiter and J. Schmidhuber, Long short-term memory,\nNeural Computation, vol. 9, no. 8, pp. 17351780, Nov. 1997.\n[2] K. Cho, B. van Merri enboer, D. Bahdanau, and Y . Bengio, On\nthe properties of neural machine translation: Encoderdecoder\napproaches, in Proc. of SSST, 2014, pp. 103111.\n[3] M. Ravanelli, P. Brakel, M. Omologo, and Y . Bengio, Light\ngated recurrent units for speech recognition, IEEE Transac-\ntions on Emerging Topics in Computational Intelligence, vol.\n2, no. 2, pp. 92102, April 2018.\n[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all\nyou need, CoRR, vol. abs/1706.03762, 2017.\n[5] G. Kerg, B. Kanuparthi, A. Goyal, K. Goyette, Y . Bengio, and\nG. Lajoie, Untangling tradeoffs between recurrence and self-\nattention in neural networks, CoRR, vol. abs/2006.09471,\n2020.\n[6] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y . Soplin, R. Yamamoto, X. Wang, S. Watan-\nabe, T. Yoshimura, and W. Zhang, A comparative study on\ntransformer vs rnn in speech applications, in Proc. of ASRU,\n2019, pp. 449456.\n[7] N. Li, S. Liu, Y . Liu, S. Zhao, and M. Liu, Neural speech\nsynthesis with transformer network, in Proc. of AAAI, 2019,\npp. 67066713.\n[8] J. Kim, M. El-Khamy, and J. Lee, T-gsa: Transformer with\ngaussian-weighted self-attention for speech enhancement, in\nProc. of ICASSP, 2020, pp. 66496653.\n[9] Q. Li, F. L. Kreyssig, C. Zhang, and P. C. Woodland, Dis-\ncriminative neural clustering for speaker diarisation, CoRR,\nvol. abs/1910.09703, 2019.\n[10] X. Chang, W. Zhang, Y . Qian, J. Le Roux, and S. Watan-\nabe, End-to-end multi-speaker speech recognition with trans-\nformer, in Proc. of ICASSP, 2020, pp. 61346138.\n[11] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, Deep\nclustering: Discriminative embeddings for segmentation and\nseparation, in Proc. of ICASSP, 2016, pp. 3135.\n[12] D. Yu, M. Kolbk, Z. Tan, and J. Jensen, Permutation in-\nvariant training of deep models for speaker-independent multi-\ntalker speech separation, in Proc. of ICASSP, 2017, pp. 241\n245.\n[13] M. Kolbk, D. Yu, Z.-H. Tan, and J. Jensen, Multitalker\nspeech separation with utterance-level permutation invariant\ntraining of deep recurrent neural networks,IEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing, vol. 25,\nno. 10, pp. 19011913, 2017.\n[14] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis,\nEnd-to-end source separation with adaptive front-ends, in\nProc. of ACSSC, 2018, pp. 684688.\n[15] Y . Luo and N. Mesgarani, Conv-TasNet: Surpassing Ideal\nTimeFrequency Magni', ' 2017.\n[14] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis,\nEnd-to-end source separation with adaptive front-ends, in\nProc. of ACSSC, 2018, pp. 684688.\n[15] Y . Luo and N. Mesgarani, Conv-TasNet: Surpassing Ideal\nTimeFrequency Magnitude Masking for Speech Separation,\nvol. 27, no. 8, pp. 12561266, Aug. 2019.\n[16] P. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,\nDeep learning for monoaural source separation, in Proc. of\nICASSP, 2014, pp. 15621566.\n[17] Y . Luo, Z. Chen, and T. Yoshioka, Dual-path rnn: ef-\ncient long sequence modeling for time-domain single-channel\nspeech separation, in Proc. of ICASSP, 2020, pp. 4650.\n[18] Y . Liu and D. Wang, Divide and conquer: A deep casa\napproach to talker-independent monaural speaker separation,\nIEEE/ACM Transactions on audio, speech, and language pro-\ncessing, vol. 27, no. 12, 2019.\n[19] Z. Shi, H. Lin, L. Liu, R. Liu, J. Han, and A. Shi, Furcanext:\nEnd-to-end monaural speech separation with dynamic gated di-\nlated temporal convolutional networks, in MultiMedia Mod-\neling, 2020, pp. 653665.\n[20] E. Nachmani, Y . Adi, and L. Wolf, V oice separation with\nan unknown number of multiple speakers, ICML, pp. 7164\n7175, 2020.\n[21] E. Tzinis, Z. Wang, and P. Smaragdis, Sudo rm -rf: Efcient\nnetworks for universal audio source separation, in MLSP,\n2020, pp. 16.\n[22] J. Chen, Q. Mao, and D. Liu, Dual-Path Transformer\nNetwork: Direct Context-Aware Modeling for End-to-End\nMonaural Speech Separation, in Proc. of Interspeech 2020,\n2020, pp. 26422646.\n[23] N. Zeghidour and D. Grangier, Wavesplit: End-to-end\nspeech separation by speaker clustering, arXiv preprint\narXiv:2002.08933, 2020.\n[24] L. J. Ba, J. R. Kiros, and G. E. Hinton, Layer normalization,\nCoRR, vol. abs/1607.06450, 2016.\n[25] D. P. Kingma and J. Ba, Adam: A method for stochastic opti-\nmization, arXiv preprint arXiv:1412.6980, 2014.\n[26] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, Sdr\nhalf-baked or well done?, in Proc. of ICASSP. IEEE, 2019,\npp. 626630.\n[27] Y . Luo and N. Mesgarani, TasNet: time-domain audio separa-\ntion network for real-time, single-channel speech separation,\nCoRR, vol. abs/1711.00541, 2017.\n[28] Zhong-Qiu Wang, Ke Tan, and DeLiang Wang, Deep learning\nbased phase reconstruction for speaker separation: A trigono-\nmetric perspective, in Proc. of ICASSP, 2019, pp. 7175.\n[29] E. Tzinis, S. Venkataramani, Z. Wang, C. Subakan, and\nP. Smaragdis, Two-step sound source separation: Training on\nlearned latent targets, in Proc. of ICASSP, 2020, pp. 3135.\n[30] E. Vincent, R. Gribonval, and C F evotte, Performance mea-\nsurement in blind audio source separation, IEEE transactions\non audio, speech, and language processing, vol. 14, no. 4, pp.\n14621469, 2006.\n[31] J. Kim, M. El-Khamy, and J. Lee, T-gsa: Transformer with\ngaussian-weighted self-attention for speech enhancement, in\nProc. of ICASSP, 2020, pp. 66496653.\n[32] Pytorch, Proler, https://pytorch.org/\ntutorials/recipes/recipes/profiler.html,\n2020, Accessed: 2020-10-21.\n[33] M. Pariente, S. Cornell, J. Cosentino, S. Sivasankaran, E. Tzi-\nnis, J. Heitkaemper, M. Olvera, F.-R. St oter, M. Hu, J. M.\nMartn-Donas, D. Ditter, A. Frank, A. Deleforge, and E. Vin-\ncent, Asteroid: the PyTorch-based audio source separation\ntoolkit for researchers, in Proc. of Interspeech, 2020, pp.\n26372641.'], 'dimensions': 1536}'
[0m

2025-12-08 14:33:39,781 - DEBUG - Using AiohttpTransport...
2025-12-08 14:33:39,781 - DEBUG - Creating AiohttpTransport...
2025-12-08 14:33:39,781 - DEBUG - NEW SESSION: Creating new ClientSession (no shared session provided)
2025-12-08 14:33:39,781 - DEBUG - RAW RESPONSE:
The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


2025-12-08 14:33:39,782 - DEBUG - Logging Details: logger_fn - None | callable(logger_fn) - False
2025-12-08 14:33:39,783 - DEBUG - Logging Details LiteLLM-Failure Call: [<bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x106c13850>>, <bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x109a16d50>>, <bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x10a04b790>>, <bound method Router.deployment_callback_on_failure of <litellm.router.Router object at 0x109ebe250>>]
2025-12-08 14:33:39,784 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:33:39,784 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:33:39,784 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:33:39,784 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:33:39,784 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:33:39,784 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:33:39,784 - DEBUG - Router: Entering 'deployment_callback_on_failure'
2025-12-08 14:33:39,784 - DEBUG - Router: Exiting 'deployment_callback_on_failure' without cooldown. No model_info found.
2025-12-08 14:33:39,786 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:33:39,787 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:33:39,789 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:33:39,790 - DEBUG - Custom Logger Error - Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1143, in aembedding
    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 384, in _get_openai_client
    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(
                                              ^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/openai/_client.py", line 488, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4054, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 1197, in aembedding
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/main.py", line 4069, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 450, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/integrations/custom_logger.py", line 441, in async_log_event
    await callback_func(
  File "/Users/gabriele/Documents/GitHub/research-agent-cli/.venv/lib/python3.11/site-packages/litellm/router.py", line 4958, in async_deployment_callback_on_failure
    deployment_name = kwargs["litellm_params"]["metadata"].get(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'get'

2025-12-08 14:33:39,790 - ERROR - Error adding /Users/gabriele/Documents/GitHub/research-agent-cli/library/38a0b433fee8c9dea59792d623fe7285/var-folders-yk-4-ysm4397c313kwrc2q92l0w0000gn-t-tmpax15kxg7.pdf: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable LiteLLM Retried: 3 times
2025-12-08 14:33:39,792 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:33:39,796 - DEBUG - connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2025-12-08 14:33:39,814 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x109a9e310>
2025-12-08 14:33:39,814 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10a3f0200> server_hostname='raw.githubusercontent.com' timeout=5
2025-12-08 14:33:39,836 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x109a9e510>
2025-12-08 14:33:39,837 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-12-08 14:33:39,837 - DEBUG - send_request_headers.complete
2025-12-08 14:33:39,837 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-12-08 14:33:39,837 - DEBUG - send_request_body.complete
2025-12-08 14:33:39,837 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-12-08 14:33:39,856 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'52540'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"3cd138251572ce0662c39b6cee3c3fb9dc040f283d3f3d4afe7b0952ce409f78"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'8389:10C5B:28F09C4:2ED595D:6936D36F'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 08 Dec 2025 13:33:39 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-lin1730062-LIN'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'1'), (b'X-Timer', b'S1765200820.919281,VS0,VE1'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'567f36d05f8b035cd43918c51d5b576ec2ffdfbe'), (b'Expires', b'Mon, 08 Dec 2025 13:38:39 GMT'), (b'Source-Age', b'68')])
2025-12-08 14:33:39,856 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-12-08 14:33:39,877 - DEBUG - receive_response_body.complete
2025-12-08 14:33:39,877 - DEBUG - response_closed.started
2025-12-08 14:33:39,877 - DEBUG - response_closed.complete
2025-12-08 14:33:39,878 - DEBUG - close.started
2025-12-08 14:33:39,878 - DEBUG - close.complete
2025-12-08 14:33:39,884 - INFO - Query successful: hat is a transformer
2025-12-08 14:33:39,885 - INFO - [LoggingWorker] atexit: Flushing 6 remaining events...
2025-12-08 14:33:39,885 - DEBUG - Using selector: KqueueSelector
2025-12-08 14:33:39,886 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:33:39,886 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:39,886 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,886 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,886 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:33:39,886 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:33:39,886 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:39,886 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,886 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,887 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,887 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,887 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,887 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,887 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,887 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,887 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,887 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,887 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,888 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,888 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,888 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,888 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:33:39,888 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:39,888 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,888 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,888 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:33:39,888 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:33:39,888 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:39,888 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,888 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,889 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,889 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,889 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,889 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,889 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,889 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,889 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,889 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,889 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,889 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,889 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,889 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,890 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:33:39,890 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:39,890 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,890 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,890 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:33:39,890 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:33:39,890 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:39,890 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,890 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,890 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,890 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,890 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,891 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,891 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,891 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,891 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,891 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,891 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,891 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,891 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,891 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,891 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:33:39,892 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:39,892 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,892 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,892 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:33:39,892 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:33:39,892 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:39,892 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,892 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,892 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,892 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,892 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,892 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,892 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,892 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,893 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,893 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,893 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,893 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,893 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,893 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,893 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:33:39,893 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:39,893 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,893 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,893 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:33:39,893 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:33:39,894 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:39,894 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,894 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,894 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,894 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,894 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,894 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,894 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,894 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,894 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,894 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,894 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,895 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,895 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,895 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,895 - DEBUG - Logging Details LiteLLM-Async Success Call, cache_hit=None
2025-12-08 14:33:39,895 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:39,895 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,895 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,895 - DEBUG - Async success callbacks: Got a complete streaming response
2025-12-08 14:33:39,895 - DEBUG - Model=gemini-2.0-flash-exp; cost=0.0
2025-12-08 14:33:39,895 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash-exp', 'custom_llm_provider': 'vertex_ai'}
2025-12-08 14:33:39,895 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,895 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,895 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,895 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,896 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,896 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,896 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,896 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,896 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,896 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,896 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,896 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,896 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-exp', 'combined_model_name': 'gemini/gemini-2.0-flash-exp', 'stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'combined_stripped_model_name': 'gemini/gemini-2.0-flash-exp', 'custom_llm_provider': 'gemini'}
2025-12-08 14:33:39,896 - DEBUG - model_info: {'key': 'gemini/gemini-2.0-flash-exp', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 0, 'input_cost_per_token_flex': None, 'input_cost_per_token_priority': None, 'cache_creation_input_token_cost': None, 'cache_creation_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost': 0.0, 'cache_read_input_token_cost_above_200k_tokens': None, 'cache_read_input_token_cost_flex': None, 'cache_read_input_token_cost_priority': None, 'cache_creation_input_token_cost_above_1hr': None, 'input_cost_per_character': 0, 'input_cost_per_token_above_128k_tokens': 0, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_token_flex': None, 'output_cost_per_token_priority': None, 'output_cost_per_audio_token': None, 'output_cost_per_character': 0, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 0, 'output_cost_per_character_above_128k_tokens': 0, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_video_per_second': None, 'output_cost_per_image': None, 'output_cost_per_image_token': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'tiered_pricing': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': True, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': True, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 10, 'ocr_cost_per_page': None, 'annotation_cost_per_page': None}
2025-12-08 14:33:39,896 - INFO - [LoggingWorker] atexit: Successfully flushed 6 events!
2025-12-08 14:33:39,896 - DEBUG - Using selector: KqueueSelector
